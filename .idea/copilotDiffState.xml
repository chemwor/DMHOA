<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/app.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/app.py" />
              <option name="originalContent" value="# Flask app with CORS fixes deployed on January 18, 2026&#10;import os&#10;import io&#10;import json&#10;import logging&#10;from typing import Dict, Any, Optional, Tuple, List&#10;import re&#10;from datetime import datetime&#10;import threading&#10;import time&#10;&#10;import requests&#10;from flask import Flask, request, jsonify&#10;from pypdf import PdfReader&#10;import stripe&#10;&#10;import smtplib&#10;from email.mime.text import MIMEText&#10;from email.mime.multipart import MIMEMultipart&#10;&#10;# Image processing and OCR imports&#10;from PIL import Image&#10;import pytesseract&#10;&#10;&#10;# Configure logging&#10;logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')&#10;logger = logging.getLogger(__name__)&#10;&#10;app = Flask(__name__)&#10;&#10;# Configuration&#10;SUPABASE_URL = os.environ.get('SUPABASE_URL')&#10;SUPABASE_SERVICE_ROLE_KEY = os.environ.get('SUPABASE_SERVICE_ROLE_KEY')&#10;DOC_EXTRACT_WEBHOOK_SECRET = os.environ.get('DOC_EXTRACT_WEBHOOK_SECRET')&#10;&#10;# Stripe Configuration&#10;STRIPE_SECRET_KEY = os.environ.get('STRIPE_SECRET_KEY')&#10;STRIPE_WEBHOOK_SECRET = os.environ.get('STRIPE_WEBHOOK_SECRET')&#10;STRIPE_PRICE_ID = os.environ.get('STRIPE_PRICE_ID')&#10;SITE_URL = os.environ.get('SITE_URL', 'https://disputemyhoa.com')&#10;&#10;# OpenAI Configuration&#10;OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')&#10;&#10;# Configure Stripe&#10;if STRIPE_SECRET_KEY:&#10;    stripe.api_key = STRIPE_SECRET_KEY&#10;&#10;# SMTP Configuration - Optional, only needed for email functionality&#10;SMTP_HOST = os.environ.get(&quot;SMTP_HOST&quot;)&#10;SMTP_PORT = int(os.environ.get(&quot;SMTP_PORT&quot;, &quot;587&quot;))&#10;# Clean SMTP credentials to handle non-ASCII characters like non-breaking spaces&#10;SMTP_USER = (os.environ.get(&quot;SMTP_USER&quot;) or &quot;&quot;).strip().replace('\xa0', ' ')&#10;SMTP_PASS = (os.environ.get(&quot;SMTP_PASS&quot;) or &quot;&quot;).strip().replace('\xa0', ' ')&#10;SMTP_FROM = os.environ.get(&quot;SMTP_FROM&quot;, &quot;support@disputemyhoa.com&quot;)&#10;&#10;SMTP_SENDER_WEBHOOK_SECRET = os.environ.get(&quot;SMTP_SENDER_WEBHOOK_SECRET&quot;)&#10;SMTP_SENDER_WEBHOOK_URL = os.environ.get(&quot;SMTP_SENDER_WEBHOOK_URL&quot;)&#10;&#10;# Request timeouts&#10;TIMEOUT = (5, 60)  # (connect, read)&#10;&#10;def supabase_headers() -&gt; Dict[str, str]:&#10;    &quot;&quot;&quot;Return headers for Supabase API requests.&quot;&quot;&quot;&#10;    return {&#10;        'apikey': SUPABASE_SERVICE_ROLE_KEY,&#10;        'Authorization': f'Bearer {SUPABASE_SERVICE_ROLE_KEY}',&#10;        'Content-Type': 'application/json'&#10;    }&#10;&#10;def fetch_ready_documents_by_token(token: str, limit: int = 3) -&gt; List[Dict]:&#10;    &quot;&quot;&quot;Query Supabase for ready documents by case token.&quot;&quot;&quot;&#10;    try:&#10;        url = f&quot;{SUPABASE_URL}/rest/v1/dmhoa_documents&quot;&#10;        params = {&#10;            'token': f'eq.{token}',&#10;            'status': 'eq.ready',&#10;            'select': 'id,filename,mime_type,page_count,char_count,updated_at,extracted_text',&#10;            'order': 'updated_at.desc',&#10;            'limit': str(limit)&#10;        }&#10;        headers = supabase_headers()&#10;&#10;        response = requests.get(url, params=params, headers=headers, timeout=TIMEOUT)&#10;        response.raise_for_status()&#10;&#10;        documents = response.json()&#10;        logger.info(f&quot;Found {len(documents)} ready documents for token {token[:12]}...&quot;)&#10;        return documents&#10;&#10;    except Exception as e:&#10;        logger.error(f&quot;Failed to fetch ready documents for token {token[:12]}...: {str(e)}&quot;)&#10;        return []&#10;&#10;def fetch_any_documents_status_by_token(token: str) -&gt; List[Dict]:&#10;    &quot;&quot;&quot;Check for any documents (including processing/pending) by token.&quot;&quot;&quot;&#10;    try:&#10;        url = f&quot;{SUPABASE_URL}/rest/v1/dmhoa_documents&quot;&#10;        params = {&#10;            'token': f'eq.{token}',&#10;            'select': 'id,status,updated_at',&#10;            'order': 'updated_at.desc'&#10;        }&#10;        headers = supabase_headers()&#10;&#10;        response = requests.get(url, params=params, headers=headers, timeout=TIMEOUT)&#10;        response.raise_for_status()&#10;&#10;        return response.json()&#10;&#10;    except Exception as e:&#10;        logger.error(f&quot;Failed to fetch document status for token {token[:12]}...: {str(e)}&quot;)&#10;        return []&#10;&#10;def summarize_doc_text_with_openai(token: str, raw_text: str) -&gt; str:&#10;    &quot;&quot;&quot;Summarize document text using OpenAI gpt-4o-mini.&quot;&quot;&quot;&#10;    try:&#10;        # Clip text to avoid token limits&#10;        clipped_text = raw_text[:12000] if raw_text else &quot;&quot;&#10;&#10;        if not clipped_text.strip():&#10;            return &quot;&quot;&#10;&#10;        headers = {&#10;            'Authorization': f'Bearer {OPENAI_API_KEY}',&#10;            'Content-Type': 'application/json'&#10;        }&#10;&#10;        prompt = &quot;&quot;&quot;Summarize the HOA notice into: (a) alleged violation, (b) what HOA demands, (c) deadlines/fines/next actions, (d) evidence/rules cited. Be factual, quote exact deadlines/amounts when present. 8-12 bullets max.&quot;&quot;&quot;&#10;&#10;        data = {&#10;            &quot;model&quot;: &quot;gpt-4o-mini&quot;,&#10;            &quot;messages&quot;: [&#10;                {&#10;                    &quot;role&quot;: &quot;system&quot;,&#10;                    &quot;content&quot;: &quot;You are a document summarizer for HOA notices. Extract key facts concisely.&quot;&#10;                },&#10;                {&#10;                    &quot;role&quot;: &quot;user&quot;,&#10;                    &quot;content&quot;: f&quot;{prompt}\n\nDocument text:\n{clipped_text}&quot;&#10;                }&#10;            ],&#10;            &quot;temperature&quot;: 0.3,&#10;            &quot;max_tokens&quot;: 800&#10;        }&#10;&#10;        response = requests.post(&#10;            'https://api.openai.com/v1/chat/completions',&#10;            headers=headers,&#10;            json=data,&#10;            timeout=30&#10;        )&#10;        response.raise_for_status()&#10;&#10;        result = response.json()&#10;        summary = result['choices'][0]['message']['content'].strip()&#10;&#10;        # Limit summary length&#10;        if len(summary) &gt; 1200:&#10;            summary = summary[:1200] + &quot;...&quot;&#10;&#10;        logger.info(f&quot;Successfully summarized document for token {token[:12]}...&quot;)&#10;        return summary&#10;&#10;    except Exception as e:&#10;        logger.error(f&quot;Failed to summarize document text for token {token[:12]}...: {str(e)}&quot;)&#10;        # Fallback to clipped excerpt&#10;        return raw_text[:1200] + &quot;...&quot; if len(raw_text) &gt; 1200 else raw_text&#10;&#10;def build_doc_brief(docs: List[Dict]) -&gt; Dict:&#10;    &quot;&quot;&quot;Build document brief from ready documents.&quot;&quot;&quot;&#10;    if not docs:&#10;        return {&#10;            &quot;doc_status&quot;: &quot;none&quot;,&#10;            &quot;doc_count&quot;: 0,&#10;            &quot;sources&quot;: [],&#10;            &quot;brief_text&quot;: &quot;&quot;&#10;        }&#10;&#10;    sources = []&#10;    raw_texts = []&#10;&#10;    for doc in docs:&#10;        sources.append({&#10;            &quot;filename&quot;: doc.get(&quot;filename&quot;, &quot;unknown&quot;),&#10;            &quot;page_count&quot;: doc.get(&quot;page_count&quot;, 0),&#10;            &quot;char_count&quot;: doc.get(&quot;char_count&quot;, 0)&#10;        })&#10;&#10;        # Extract text with limits&#10;        extracted_text = doc.get(&quot;extracted_text&quot;, &quot;&quot;)&#10;        if extracted_text:&#10;            # Limit per document to 6000 chars&#10;            doc_text = extracted_text[:6000]&#10;            raw_texts.append(doc_text)&#10;&#10;    if not raw_texts:&#10;        return {&#10;            &quot;doc_status&quot;: &quot;ready&quot;,&#10;            &quot;doc_count&quot;: len(docs),&#10;            &quot;sources&quot;: sources,&#10;            &quot;brief_text&quot;: &quot;&quot;&#10;        }&#10;&#10;    # Combine texts with total limit of 12000 chars&#10;    combined_text = &quot; &quot;.join(raw_texts)&#10;    if len(combined_text) &gt; 12000:&#10;        combined_text = combined_text[:12000]&#10;&#10;    # Try to summarize with OpenAI&#10;    token = docs[0].get(&quot;token&quot;, &quot;unknown&quot;) if docs else &quot;unknown&quot;&#10;    brief_text = summarize_doc_text_with_openai(token, combined_text)&#10;&#10;    return {&#10;        &quot;doc_status&quot;: &quot;ready&quot;,&#10;        &quot;doc_count&quot;: len(docs),&#10;        &quot;sources&quot;: sources,&#10;        &quot;brief_text&quot;: brief_text&#10;    }&#10;&#10;&#10;def fetch_document_status(document_id: str) -&gt; Optional[Dict[str, Any]]:&#10;    &quot;&quot;&quot;Fetch current document status from Supabase.&quot;&quot;&quot;&#10;    try:&#10;        url = f&quot;{SUPABASE_URL}/rest/v1/dmhoa_documents&quot;&#10;        params = {&#10;            'id': f'eq.{document_id}',&#10;            'select': 'id,token,status'&#10;        }&#10;        headers = supabase_headers()&#10;&#10;        response = requests.get(url, params=params, headers=headers, timeout=TIMEOUT)&#10;        response.raise_for_status()&#10;&#10;        data = response.json()&#10;        return data[0] if data else None&#10;&#10;    except Exception as e:&#10;        logger.error(f&quot;Failed to fetch document status for {document_id}: {str(e)}&quot;)&#10;        return None&#10;&#10;def update_document(document_id: str, token: str, updates: Dict[str, Any]) -&gt; bool:&#10;    &quot;&quot;&quot;Update document in Supabase database.&quot;&quot;&quot;&#10;    try:&#10;        url = f&quot;{SUPABASE_URL}/rest/v1/dmhoa_documents&quot;&#10;        params = {&#10;            'id': f'eq.{document_id}',&#10;            'token': f'eq.{token}'&#10;        }&#10;        headers = supabase_headers()&#10;        headers['Prefer'] = 'return=representation'&#10;&#10;        response = requests.patch(url, params=params, headers=headers,&#10;                                json=updates, timeout=TIMEOUT)&#10;        response.raise_for_status()&#10;&#10;        logger.info(f&quot;Updated document {document_id} with: {updates}&quot;)&#10;        return True&#10;&#10;    except Exception as e:&#10;        logger.error(f&quot;Failed to update document {document_id}: {str(e)}&quot;)&#10;        return False&#10;&#10;def download_storage_object(bucket: str, path: str) -&gt; Optional[bytes]:&#10;    &quot;&quot;&quot;Download file from Supabase Storage.&quot;&quot;&quot;&#10;    try:&#10;        url = f&quot;{SUPABASE_URL}/storage/v1/object/{bucket}/{path}&quot;&#10;        headers = {&#10;            'apikey': SUPABASE_SERVICE_ROLE_KEY,&#10;            'Authorization': f'Bearer {SUPABASE_SERVICE_ROLE_KEY}'&#10;        }&#10;&#10;        response = requests.get(url, headers=headers, timeout=TIMEOUT)&#10;&#10;        if response.status_code == 404:&#10;            logger.error(f&quot;File not found: {bucket}/{path}&quot;)&#10;            return None&#10;&#10;        response.raise_for_status()&#10;&#10;        logger.info(f&quot;Downloaded {len(response.content)} bytes from {bucket}/{path}&quot;)&#10;        return response.content&#10;&#10;    except Exception as e:&#10;        logger.error(f&quot;Failed to download {bucket}/{path}: {str(e)}&quot;)&#10;        return None&#10;&#10;def extract_pdf_text(pdf_bytes: bytes) -&gt; Tuple[str, int, int, Optional[str]]:&#10;    &quot;&quot;&quot;&#10;    Extract text from PDF bytes.&#10;    Returns: (extracted_text, page_count, char_count, error_message)&#10;    &quot;&quot;&quot;&#10;    try:&#10;        reader = PdfReader(io.BytesIO(pdf_bytes))&#10;        page_count = len(reader.pages)&#10;&#10;        text_parts = []&#10;        for page in reader.pages:&#10;            try:&#10;                page_text = page.extract_text() or &quot;&quot;&#10;                text_parts.append(page_text)&#10;            except Exception as e:&#10;                logger.warning(f&quot;Failed to extract text from page: {str(e)}&quot;)&#10;                text_parts.append(&quot;&quot;)&#10;&#10;        extracted_text = &quot;\n\n&quot;.join(text_parts)&#10;        char_count = len(extracted_text)&#10;&#10;        # Check if text is empty or only whitespace&#10;        if not extracted_text or extracted_text.strip() == &quot;&quot;:&#10;            return &quot;&quot;, page_count, 0, &quot;No text layer found - document may be scanned and require OCR&quot;&#10;&#10;        logger.info(f&quot;Extracted {char_count} characters from {page_count} pages&quot;)&#10;        return extracted_text, page_count, char_count, None&#10;&#10;    except Exception as e:&#10;        error_msg = f&quot;Failed to extract text from PDF: {str(e)}&quot;&#10;        logger.error(error_msg)&#10;        return &quot;&quot;, 0, 0, error_msg&#10;&#10;&#10;def extract_image_text(image_bytes: bytes, filename: str = &quot;&quot;) -&gt; Tuple[str, int, int, Optional[str]]:&#10;    &quot;&quot;&quot;&#10;    Extract text from image bytes using OCR.&#10;    Returns: (extracted_text, page_count=1, char_count, error_message)&#10;    &quot;&quot;&quot;&#10;    try:&#10;        # Open image from bytes&#10;        image = Image.open(io.BytesIO(image_bytes))&#10;&#10;        # Convert to RGB if necessary (some formats like RGBA or P need conversion)&#10;        if image.mode not in ('RGB', 'L'):&#10;            image = image.convert('RGB')&#10;&#10;        logger.info(f&quot;Processing image: {image.size[0]}x{image.size[1]} pixels, mode: {image.mode}&quot;)&#10;&#10;        # Set TESSDATA_PREFIX if not already set (for Heroku compatibility)&#10;        if 'TESSDATA_PREFIX' not in os.environ:&#10;            possible_paths = [&#10;                '/usr/share/tesseract-ocr/5/tessdata',&#10;                '/usr/share/tesseract-ocr/tessdata',&#10;                '/usr/share/tesseract-ocr/4.00/tessdata',&#10;                '/usr/share/tesseract-ocr/4/tessdata',&#10;                '/usr/share/tesseract-ocr/tessdata',&#10;                '/usr/share/tessdata',&#10;                '/app/.apt/usr/share/tesseract-ocr/5/tessdata',&#10;                '/app/.apt/usr/share/tesseract-ocr/tessdata'&#10;            ]&#10;            for path in possible_paths:&#10;                if os.path.exists(path):&#10;                    os.environ['TESSDATA_PREFIX'] = path&#10;                    logger.info(f&quot;Set TESSDATA_PREFIX to: {path}&quot;)&#10;                    break&#10;            else:&#10;                logger.warning(&quot;Could not find tessdata directory in any expected location&quot;)&#10;&#10;        # Try multiple OCR configurations for better compatibility&#10;        configs_to_try = [&#10;            r'--oem 1 --psm 1 -l eng',   # Automatic page segmentation with OSD&#10;            r'--oem 1 --psm 3 -l eng',   # Fully automatic page segmentation, but no OSD&#10;            r'--oem 1 --psm 4 -l eng',   # Assume a single column of text of variable sizes&#10;            r'--oem 1 --psm 6 -l eng',   # Assume a single uniform block of text&#10;            r'--oem 3 --psm 1 -l eng',   # LSTM with automatic page segmentation&#10;            r'--oem 3 --psm 3 -l eng',   # LSTM with fully automatic page segmentation&#10;            r'--oem 3 --psm 6 -l eng',   # LSTM standard config&#10;            r'--oem 3 --psm 11 -l eng',  # Sparse text - find as much text as possible&#10;            r'--oem 3 --psm 12 -l eng',  # Sparse text with OSD&#10;            r'--psm 6',                  # No language specified fallback&#10;        ]&#10;&#10;        extracted_text = &quot;&quot;&#10;        best_text = &quot;&quot;&#10;        best_char_count = 0&#10;        last_error = None&#10;&#10;        for config in configs_to_try:&#10;            try:&#10;                logger.info(f&quot;Trying OCR with config: {config}&quot;)&#10;                current_text = pytesseract.image_to_string(image, config=config)&#10;                current_text = current_text.strip()&#10;                char_count = len(current_text)&#10;&#10;                # Keep track of the best result (most text that looks reasonable)&#10;                if char_count &gt; best_char_count:&#10;                    # Basic heuristic: prefer results with more alphanumeric content&#10;                    alphanumeric_ratio = sum(c.isalnum() or c.isspace() for c in current_text) / max(len(current_text), 1)&#10;                    if alphanumeric_ratio &gt; 0.3:  # At least 30% should be readable characters&#10;                        best_text = current_text&#10;                        best_char_count = char_count&#10;                        logger.info(f&quot;New best result: {char_count} chars, {alphanumeric_ratio:.2f} alphanumeric ratio&quot;)&#10;&#10;                # If we got a decent amount of readable text, we can stop&#10;                if char_count &gt; 50 and best_text:&#10;                    extracted_text = best_text&#10;                    break&#10;&#10;            except Exception as e:&#10;                last_error = str(e)&#10;                logger.warning(f&quot;OCR config failed: {config}, error: {str(e)}&quot;)&#10;                continue&#10;&#10;        # Use the best result we found&#10;        if not extracted_text and best_text:&#10;            extracted_text = best_text&#10;&#10;        # Clean up the extracted text&#10;        extracted_text = extracted_text.strip()&#10;        char_count = len(extracted_text)&#10;&#10;        if char_count == 0:&#10;            error_msg = f&quot;No text found in image - image may be blank or contain no readable text. Last OCR error: {last_error}&quot;&#10;            return &quot;&quot;, 1, 0, error_msg&#10;&#10;        logger.info(f&quot;OCR extracted {char_count} characters from image {filename}&quot;)&#10;        return extracted_text, 1, char_count, None&#10;&#10;    except Exception as e:&#10;        error_msg = f&quot;Failed to extract text from image: {str(e)}&quot;&#10;        logger.error(error_msg)&#10;        return &quot;&quot;, 0, 0, error_msg&#10;&#10;&#10;def is_image_file(filename: str, mime_type: str = &quot;&quot;) -&gt; bool:&#10;    &quot;&quot;&quot;Check if file is a supported image format.&quot;&quot;&quot;&#10;    image_extensions = {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.tif', '.webp'}&#10;    image_mime_types = {&#10;        'image/jpeg', 'image/jpg', 'image/png', 'image/gif',&#10;        'image/bmp', 'image/tiff', 'image/webp'&#10;    }&#10;&#10;    # Check by file extension&#10;    if filename:&#10;        ext = os.path.splitext(filename.lower())[1]&#10;        if ext in image_extensions:&#10;            return True&#10;&#10;    # Check by MIME type&#10;    if mime_type and mime_type.lower() in image_mime_types:&#10;        return True&#10;&#10;    return False&#10;&#10;&#10;def is_pdf_file(filename: str, mime_type: str = &quot;&quot;) -&gt; bool:&#10;    &quot;&quot;&quot;Check if file is a PDF.&quot;&quot;&quot;&#10;    if filename and filename.lower().endswith('.pdf'):&#10;        return True&#10;    if mime_type and mime_type.lower() == 'application/pdf':&#10;        return True&#10;    return False&#10;&#10;&#10;@app.route('/health', methods=['GET'])&#10;def health_check():&#10;    &quot;&quot;&quot;Health check endpoint.&quot;&quot;&quot;&#10;    return jsonify({'status': 'healthy'}), 200&#10;&#10;&#10;@app.route('/debug/env', methods=['GET'])&#10;def debug_env():&#10;    &quot;&quot;&quot;Return presence of critical env vars (gated by DOC_EXTRACT_WEBHOOK_SECRET).&#10;&#10;    This does NOT return any secret values, only boolean flags indicating whether each&#10;    required configuration item is set. Intended for quick diagnostics on deployed app.&#10;    &quot;&quot;&quot;&#10;    secret = request.headers.get('X-Webhook-Secret')&#10;    if not secret or secret != DOC_EXTRACT_WEBHOOK_SECRET:&#10;        logger.warning(&quot;Unauthorized request to /debug/env&quot;)&#10;        return jsonify({'error': 'Unauthorized'}), 401&#10;&#10;    keys = [&#10;        'SUPABASE_URL', 'SUPABASE_SERVICE_ROLE_KEY', 'DOC_EXTRACT_WEBHOOK_SECRET',&#10;        'SMTP_HOST', 'SMTP_PORT', 'SMTP_USER', 'SMTP_PASS', 'SMTP_FROM', 'SMTP_SENDER_WEBHOOK_SECRET'&#10;    ]&#10;    presence = {k: bool(os.environ.get(k)) for k in keys}&#10;    logger.info(f&quot;/debug/env requested; presence: { {k: presence[k] for k in presence} }&quot;)&#10;    return jsonify({'env_presence': presence}), 200&#10;&#10;@app.route('/webhooks/doc-extract', methods=['POST'])&#10;def doc_extract_webhook():&#10;    &quot;&quot;&quot;Main webhook endpoint for document extraction.&quot;&quot;&quot;&#10;    # Validate webhook secret&#10;    webhook_secret = request.headers.get('X-Webhook-Secret')&#10;    if not webhook_secret or webhook_secret != DOC_EXTRACT_WEBHOOK_SECRET:&#10;        logger.warning(&quot;Invalid or missing webhook secret&quot;)&#10;        return jsonify({'error': 'Unauthorized'}), 401&#10;&#10;    document_id = None&#10;    token = None&#10;&#10;    try:&#10;        # Parse JSON body&#10;        data = request.get_json()&#10;        if not data:&#10;            return jsonify({'error': 'Invalid JSON body'}), 400&#10;&#10;        # Validate required fields&#10;        required_fields = ['token', 'document_id', 'bucket', 'path']&#10;        missing_fields = [field for field in required_fields if not data.get(field)]&#10;        if missing_fields:&#10;            return jsonify({&#10;                'error': f'Missing required fields: {&quot;, &quot;.join(missing_fields)}'&#10;            }), 400&#10;&#10;        token = data['token']&#10;        document_id = data['document_id']&#10;        bucket = data['bucket']&#10;        path = data['path']&#10;        filename = data.get('filename', '') or ''  # Handle null values&#10;        mime_type = data.get('mime_type', '') or ''  # Handle null values&#10;&#10;        logger.info(f&quot;Processing document extraction - ID: {document_id}, Token: {token[:8]}...&quot;)&#10;&#10;        # Check if document is already processed&#10;        current_doc = fetch_document_status(document_id)&#10;        if current_doc and current_doc.get('status') == 'ready':&#10;            logger.info(f&quot;Document {document_id} already processed&quot;)&#10;            return jsonify({&#10;                'message': 'Document already processed',&#10;                'document_id': document_id,&#10;                'status': 'ready'&#10;            }), 200&#10;&#10;        # Mark document as processing&#10;        if not update_document(document_id, token, {'status': 'processing'}):&#10;            return jsonify({&#10;                'error': 'Failed to update document status to processing',&#10;                'document_id': document_id&#10;            }), 500&#10;&#10;        # Download file from Supabase Storage&#10;        file_bytes = download_storage_object(bucket, path)&#10;        if file_bytes is None:&#10;            error_msg = f&quot;Failed to download file from {bucket}/{path}&quot;&#10;            update_document(document_id, token, {&#10;                'status': 'failed',&#10;                'error': error_msg[:2000]&#10;            })&#10;            return jsonify({&#10;                'error': error_msg,&#10;                'document_id': document_id&#10;            }), 500&#10;&#10;        # Determine file type with multiple fallback strategies&#10;        logger.info(f&quot;Initial file detection - filename: '{filename}', mime_type: '{mime_type}'&quot;)&#10;&#10;        # Strategy 1: If filename is empty/null, extract from path&#10;        if not filename or filename.lower() == 'null':&#10;            filename = os.path.basename(path)&#10;            logger.info(f&quot;Extracted filename from path: '{filename}'&quot;)&#10;&#10;        # Strategy 2: If mime_type is empty/null, guess from filename&#10;        if not mime_type or mime_type.lower() == 'null':&#10;            if filename:&#10;                ext = os.path.splitext(filename.lower())[1]&#10;                mime_type_map = {&#10;                    '.pdf': 'application/pdf',&#10;                    '.jpg': 'image/jpeg', '.jpeg': 'image/jpeg',&#10;                    '.png': 'image/png', '.gif': 'image/gif',&#10;                    '.bmp': 'image/bmp', '.tiff': 'image/tiff', '.tif': 'image/tiff',&#10;                    '.webp': 'image/webp'&#10;                }&#10;                mime_type = mime_type_map.get(ext, '')&#10;                logger.info(f&quot;Guessed MIME type from extension '{ext}': '{mime_type}'&quot;)&#10;&#10;        # Strategy 3: If still no filename, try extracting just the filename from the full path&#10;        if not filename:&#10;            # Handle paths like &quot;dmhoa-docs/case_xxx/original/image.jpg&quot;&#10;            path_parts = path.split('/')&#10;            if path_parts:&#10;                filename = path_parts[-1]  # Get the last part&#10;                logger.info(f&quot;Extracted filename from path parts: '{filename}'&quot;)&#10;&#10;        # Strategy 4: If we still have no clear type, try to detect from file content&#10;        detected_type = None&#10;        if not (is_pdf_file(filename, mime_type) or is_image_file(filename, mime_type)):&#10;            # Check file magic bytes as last resort&#10;            if file_bytes and len(file_bytes) &gt;= 4:&#10;                # PDF magic bytes&#10;                if file_bytes.startswith(b'%PDF'):&#10;                    detected_type = 'pdf'&#10;                    logger.info(&quot;Detected PDF from file magic bytes&quot;)&#10;                # JPEG magic bytes&#10;                elif file_bytes.startswith(b'\xff\xd8\xff'):&#10;                    detected_type = 'image'&#10;                    mime_type = 'image/jpeg'&#10;                    logger.info(&quot;Detected JPEG from file magic bytes&quot;)&#10;                # PNG magic bytes&#10;                elif file_bytes.startswith(b'\x89PNG'):&#10;                    detected_type = 'image'&#10;                    mime_type = 'image/png'&#10;                    logger.info(&quot;Detected PNG from file magic bytes&quot;)&#10;&#10;        logger.info(f&quot;Final file detection - filename: '{filename}', mime_type: '{mime_type}', detected_type: {detected_type}&quot;)&#10;&#10;        # Process based on detected file type&#10;        if is_pdf_file(filename, mime_type) or detected_type == 'pdf':&#10;            logger.info(f&quot;Processing as PDF: {filename}&quot;)&#10;            extracted_text, page_count, char_count, extraction_error = extract_pdf_text(file_bytes)&#10;        elif is_image_file(filename, mime_type) or detected_type == 'image':&#10;            logger.info(f&quot;Processing as image: {filename}&quot;)&#10;            extracted_text, page_count, char_count, extraction_error = extract_image_text(file_bytes, filename)&#10;        else:&#10;            error_msg = f&quot;Unsupported file type: {filename} (MIME: {mime_type})&quot;&#10;            logger.error(error_msg)&#10;            update_document(document_id, token, {&#10;                'status': 'failed',&#10;                'error': error_msg[:2000]&#10;            })&#10;            return jsonify({&#10;                'error': error_msg,&#10;                'document_id': document_id&#10;            }), 400&#10;&#10;        # Update document with extraction results&#10;        update_data = {&#10;            'status': 'ready',&#10;            'extracted_text': extracted_text[:50000],  # Limit text size&#10;            'page_count': page_count,&#10;            'char_count': char_count&#10;        }&#10;&#10;        if extraction_error:&#10;            update_data['error'] = extraction_error[:2000]&#10;            logger.warning(f&quot;Document {document_id} processed with warning: {extraction_error}&quot;)&#10;        else:&#10;            logger.info(f&quot;Document {document_id} processed successfully: {char_count} chars, {page_count} pages&quot;)&#10;&#10;        if not update_document(document_id, token, update_data):&#10;            return jsonify({&#10;                'error': 'Failed to save extraction results',&#10;                'document_id': document_id&#10;            }), 500&#10;&#10;        return jsonify({&#10;            'message': 'Document processed successfully',&#10;            'document_id': document_id,&#10;            'status': 'ready',&#10;            'page_count': page_count,&#10;            'char_count': char_count,&#10;            'has_error': bool(extraction_error)&#10;        }), 200&#10;&#10;    except Exception as e:&#10;        error_msg = f&quot;Unexpected error processing document: {str(e)}&quot;&#10;        logger.error(error_msg)&#10;&#10;        # Try to update document status to failed if we have the IDs&#10;        if document_id and token:&#10;            try:&#10;                update_document(document_id, token, {&#10;                    'status': 'failed',&#10;                    'error': error_msg[:2000]&#10;                })&#10;            except:&#10;                pass  # Don't fail the response if we can't update status&#10;&#10;        return jsonify({&#10;            'error': error_msg,&#10;            'document_id': document_id&#10;        }), 500&#10;&#10;&#10;def read_case_by_token(token: str) -&gt; Optional[Dict[str, Any]]:&#10;    &quot;&quot;&quot;Fetch case details from Supabase by token.&quot;&quot;&quot;&#10;    try:&#10;        url = f&quot;{SUPABASE_URL}/rest/v1/dmhoa_cases&quot;&#10;        params = {&#10;            'token': f'eq.{token}',&#10;            'select': '*'&#10;        }&#10;        headers = supabase_headers()&#10;&#10;        response = requests.get(url, params=params, headers=headers, timeout=TIMEOUT)&#10;        response.raise_for_status()&#10;&#10;        cases = response.json()&#10;        if not cases:&#10;            logger.warning(f&quot;No case found for token: {token[:12]}...&quot;)&#10;            return None&#10;&#10;        case = cases[0]&#10;        logger.info(f&quot;Found case for token {token[:12]}...: ID {case.get('id')}&quot;)&#10;        return case&#10;&#10;    except Exception as e:&#10;        logger.error(f&quot;Failed to fetch case for token {token[:12]}...: {str(e)}&quot;)&#10;        return None&#10;&#10;&#10;def generate_case_preview_with_openai(case_data: Dict, doc_brief: Dict) -&gt; str:&#10;    &quot;&quot;&quot;Generate case preview using OpenAI.&quot;&quot;&quot;&#10;    try:&#10;        if not OPENAI_API_KEY:&#10;            logger.warning(&quot;OpenAI API key not configured&quot;)&#10;            return &quot;Preview generation unavailable - OpenAI not configured&quot;&#10;&#10;        # Extract case information&#10;        hoa_name = case_data.get('hoa_name', 'Unknown HOA')&#10;        violation_type = case_data.get('violation_type', 'Unknown violation')&#10;        case_description = case_data.get('case_description', 'No description provided')&#10;&#10;        # Get document brief&#10;        doc_text = doc_brief.get('brief_text', '')&#10;        doc_count = doc_brief.get('doc_count', 0)&#10;&#10;        # Prepare the prompt&#10;        prompt = f&quot;&quot;&quot;Generate a professional case preview for this HOA dispute:&#10;&#10;HOA: {hoa_name}&#10;Violation Type: {violation_type}&#10;Case Description: {case_description}&#10;Documents Analyzed: {doc_count}&#10;&#10;Document Summary:&#10;{doc_text[:2000] if doc_text else 'No documents available'}&#10;&#10;Create a concise case preview that includes:&#10;1. Brief case summary&#10;2. Key issues identified&#10;3. Potential legal considerations&#10;4. Recommended next steps&#10;&#10;Keep it professional, factual, and under 800 words.&quot;&quot;&quot;&#10;&#10;        headers = {&#10;            'Authorization': f'Bearer {OPENAI_API_KEY}',&#10;            'Content-Type': 'application/json'&#10;        }&#10;&#10;        data = {&#10;            &quot;model&quot;: &quot;gpt-4o-mini&quot;,&#10;            &quot;messages&quot;: [&#10;                {&#10;                    &quot;role&quot;: &quot;system&quot;,&#10;                    &quot;content&quot;: &quot;You are a legal case analyst specializing in HOA disputes. Provide professional, factual analysis.&quot;&#10;                },&#10;                {&#10;                    &quot;role&quot;: &quot;user&quot;,&#10;                    &quot;content&quot;: prompt&#10;                }&#10;            ],&#10;            &quot;temperature&quot;: 0.3,&#10;            &quot;max_tokens&quot;: 1200&#10;        }&#10;&#10;        response = requests.post(&#10;            'https://api.openai.com/v1/chat/completions',&#10;            headers=headers,&#10;            json=data,&#10;            timeout=30&#10;        )&#10;        response.raise_for_status()&#10;&#10;        result = response.json()&#10;        preview = result['choices'][0]['message']['content'].strip()&#10;&#10;        logger.info(f&quot;Generated case preview: {len(preview)} characters&quot;)&#10;        return preview&#10;&#10;    except Exception as e:&#10;        logger.error(f&quot;Failed to generate case preview: {str(e)}&quot;)&#10;        return f&quot;Error generating preview: {str(e)}&quot;&#10;&#10;&#10;def save_case_preview(token: str, preview_text: str, doc_brief: Dict) -&gt; bool:&#10;    &quot;&quot;&quot;Save generated case preview to Supabase.&quot;&quot;&quot;&#10;    try:&#10;        url = f&quot;{SUPABASE_URL}/rest/v1/dmhoa_cases&quot;&#10;        params = {'token': f'eq.{token}'}&#10;        headers = supabase_headers()&#10;        headers['Prefer'] = 'return=representation'&#10;&#10;        update_data = {&#10;            'preview_text': preview_text[:10000],  # Limit size&#10;            'preview_generated_at': datetime.utcnow().isoformat(),&#10;            'doc_summary': doc_brief&#10;        }&#10;&#10;        response = requests.patch(url, params=params, headers=headers,&#10;                                json=update_data, timeout=TIMEOUT)&#10;        response.raise_for_status()&#10;&#10;        logger.info(f&quot;Saved case preview for token {token[:12]}...&quot;)&#10;        return True&#10;&#10;    except Exception as e:&#10;        logger.error(f&quot;Failed to save case preview for token {token[:12]}...: {str(e)}&quot;)&#10;        return False&#10;&#10;&#10;@app.route('/api/case-preview', methods=['POST'])&#10;def case_preview_endpoint():&#10;    &quot;&quot;&quot;Generate and return case preview with document analysis.&quot;&quot;&quot;&#10;    try:&#10;        data = request.get_json()&#10;        if not data:&#10;            return jsonify({'error': 'Invalid JSON body'}), 400&#10;&#10;        token = data.get('token')&#10;        if not token:&#10;            return jsonify({'error': 'Missing required field: token'}), 400&#10;&#10;        logger.info(f&quot;Processing case preview request for token: {token[:12]}...&quot;)&#10;&#10;        # Fetch case details&#10;        case = read_case_by_token(token)&#10;        if not case:&#10;            return jsonify({&#10;                'error': 'Case not found',&#10;                'token': token[:12] + '...'&#10;            }), 404&#10;&#10;        # Check if preview already exists and is recent&#10;        existing_preview = case.get('preview_text')&#10;        preview_generated_at = case.get('preview_generated_at')&#10;&#10;        if existing_preview and preview_generated_at:&#10;            try:&#10;                # If preview was generated less than 1 hour ago, return it&#10;                from datetime import datetime, timedelta&#10;                generated_time = datetime.fromisoformat(preview_generated_at.replace('Z', '+00:00'))&#10;                if datetime.now().replace(tzinfo=generated_time.tzinfo) - generated_time &lt; timedelta(hours=1):&#10;                    logger.info(f&quot;Returning existing preview for token {token[:12]}...&quot;)&#10;                    return jsonify({&#10;                        'preview': existing_preview,&#10;                        'cached': True,&#10;                        'generated_at': preview_generated_at&#10;                    }), 200&#10;            except:&#10;                pass  # If there's any issue with date parsing, continue with new generation&#10;&#10;        # Fetch and analyze documents&#10;        documents = fetch_ready_documents_by_token(token, limit=5)&#10;        doc_brief = build_doc_brief(documents)&#10;&#10;        # Generate case preview&#10;        preview_text = generate_case_preview_with_openai(case, doc_brief)&#10;&#10;        # Save the preview&#10;        if save_case_preview(token, preview_text, doc_brief):&#10;            logger.info(f&quot;Successfully generated and saved preview for token {token[:12]}...&quot;)&#10;        else:&#10;            logger.warning(f&quot;Generated preview but failed to save for token {token[:12]}...&quot;)&#10;&#10;        return jsonify({&#10;            'preview': preview_text,&#10;            'doc_summary': doc_brief,&#10;            'cached': False,&#10;            'generated_at': datetime.utcnow().isoformat()&#10;        }), 200&#10;&#10;    except Exception as e:&#10;        logger.error(f&quot;Error in case preview endpoint: {str(e)}&quot;)&#10;        return jsonify({&#10;            'error': f'Internal server error: {str(e)}'&#10;        }), 500&#10;&#10;&#10;@app.route('/api/case/&lt;token&gt;/status', methods=['GET'])&#10;def case_status(token: str):&#10;    &quot;&quot;&quot;Get case and document status.&quot;&quot;&quot;&#10;    try:&#10;        # Fetch case&#10;        case = read_case_by_token(token)&#10;        if not case:&#10;            return jsonify({'error': 'Case not found'}), 404&#10;&#10;        # Fetch document status&#10;        documents = fetch_any_documents_status_by_token(token)&#10;&#10;        # Count documents by status&#10;        doc_counts = {}&#10;        for doc in documents:&#10;            status = doc.get('status', 'unknown')&#10;            doc_counts[status] = doc_counts.get(status, 0) + 1&#10;&#10;        return jsonify({&#10;            'case_id': case.get('id'),&#10;            'token': token[:12] + '...',&#10;            'hoa_name': case.get('hoa_name'),&#10;            'violation_type': case.get('violation_type'),&#10;            'documents': {&#10;                'total': len(documents),&#10;                'by_status': doc_counts&#10;            },&#10;            'preview_available': bool(case.get('preview_text'))&#10;        }), 200&#10;&#10;    except Exception as e:&#10;        logger.error(f&quot;Error in case status endpoint: {str(e)}&quot;)&#10;        return jsonify({'error': str(e)}), 500&#10;&#10;&#10;# Stripe webhook endpoints&#10;@app.route('/webhooks/stripe', methods=['POST'])&#10;def stripe_webhook():&#10;    &quot;&quot;&quot;Handle Stripe webhook events.&quot;&quot;&quot;&#10;    try:&#10;        payload = request.get_data()&#10;        sig_header = request.headers.get('Stripe-Signature')&#10;&#10;        if not STRIPE_WEBHOOK_SECRET:&#10;            logger.error(&quot;Stripe webhook secret not configured&quot;)&#10;            return jsonify({'error': 'Webhook not configured'}), 500&#10;&#10;        # Verify webhook signature&#10;        try:&#10;            event = stripe.Webhook.construct_event(&#10;                payload, sig_header, STRIPE_WEBHOOK_SECRET&#10;            )&#10;        except ValueError as e:&#10;            logger.error(f&quot;Invalid payload: {e}&quot;)&#10;            return jsonify({'error': 'Invalid payload'}), 400&#10;        except stripe.error.SignatureVerificationError as e:&#10;            logger.error(f&quot;Invalid signature: {e}&quot;)&#10;            return jsonify({'error': 'Invalid signature'}), 400&#10;&#10;        # Handle the event&#10;        if event['type'] == 'checkout.session.completed':&#10;            session = event['data']['object']&#10;            handle_successful_payment(session)&#10;        else:&#10;            logger.info(f&quot;Unhandled Stripe event type: {event['type']}&quot;)&#10;&#10;        return jsonify({'received': True}), 200&#10;&#10;    except Exception as e:&#10;        logger.error(f&quot;Error handling Stripe webhook: {str(e)}&quot;)&#10;        return jsonify({'error': str(e)}), 500&#10;&#10;&#10;def handle_successful_payment(session):&#10;    &quot;&quot;&quot;Handle successful Stripe payment.&quot;&quot;&quot;&#10;    try:&#10;        customer_email = session.get('customer_details', {}).get('email')&#10;        metadata = session.get('metadata', {})&#10;&#10;        logger.info(f&quot;Processing successful payment for email: {customer_email}&quot;)&#10;&#10;        # You can add logic here to update user permissions, send confirmation emails, etc.&#10;&#10;    except Exception as e:&#10;        logger.error(f&quot;Error handling successful payment: {str(e)}&quot;)&#10;&#10;&#10;@app.route('/create-checkout-session', methods=['POST'])&#10;def create_checkout_session():&#10;    &quot;&quot;&quot;Create Stripe checkout session.&quot;&quot;&quot;&#10;    try:&#10;        if not STRIPE_SECRET_KEY or not STRIPE_PRICE_ID:&#10;            return jsonify({'error': 'Stripe not configured'}), 500&#10;&#10;        data = request.get_json()&#10;&#10;        session = stripe.checkout.Session.create(&#10;            payment_method_types=['card'],&#10;            line_items=[{&#10;                'price': STRIPE_PRICE_ID,&#10;                'quantity': 1,&#10;            }],&#10;            mode='payment',&#10;            success_url=f&quot;{SITE_URL}/success?session_id={{CHECKOUT_SESSION_ID}}&quot;,&#10;            cancel_url=f&quot;{SITE_URL}/cancel&quot;,&#10;            customer_email=data.get('email') if data else None,&#10;        )&#10;&#10;        return jsonify({'checkout_url': session.url}), 200&#10;&#10;    except Exception as e:&#10;        logger.error(f&quot;Error creating checkout session: {str(e)}&quot;)&#10;        return jsonify({'error': str(e)}), 500&#10;&#10;&#10;# Email functionality&#10;def send_email(to_email: str, subject: str, body: str, html_body: str = None) -&gt; bool:&#10;    &quot;&quot;&quot;Send email via SMTP.&quot;&quot;&quot;&#10;    try:&#10;        if not all([SMTP_HOST, SMTP_USER, SMTP_PASS]):&#10;            logger.warning(&quot;SMTP not configured - cannot send email&quot;)&#10;            return False&#10;&#10;        msg = MIMEMultipart('alternative')&#10;        msg['From'] = SMTP_FROM&#10;        msg['To'] = to_email&#10;        msg['Subject'] = subject&#10;&#10;        # Add text part&#10;        text_part = MIMEText(body, 'plain')&#10;        msg.attach(text_part)&#10;&#10;        # Add HTML part if provided&#10;        if html_body:&#10;            html_part = MIMEText(html_body, 'html')&#10;            msg.attach(html_part)&#10;&#10;        # Send email&#10;        with smtplib.SMTP(SMTP_HOST, SMTP_PORT) as server:&#10;            server.starttls()&#10;            server.login(SMTP_USER, SMTP_PASS)&#10;            server.sendmail(SMTP_FROM, to_email, msg.as_string())&#10;&#10;        logger.info(f&quot;Email sent successfully to {to_email}&quot;)&#10;        return True&#10;&#10;    except Exception as e:&#10;        logger.error(f&quot;Failed to send email to {to_email}: {str(e)}&quot;)&#10;        return False&#10;&#10;&#10;if __name__ == '__main__':&#10;    port = int(os.environ.get('PORT', 5000))&#10;    debug = os.environ.get('FLASK_DEBUG', 'False').lower() == 'true'&#10;    app.run(host='0.0.0.0', port=port, debug=debug)&#10;" />
              <option name="updatedContent" value="# Flask app with CORS fixes deployed on January 18, 2026&#10;import os&#10;import io&#10;import json&#10;import logging&#10;from typing import Dict, Any, Optional, Tuple, List&#10;import re&#10;from datetime import datetime&#10;import threading&#10;import time&#10;&#10;import requests&#10;from flask import Flask, request, jsonify&#10;from flask_cors import CORS&#10;from pypdf import PdfReader&#10;import stripe&#10;&#10;import smtplib&#10;from email.mime.text import MIMEText&#10;from email.mime.multipart import MIMEMultipart&#10;&#10;# Image processing and OCR imports&#10;from PIL import Image&#10;import pytesseract&#10;&#10;&#10;# Configure logging&#10;logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')&#10;logger = logging.getLogger(__name__)&#10;&#10;app = Flask(__name__)&#10;CORS(app)  # Enable CORS for all routes&#10;&#10;# Configuration&#10;SUPABASE_URL = os.environ.get('SUPABASE_URL')&#10;SUPABASE_SERVICE_ROLE_KEY = os.environ.get('SUPABASE_SERVICE_ROLE_KEY')&#10;DOC_EXTRACT_WEBHOOK_SECRET = os.environ.get('DOC_EXTRACT_WEBHOOK_SECRET')&#10;&#10;# Stripe Configuration&#10;STRIPE_SECRET_KEY = os.environ.get('STRIPE_SECRET_KEY')&#10;STRIPE_WEBHOOK_SECRET = os.environ.get('STRIPE_WEBHOOK_SECRET')&#10;STRIPE_PRICE_ID = os.environ.get('STRIPE_PRICE_ID')&#10;SITE_URL = os.environ.get('SITE_URL', 'https://disputemyhoa.com')&#10;&#10;# OpenAI Configuration&#10;OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')&#10;&#10;# Configure Stripe&#10;if STRIPE_SECRET_KEY:&#10;    stripe.api_key = STRIPE_SECRET_KEY&#10;&#10;# SMTP Configuration - Optional, only needed for email functionality&#10;SMTP_HOST = os.environ.get(&quot;SMTP_HOST&quot;)&#10;SMTP_PORT = int(os.environ.get(&quot;SMTP_PORT&quot;, &quot;587&quot;))&#10;# Clean SMTP credentials to handle non-ASCII characters like non-breaking spaces&#10;SMTP_USER = (os.environ.get(&quot;SMTP_USER&quot;) or &quot;&quot;).strip().replace('\xa0', ' ')&#10;SMTP_PASS = (os.environ.get(&quot;SMTP_PASS&quot;) or &quot;&quot;).strip().replace('\xa0', ' ')&#10;SMTP_FROM = os.environ.get(&quot;SMTP_FROM&quot;, &quot;support@disputemyhoa.com&quot;)&#10;&#10;SMTP_SENDER_WEBHOOK_SECRET = os.environ.get(&quot;SMTP_SENDER_WEBHOOK_SECRET&quot;)&#10;SMTP_SENDER_WEBHOOK_URL = os.environ.get(&quot;SMTP_SENDER_WEBHOOK_URL&quot;)&#10;&#10;# Request timeouts&#10;TIMEOUT = (5, 60)  # (connect, read)&#10;&#10;def supabase_headers() -&gt; Dict[str, str]:&#10;    &quot;&quot;&quot;Return headers for Supabase API requests.&quot;&quot;&quot;&#10;    return {&#10;        'apikey': SUPABASE_SERVICE_ROLE_KEY,&#10;        'Authorization': f'Bearer {SUPABASE_SERVICE_ROLE_KEY}',&#10;        'Content-Type': 'application/json'&#10;    }&#10;&#10;def fetch_ready_documents_by_token(token: str, limit: int = 3) -&gt; List[Dict]:&#10;    &quot;&quot;&quot;Query Supabase for ready documents by case token.&quot;&quot;&quot;&#10;    try:&#10;        url = f&quot;{SUPABASE_URL}/rest/v1/dmhoa_documents&quot;&#10;        params = {&#10;            'token': f'eq.{token}',&#10;            'status': 'eq.ready',&#10;            'select': 'id,filename,mime_type,page_count,char_count,updated_at,extracted_text',&#10;            'order': 'updated_at.desc',&#10;            'limit': str(limit)&#10;        }&#10;        headers = supabase_headers()&#10;&#10;        response = requests.get(url, params=params, headers=headers, timeout=TIMEOUT)&#10;        response.raise_for_status()&#10;&#10;        documents = response.json()&#10;        logger.info(f&quot;Found {len(documents)} ready documents for token {token[:12]}...&quot;)&#10;        return documents&#10;&#10;    except Exception as e:&#10;        logger.error(f&quot;Failed to fetch ready documents for token {token[:12]}...: {str(e)}&quot;)&#10;        return []&#10;&#10;def fetch_any_documents_status_by_token(token: str) -&gt; List[Dict]:&#10;    &quot;&quot;&quot;Check for any documents (including processing/pending) by token.&quot;&quot;&quot;&#10;    try:&#10;        url = f&quot;{SUPABASE_URL}/rest/v1/dmhoa_documents&quot;&#10;        params = {&#10;            'token': f'eq.{token}',&#10;            'select': 'id,status,updated_at',&#10;            'order': 'updated_at.desc'&#10;        }&#10;        headers = supabase_headers()&#10;&#10;        response = requests.get(url, params=params, headers=headers, timeout=TIMEOUT)&#10;        response.raise_for_status()&#10;&#10;        return response.json()&#10;&#10;    except Exception as e:&#10;        logger.error(f&quot;Failed to fetch document status for token {token[:12]}...: {str(e)}&quot;)&#10;        return []&#10;&#10;def summarize_doc_text_with_openai(token: str, raw_text: str) -&gt; str:&#10;    &quot;&quot;&quot;Summarize document text using OpenAI gpt-4o-mini.&quot;&quot;&quot;&#10;    try:&#10;        # Clip text to avoid token limits&#10;        clipped_text = raw_text[:12000] if raw_text else &quot;&quot;&#10;&#10;        if not clipped_text.strip():&#10;            return &quot;&quot;&#10;&#10;        headers = {&#10;            'Authorization': f'Bearer {OPENAI_API_KEY}',&#10;            'Content-Type': 'application/json'&#10;        }&#10;&#10;        prompt = &quot;&quot;&quot;Summarize the HOA notice into: (a) alleged violation, (b) what HOA demands, (c) deadlines/fines/next actions, (d) evidence/rules cited. Be factual, quote exact deadlines/amounts when present. 8-12 bullets max.&quot;&quot;&quot;&#10;&#10;        data = {&#10;            &quot;model&quot;: &quot;gpt-4o-mini&quot;,&#10;            &quot;messages&quot;: [&#10;                {&#10;                    &quot;role&quot;: &quot;system&quot;,&#10;                    &quot;content&quot;: &quot;You are a document summarizer for HOA notices. Extract key facts concisely.&quot;&#10;                },&#10;                {&#10;                    &quot;role&quot;: &quot;user&quot;,&#10;                    &quot;content&quot;: f&quot;{prompt}\n\nDocument text:\n{clipped_text}&quot;&#10;                }&#10;            ],&#10;            &quot;temperature&quot;: 0.3,&#10;            &quot;max_tokens&quot;: 800&#10;        }&#10;&#10;        response = requests.post(&#10;            'https://api.openai.com/v1/chat/completions',&#10;            headers=headers,&#10;            json=data,&#10;            timeout=30&#10;        )&#10;        response.raise_for_status()&#10;&#10;        result = response.json()&#10;        summary = result['choices'][0]['message']['content'].strip()&#10;&#10;        # Limit summary length&#10;        if len(summary) &gt; 1200:&#10;            summary = summary[:1200] + &quot;...&quot;&#10;&#10;        logger.info(f&quot;Successfully summarized document for token {token[:12]}...&quot;)&#10;        return summary&#10;&#10;    except Exception as e:&#10;        logger.error(f&quot;Failed to summarize document text for token {token[:12]}...: {str(e)}&quot;)&#10;        # Fallback to clipped excerpt&#10;        return raw_text[:1200] + &quot;...&quot; if len(raw_text) &gt; 1200 else raw_text&#10;&#10;def build_doc_brief(docs: List[Dict]) -&gt; Dict:&#10;    &quot;&quot;&quot;Build document brief from ready documents.&quot;&quot;&quot;&#10;    if not docs:&#10;        return {&#10;            &quot;doc_status&quot;: &quot;none&quot;,&#10;            &quot;doc_count&quot;: 0,&#10;            &quot;sources&quot;: [],&#10;            &quot;brief_text&quot;: &quot;&quot;&#10;        }&#10;&#10;    sources = []&#10;    raw_texts = []&#10;&#10;    for doc in docs:&#10;        sources.append({&#10;            &quot;filename&quot;: doc.get(&quot;filename&quot;, &quot;unknown&quot;),&#10;            &quot;page_count&quot;: doc.get(&quot;page_count&quot;, 0),&#10;            &quot;char_count&quot;: doc.get(&quot;char_count&quot;, 0)&#10;        })&#10;&#10;        # Extract text with limits&#10;        extracted_text = doc.get(&quot;extracted_text&quot;, &quot;&quot;)&#10;        if extracted_text:&#10;            # Limit per document to 6000 chars&#10;            doc_text = extracted_text[:6000]&#10;            raw_texts.append(doc_text)&#10;&#10;    if not raw_texts:&#10;        return {&#10;            &quot;doc_status&quot;: &quot;ready&quot;,&#10;            &quot;doc_count&quot;: len(docs),&#10;            &quot;sources&quot;: sources,&#10;            &quot;brief_text&quot;: &quot;&quot;&#10;        }&#10;&#10;    # Combine texts with total limit of 12000 chars&#10;    combined_text = &quot; &quot;.join(raw_texts)&#10;    if len(combined_text) &gt; 12000:&#10;        combined_text = combined_text[:12000]&#10;&#10;    # Try to summarize with OpenAI&#10;    token = docs[0].get(&quot;token&quot;, &quot;unknown&quot;) if docs else &quot;unknown&quot;&#10;    brief_text = summarize_doc_text_with_openai(token, combined_text)&#10;&#10;    return {&#10;        &quot;doc_status&quot;: &quot;ready&quot;,&#10;        &quot;doc_count&quot;: len(docs),&#10;        &quot;sources&quot;: sources,&#10;        &quot;brief_text&quot;: brief_text&#10;    }&#10;&#10;&#10;def fetch_document_status(document_id: str) -&gt; Optional[Dict[str, Any]]:&#10;    &quot;&quot;&quot;Fetch current document status from Supabase.&quot;&quot;&quot;&#10;    try:&#10;        url = f&quot;{SUPABASE_URL}/rest/v1/dmhoa_documents&quot;&#10;        params = {&#10;            'id': f'eq.{document_id}',&#10;            'select': 'id,token,status'&#10;        }&#10;        headers = supabase_headers()&#10;&#10;        response = requests.get(url, params=params, headers=headers, timeout=TIMEOUT)&#10;        response.raise_for_status()&#10;&#10;        data = response.json()&#10;        return data[0] if data else None&#10;&#10;    except Exception as e:&#10;        logger.error(f&quot;Failed to fetch document status for {document_id}: {str(e)}&quot;)&#10;        return None&#10;&#10;def update_document(document_id: str, token: str, updates: Dict[str, Any]) -&gt; bool:&#10;    &quot;&quot;&quot;Update document in Supabase database.&quot;&quot;&quot;&#10;    try:&#10;        url = f&quot;{SUPABASE_URL}/rest/v1/dmhoa_documents&quot;&#10;        params = {&#10;            'id': f'eq.{document_id}',&#10;            'token': f'eq.{token}'&#10;        }&#10;        headers = supabase_headers()&#10;        headers['Prefer'] = 'return=representation'&#10;&#10;        response = requests.patch(url, params=params, headers=headers,&#10;                                json=updates, timeout=TIMEOUT)&#10;        response.raise_for_status()&#10;&#10;        logger.info(f&quot;Updated document {document_id} with: {updates}&quot;)&#10;        return True&#10;&#10;    except Exception as e:&#10;        logger.error(f&quot;Failed to update document {document_id}: {str(e)}&quot;)&#10;        return False&#10;&#10;def download_storage_object(bucket: str, path: str) -&gt; Optional[bytes]:&#10;    &quot;&quot;&quot;Download file from Supabase Storage.&quot;&quot;&quot;&#10;    try:&#10;        url = f&quot;{SUPABASE_URL}/storage/v1/object/{bucket}/{path}&quot;&#10;        headers = {&#10;            'apikey': SUPABASE_SERVICE_ROLE_KEY,&#10;            'Authorization': f'Bearer {SUPABASE_SERVICE_ROLE_KEY}'&#10;        }&#10;&#10;        response = requests.get(url, headers=headers, timeout=TIMEOUT)&#10;&#10;        if response.status_code == 404:&#10;            logger.error(f&quot;File not found: {bucket}/{path}&quot;)&#10;            return None&#10;&#10;        response.raise_for_status()&#10;&#10;        logger.info(f&quot;Downloaded {len(response.content)} bytes from {bucket}/{path}&quot;)&#10;        return response.content&#10;&#10;    except Exception as e:&#10;        logger.error(f&quot;Failed to download {bucket}/{path}: {str(e)}&quot;)&#10;        return None&#10;&#10;def extract_pdf_text(pdf_bytes: bytes) -&gt; Tuple[str, int, int, Optional[str]]:&#10;    &quot;&quot;&quot;&#10;    Extract text from PDF bytes.&#10;    Returns: (extracted_text, page_count, char_count, error_message)&#10;    &quot;&quot;&quot;&#10;    try:&#10;        reader = PdfReader(io.BytesIO(pdf_bytes))&#10;        page_count = len(reader.pages)&#10;&#10;        text_parts = []&#10;        for page in reader.pages:&#10;            try:&#10;                page_text = page.extract_text() or &quot;&quot;&#10;                text_parts.append(page_text)&#10;            except Exception as e:&#10;                logger.warning(f&quot;Failed to extract text from page: {str(e)}&quot;)&#10;                text_parts.append(&quot;&quot;)&#10;&#10;        extracted_text = &quot;\n\n&quot;.join(text_parts)&#10;        char_count = len(extracted_text)&#10;&#10;        # Check if text is empty or only whitespace&#10;        if not extracted_text or extracted_text.strip() == &quot;&quot;:&#10;            return &quot;&quot;, page_count, 0, &quot;No text layer found - document may be scanned and require OCR&quot;&#10;&#10;        logger.info(f&quot;Extracted {char_count} characters from {page_count} pages&quot;)&#10;        return extracted_text, page_count, char_count, None&#10;&#10;    except Exception as e:&#10;        error_msg = f&quot;Failed to extract text from PDF: {str(e)}&quot;&#10;        logger.error(error_msg)&#10;        return &quot;&quot;, 0, 0, error_msg&#10;&#10;&#10;def extract_image_text(image_bytes: bytes, filename: str = &quot;&quot;) -&gt; Tuple[str, int, int, Optional[str]]:&#10;    &quot;&quot;&quot;&#10;    Extract text from image bytes using OCR.&#10;    Returns: (extracted_text, page_count=1, char_count, error_message)&#10;    &quot;&quot;&quot;&#10;    try:&#10;        # Open image from bytes&#10;        image = Image.open(io.BytesIO(image_bytes))&#10;&#10;        # Convert to RGB if necessary (some formats like RGBA or P need conversion)&#10;        if image.mode not in ('RGB', 'L'):&#10;            image = image.convert('RGB')&#10;&#10;        logger.info(f&quot;Processing image: {image.size[0]}x{image.size[1]} pixels, mode: {image.mode}&quot;)&#10;&#10;        # Set TESSDATA_PREFIX if not already set (for Heroku compatibility)&#10;        if 'TESSDATA_PREFIX' not in os.environ:&#10;            possible_paths = [&#10;                '/usr/share/tesseract-ocr/5/tessdata',&#10;                '/usr/share/tesseract-ocr/tessdata',&#10;                '/usr/share/tesseract-ocr/4.00/tessdata',&#10;                '/usr/share/tesseract-ocr/4/tessdata',&#10;                '/usr/share/tesseract-ocr/tessdata',&#10;                '/usr/share/tessdata',&#10;                '/app/.apt/usr/share/tesseract-ocr/5/tessdata',&#10;                '/app/.apt/usr/share/tesseract-ocr/tessdata'&#10;            ]&#10;            for path in possible_paths:&#10;                if os.path.exists(path):&#10;                    os.environ['TESSDATA_PREFIX'] = path&#10;                    logger.info(f&quot;Set TESSDATA_PREFIX to: {path}&quot;)&#10;                    break&#10;            else:&#10;                logger.warning(&quot;Could not find tessdata directory in any expected location&quot;)&#10;&#10;        # Try multiple OCR configurations for better compatibility&#10;        configs_to_try = [&#10;            r'--oem 1 --psm 1 -l eng',   # Automatic page segmentation with OSD&#10;            r'--oem 1 --psm 3 -l eng',   # Fully automatic page segmentation, but no OSD&#10;            r'--oem 1 --psm 4 -l eng',   # Assume a single column of text of variable sizes&#10;            r'--oem 1 --psm 6 -l eng',   # Assume a single uniform block of text&#10;            r'--oem 3 --psm 1 -l eng',   # LSTM with automatic page segmentation&#10;            r'--oem 3 --psm 3 -l eng',   # LSTM with fully automatic page segmentation&#10;            r'--oem 3 --psm 6 -l eng',   # LSTM standard config&#10;            r'--oem 3 --psm 11 -l eng',  # Sparse text - find as much text as possible&#10;            r'--oem 3 --psm 12 -l eng',  # Sparse text with OSD&#10;            r'--psm 6',                  # No language specified fallback&#10;        ]&#10;&#10;        extracted_text = &quot;&quot;&#10;        best_text = &quot;&quot;&#10;        best_char_count = 0&#10;        last_error = None&#10;&#10;        for config in configs_to_try:&#10;            try:&#10;                logger.info(f&quot;Trying OCR with config: {config}&quot;)&#10;                current_text = pytesseract.image_to_string(image, config=config)&#10;                current_text = current_text.strip()&#10;                char_count = len(current_text)&#10;&#10;                # Keep track of the best result (most text that looks reasonable)&#10;                if char_count &gt; best_char_count:&#10;                    # Basic heuristic: prefer results with more alphanumeric content&#10;                    alphanumeric_ratio = sum(c.isalnum() or c.isspace() for c in current_text) / max(len(current_text), 1)&#10;                    if alphanumeric_ratio &gt; 0.3:  # At least 30% should be readable characters&#10;                        best_text = current_text&#10;                        best_char_count = char_count&#10;                        logger.info(f&quot;New best result: {char_count} chars, {alphanumeric_ratio:.2f} alphanumeric ratio&quot;)&#10;&#10;                # If we got a decent amount of readable text, we can stop&#10;                if char_count &gt; 50 and best_text:&#10;                    extracted_text = best_text&#10;                    break&#10;&#10;            except Exception as e:&#10;                last_error = str(e)&#10;                logger.warning(f&quot;OCR config failed: {config}, error: {str(e)}&quot;)&#10;                continue&#10;&#10;        # Use the best result we found&#10;        if not extracted_text and best_text:&#10;            extracted_text = best_text&#10;&#10;        # Clean up the extracted text&#10;        extracted_text = extracted_text.strip()&#10;        char_count = len(extracted_text)&#10;&#10;        if char_count == 0:&#10;            error_msg = f&quot;No text found in image - image may be blank or contain no readable text. Last OCR error: {last_error}&quot;&#10;            return &quot;&quot;, 1, 0, error_msg&#10;&#10;        logger.info(f&quot;OCR extracted {char_count} characters from image {filename}&quot;)&#10;        return extracted_text, 1, char_count, None&#10;&#10;    except Exception as e:&#10;        error_msg = f&quot;Failed to extract text from image: {str(e)}&quot;&#10;        logger.error(error_msg)&#10;        return &quot;&quot;, 0, 0, error_msg&#10;&#10;&#10;def is_image_file(filename: str, mime_type: str = &quot;&quot;) -&gt; bool:&#10;    &quot;&quot;&quot;Check if file is a supported image format.&quot;&quot;&quot;&#10;    image_extensions = {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.tif', '.webp'}&#10;    image_mime_types = {&#10;        'image/jpeg', 'image/jpg', 'image/png', 'image/gif',&#10;        'image/bmp', 'image/tiff', 'image/webp'&#10;    }&#10;&#10;    # Check by file extension&#10;    if filename:&#10;        ext = os.path.splitext(filename.lower())[1]&#10;        if ext in image_extensions:&#10;            return True&#10;&#10;    # Check by MIME type&#10;    if mime_type and mime_type.lower() in image_mime_types:&#10;        return True&#10;&#10;    return False&#10;&#10;&#10;def is_pdf_file(filename: str, mime_type: str = &quot;&quot;) -&gt; bool:&#10;    &quot;&quot;&quot;Check if file is a PDF.&quot;&quot;&quot;&#10;    if filename and filename.lower().endswith('.pdf'):&#10;        return True&#10;    if mime_type and mime_type.lower() == 'application/pdf':&#10;        return True&#10;    return False&#10;&#10;&#10;@app.route('/health', methods=['GET'])&#10;def health_check():&#10;    &quot;&quot;&quot;Health check endpoint.&quot;&quot;&quot;&#10;    return jsonify({'status': 'healthy'}), 200&#10;&#10;&#10;@app.route('/debug/env', methods=['GET'])&#10;def debug_env():&#10;    &quot;&quot;&quot;Return presence of critical env vars (gated by DOC_EXTRACT_WEBHOOK_SECRET).&#10;&#10;    This does NOT return any secret values, only boolean flags indicating whether each&#10;    required configuration item is set. Intended for quick diagnostics on deployed app.&#10;    &quot;&quot;&quot;&#10;    secret = request.headers.get('X-Webhook-Secret')&#10;    if not secret or secret != DOC_EXTRACT_WEBHOOK_SECRET:&#10;        logger.warning(&quot;Unauthorized request to /debug/env&quot;)&#10;        return jsonify({'error': 'Unauthorized'}), 401&#10;&#10;    keys = [&#10;        'SUPABASE_URL', 'SUPABASE_SERVICE_ROLE_KEY', 'DOC_EXTRACT_WEBHOOK_SECRET',&#10;        'SMTP_HOST', 'SMTP_PORT', 'SMTP_USER', 'SMTP_PASS', 'SMTP_FROM', 'SMTP_SENDER_WEBHOOK_SECRET'&#10;    ]&#10;    presence = {k: bool(os.environ.get(k)) for k in keys}&#10;    logger.info(f&quot;/debug/env requested; presence: { {k: presence[k] for k in presence} }&quot;)&#10;    return jsonify({'env_presence': presence}), 200&#10;&#10;@app.route('/webhooks/doc-extract', methods=['POST'])&#10;def doc_extract_webhook():&#10;    &quot;&quot;&quot;Main webhook endpoint for document extraction.&quot;&quot;&quot;&#10;    # Validate webhook secret&#10;    webhook_secret = request.headers.get('X-Webhook-Secret')&#10;    if not webhook_secret or webhook_secret != DOC_EXTRACT_WEBHOOK_SECRET:&#10;        logger.warning(&quot;Invalid or missing webhook secret&quot;)&#10;        return jsonify({'error': 'Unauthorized'}), 401&#10;&#10;    document_id = None&#10;    token = None&#10;&#10;    try:&#10;        # Parse JSON body&#10;        data = request.get_json()&#10;        if not data:&#10;            return jsonify({'error': 'Invalid JSON body'}), 400&#10;&#10;        # Validate required fields&#10;        required_fields = ['token', 'document_id', 'bucket', 'path']&#10;        missing_fields = [field for field in required_fields if not data.get(field)]&#10;        if missing_fields:&#10;            return jsonify({&#10;                'error': f'Missing required fields: {&quot;, &quot;.join(missing_fields)}'&#10;            }), 400&#10;&#10;        token = data['token']&#10;        document_id = data['document_id']&#10;        bucket = data['bucket']&#10;        path = data['path']&#10;        filename = data.get('filename', '') or ''  # Handle null values&#10;        mime_type = data.get('mime_type', '') or ''  # Handle null values&#10;&#10;        logger.info(f&quot;Processing document extraction - ID: {document_id}, Token: {token[:8]}...&quot;)&#10;&#10;        # Check if document is already processed&#10;        current_doc = fetch_document_status(document_id)&#10;        if current_doc and current_doc.get('status') == 'ready':&#10;            logger.info(f&quot;Document {document_id} already processed&quot;)&#10;            return jsonify({&#10;                'message': 'Document already processed',&#10;                'document_id': document_id,&#10;                'status': 'ready'&#10;            }), 200&#10;&#10;        # Mark document as processing&#10;        if not update_document(document_id, token, {'status': 'processing'}):&#10;            return jsonify({&#10;                'error': 'Failed to update document status to processing',&#10;                'document_id': document_id&#10;            }), 500&#10;&#10;        # Download file from Supabase Storage&#10;        file_bytes = download_storage_object(bucket, path)&#10;        if file_bytes is None:&#10;            error_msg = f&quot;Failed to download file from {bucket}/{path}&quot;&#10;            update_document(document_id, token, {&#10;                'status': 'failed',&#10;                'error': error_msg[:2000]&#10;            })&#10;            return jsonify({&#10;                'error': error_msg,&#10;                'document_id': document_id&#10;            }), 500&#10;&#10;        # Determine file type with multiple fallback strategies&#10;        logger.info(f&quot;Initial file detection - filename: '{filename}', mime_type: '{mime_type}'&quot;)&#10;&#10;        # Strategy 1: If filename is empty/null, extract from path&#10;        if not filename or filename.lower() == 'null':&#10;            filename = os.path.basename(path)&#10;            logger.info(f&quot;Extracted filename from path: '{filename}'&quot;)&#10;&#10;        # Strategy 2: If mime_type is empty/null, guess from filename&#10;        if not mime_type or mime_type.lower() == 'null':&#10;            if filename:&#10;                ext = os.path.splitext(filename.lower())[1]&#10;                mime_type_map = {&#10;                    '.pdf': 'application/pdf',&#10;                    '.jpg': 'image/jpeg', '.jpeg': 'image/jpeg',&#10;                    '.png': 'image/png', '.gif': 'image/gif',&#10;                    '.bmp': 'image/bmp', '.tiff': 'image/tiff', '.tif': 'image/tiff',&#10;                    '.webp': 'image/webp'&#10;                }&#10;                mime_type = mime_type_map.get(ext, '')&#10;                logger.info(f&quot;Guessed MIME type from extension '{ext}': '{mime_type}'&quot;)&#10;&#10;        # Strategy 3: If still no filename, try extracting just the filename from the full path&#10;        if not filename:&#10;            # Handle paths like &quot;dmhoa-docs/case_xxx/original/image.jpg&quot;&#10;            path_parts = path.split('/')&#10;            if path_parts:&#10;                filename = path_parts[-1]  # Get the last part&#10;                logger.info(f&quot;Extracted filename from path parts: '{filename}'&quot;)&#10;&#10;        # Strategy 4: If we still have no clear type, try to detect from file content&#10;        detected_type = None&#10;        if not (is_pdf_file(filename, mime_type) or is_image_file(filename, mime_type)):&#10;            # Check file magic bytes as last resort&#10;            if file_bytes and len(file_bytes) &gt;= 4:&#10;                # PDF magic bytes&#10;                if file_bytes.startswith(b'%PDF'):&#10;                    detected_type = 'pdf'&#10;                    logger.info(&quot;Detected PDF from file magic bytes&quot;)&#10;                # JPEG magic bytes&#10;                elif file_bytes.startswith(b'\xff\xd8\xff'):&#10;                    detected_type = 'image'&#10;                    mime_type = 'image/jpeg'&#10;                    logger.info(&quot;Detected JPEG from file magic bytes&quot;)&#10;                # PNG magic bytes&#10;                elif file_bytes.startswith(b'\x89PNG'):&#10;                    detected_type = 'image'&#10;                    mime_type = 'image/png'&#10;                    logger.info(&quot;Detected PNG from file magic bytes&quot;)&#10;&#10;        logger.info(f&quot;Final file detection - filename: '{filename}', mime_type: '{mime_type}', detected_type: {detected_type}&quot;)&#10;&#10;        # Process based on detected file type&#10;        if is_pdf_file(filename, mime_type) or detected_type == 'pdf':&#10;            logger.info(f&quot;Processing as PDF: {filename}&quot;)&#10;            extracted_text, page_count, char_count, extraction_error = extract_pdf_text(file_bytes)&#10;        elif is_image_file(filename, mime_type) or detected_type == 'image':&#10;            logger.info(f&quot;Processing as image: {filename}&quot;)&#10;            extracted_text, page_count, char_count, extraction_error = extract_image_text(file_bytes, filename)&#10;        else:&#10;            error_msg = f&quot;Unsupported file type: {filename} (MIME: {mime_type})&quot;&#10;            logger.error(error_msg)&#10;            update_document(document_id, token, {&#10;                'status': 'failed',&#10;                'error': error_msg[:2000]&#10;            })&#10;            return jsonify({&#10;                'error': error_msg,&#10;                'document_id': document_id&#10;            }), 400&#10;&#10;        # Update document with extraction results&#10;        update_data = {&#10;            'status': 'ready',&#10;            'extracted_text': extracted_text[:50000],  # Limit text size&#10;            'page_count': page_count,&#10;            'char_count': char_count&#10;        }&#10;&#10;        if extraction_error:&#10;            update_data['error'] = extraction_error[:2000]&#10;            logger.warning(f&quot;Document {document_id} processed with warning: {extraction_error}&quot;)&#10;        else:&#10;            logger.info(f&quot;Document {document_id} processed successfully: {char_count} chars, {page_count} pages&quot;)&#10;&#10;        if not update_document(document_id, token, update_data):&#10;            return jsonify({&#10;                'error': 'Failed to save extraction results',&#10;                'document_id': document_id&#10;            }), 500&#10;&#10;        return jsonify({&#10;            'message': 'Document processed successfully',&#10;            'document_id': document_id,&#10;            'status': 'ready',&#10;            'page_count': page_count,&#10;            'char_count': char_count,&#10;            'has_error': bool(extraction_error)&#10;        }), 200&#10;&#10;    except Exception as e:&#10;        error_msg = f&quot;Unexpected error processing document: {str(e)}&quot;&#10;        logger.error(error_msg)&#10;&#10;        # Try to update document status to failed if we have the IDs&#10;        if document_id and token:&#10;            try:&#10;                update_document(document_id, token, {&#10;                    'status': 'failed',&#10;                    'error': error_msg[:2000]&#10;                })&#10;            except:&#10;                pass  # Don't fail the response if we can't update status&#10;&#10;        return jsonify({&#10;            'error': error_msg,&#10;            'document_id': document_id&#10;        }), 500&#10;&#10;&#10;def read_case_by_token(token: str) -&gt; Optional[Dict[str, Any]]:&#10;    &quot;&quot;&quot;Fetch case details from Supabase by token.&quot;&quot;&quot;&#10;    try:&#10;        url = f&quot;{SUPABASE_URL}/rest/v1/dmhoa_cases&quot;&#10;        params = {&#10;            'token': f'eq.{token}',&#10;            'select': '*'&#10;        }&#10;        headers = supabase_headers()&#10;&#10;        response = requests.get(url, params=params, headers=headers, timeout=TIMEOUT)&#10;        response.raise_for_status()&#10;&#10;        cases = response.json()&#10;        if not cases:&#10;            logger.warning(f&quot;No case found for token: {token[:12]}...&quot;)&#10;            return None&#10;&#10;        case = cases[0]&#10;        logger.info(f&quot;Found case for token {token[:12]}...: ID {case.get('id')}&quot;)&#10;        return case&#10;&#10;    except Exception as e:&#10;        logger.error(f&quot;Failed to fetch case for token {token[:12]}...: {str(e)}&quot;)&#10;        return None&#10;&#10;&#10;def read_active_preview(case_id: str) -&gt; Optional[Dict]:&#10;    &quot;&quot;&quot;Read the active preview for a case from dmhoa_case_previews table.&quot;&quot;&quot;&#10;    try:&#10;        url = f&quot;{SUPABASE_URL}/rest/v1/dmhoa_case_previews&quot;&#10;        params = {&#10;            'case_id': f'eq.{case_id}',&#10;            'is_active': 'eq.true',&#10;            'select': '*',&#10;            'order': 'created_at.desc',&#10;            'limit': '1'&#10;        }&#10;        headers = supabase_headers()&#10;&#10;        response = requests.get(url, params=params, headers=headers, timeout=TIMEOUT)&#10;        response.raise_for_status()&#10;&#10;        previews = response.json()&#10;        if previews:&#10;            logger.info(f&quot;Found active preview for case {case_id}&quot;)&#10;            return previews[0]&#10;&#10;        logger.info(f&quot;No active preview found for case {case_id}&quot;)&#10;        return None&#10;&#10;    except Exception as e:&#10;        logger.error(f&quot;Failed to read active preview for case {case_id}: {str(e)}&quot;)&#10;        return None&#10;&#10;&#10;def deactivate_previews(case_id: str) -&gt; bool:&#10;    &quot;&quot;&quot;Deactivate all existing previews for a case.&quot;&quot;&quot;&#10;    try:&#10;        url = f&quot;{SUPABASE_URL}/rest/v1/dmhoa_case_previews&quot;&#10;        params = {'case_id': f'eq.{case_id}'}&#10;        headers = supabase_headers()&#10;&#10;        update_data = {'is_active': False}&#10;&#10;        response = requests.patch(url, params=params, headers=headers,&#10;                                json=update_data, timeout=TIMEOUT)&#10;        response.raise_for_status()&#10;&#10;        logger.info(f&quot;Deactivated existing previews for case {case_id}&quot;)&#10;        return True&#10;&#10;    except Exception as e:&#10;        logger.error(f&quot;Failed to deactivate previews for case {case_id}: {str(e)}&quot;)&#10;        return False&#10;&#10;&#10;def insert_preview(case_id: str, preview_content: Dict, preview_snippet: str = None,&#10;                  prompt_version: str = None, model: str = &quot;gpt-4o-mini&quot;,&#10;                  token_input: int = None, token_output: int = None,&#10;                  cost_usd: float = None, latency_ms: int = None) -&gt; Optional[str]:&#10;    &quot;&quot;&quot;Insert a new preview into dmhoa_case_previews table.&quot;&quot;&quot;&#10;    try:&#10;        url = f&quot;{SUPABASE_URL}/rest/v1/dmhoa_case_previews&quot;&#10;        headers = supabase_headers()&#10;        headers['Prefer'] = 'return=representation'&#10;&#10;        preview_data = {&#10;            'case_id': case_id,&#10;            'preview_content': preview_content,&#10;            'preview_snippet': preview_snippet,&#10;            'prompt_version': prompt_version,&#10;            'model': model,&#10;            'token_input': token_input,&#10;            'token_output': token_output,&#10;            'cost_usd': cost_usd,&#10;            'latency_ms': latency_ms,&#10;            'is_active': True&#10;        }&#10;&#10;        # Remove None values&#10;        preview_data = {k: v for k, v in preview_data.items() if v is not None}&#10;&#10;        response = requests.post(url, headers=headers, json=preview_data, timeout=TIMEOUT)&#10;        response.raise_for_status()&#10;&#10;        result = response.json()&#10;        if result:&#10;            preview_id = result[0]['id']&#10;            logger.info(f&quot;Inserted new preview {preview_id} for case {case_id}&quot;)&#10;            return preview_id&#10;&#10;        return None&#10;&#10;    except Exception as e:&#10;        logger.error(f&quot;Failed to insert preview for case {case_id}: {str(e)}&quot;)&#10;        return None&#10;&#10;&#10;def save_case_preview_to_new_table(case_id: str, preview_text: str, doc_brief: Dict,&#10;                                  token_usage: Dict = None, latency_ms: int = None) -&gt; bool:&#10;    &quot;&quot;&quot;Save generated case preview to the new dmhoa_case_previews table.&quot;&quot;&quot;&#10;    try:&#10;        # Deactivate existing previews first&#10;        deactivate_previews(case_id)&#10;&#10;        # Prepare preview content as JSONB&#10;        preview_content = {&#10;            'preview_text': preview_text,&#10;            'doc_summary': doc_brief,&#10;            'generated_at': datetime.utcnow().isoformat()&#10;        }&#10;&#10;        # Extract first paragraph or 200 chars as snippet&#10;        preview_snippet = preview_text[:200] + &quot;...&quot; if len(preview_text) &gt; 200 else preview_text&#10;&#10;        # Extract token usage if available&#10;        token_input = token_usage.get('prompt_tokens') if token_usage else None&#10;        token_output = token_usage.get('completion_tokens') if token_usage else None&#10;        cost_usd = token_usage.get('cost_usd') if token_usage else None&#10;&#10;        # Insert new preview&#10;        preview_id = insert_preview(&#10;            case_id=case_id,&#10;            preview_content=preview_content,&#10;            preview_snippet=preview_snippet,&#10;            prompt_version=&quot;v1.0&quot;,&#10;            model=&quot;gpt-4o-mini&quot;,&#10;            token_input=token_input,&#10;            token_output=token_output,&#10;            cost_usd=cost_usd,&#10;            latency_ms=latency_ms&#10;        )&#10;&#10;        return preview_id is not None&#10;&#10;    except Exception as e:&#10;        logger.error(f&quot;Failed to save case preview to new table for case {case_id}: {str(e)}&quot;)&#10;        return False&#10;&#10;&#10;def generate_case_preview_with_openai(case_data: Dict, doc_brief: Dict) -&gt; Tuple[str, Dict, int]:&#10;    &quot;&quot;&quot;Generate case preview using OpenAI and return preview, token usage, and latency.&quot;&quot;&quot;&#10;    start_time = time.time()&#10;&#10;    try:&#10;        if not OPENAI_API_KEY:&#10;            logger.warning(&quot;OpenAI API key not configured&quot;)&#10;            return &quot;Preview generation unavailable - OpenAI not configured&quot;, {}, 0&#10;&#10;        # Extract case information&#10;        hoa_name = case_data.get('hoa_name', 'Unknown HOA')&#10;        violation_type = case_data.get('violation_type', 'Unknown violation')&#10;        case_description = case_data.get('case_description', 'No description provided')&#10;&#10;        # Get document brief&#10;        doc_text = doc_brief.get('brief_text', '')&#10;        doc_count = doc_brief.get('doc_count', 0)&#10;&#10;        # Prepare the prompt&#10;        prompt = f&quot;&quot;&quot;Generate a professional case preview for this HOA dispute:&#10;&#10;HOA: {hoa_name}&#10;Violation Type: {violation_type}&#10;Case Description: {case_description}&#10;Documents Analyzed: {doc_count}&#10;&#10;Document Summary:&#10;{doc_text[:2000] if doc_text else 'No documents available'}&#10;&#10;Create a concise case preview that includes:&#10;1. Brief case summary&#10;2. Key issues identified&#10;3. Potential legal considerations&#10;4. Recommended next steps&#10;&#10;Keep it professional, factual, and under 800 words.&quot;&quot;&quot;&#10;&#10;        headers = {&#10;            'Authorization': f'Bearer {OPENAI_API_KEY}',&#10;            'Content-Type': 'application/json'&#10;        }&#10;&#10;        data = {&#10;            &quot;model&quot;: &quot;gpt-4o-mini&quot;,&#10;            &quot;messages&quot;: [&#10;                {&#10;                    &quot;role&quot;: &quot;system&quot;,&#10;                    &quot;content&quot;: &quot;You are a legal case analyst specializing in HOA disputes. Provide professional, factual analysis.&quot;&#10;                },&#10;                {&#10;                    &quot;role&quot;: &quot;user&quot;,&#10;                    &quot;content&quot;: prompt&#10;                }&#10;            ],&#10;            &quot;temperature&quot;: 0.3,&#10;            &quot;max_tokens&quot;: 1200&#10;        }&#10;&#10;        response = requests.post(&#10;            'https://api.openai.com/v1/chat/completions',&#10;            headers=headers,&#10;            json=data,&#10;            timeout=30&#10;        )&#10;        response.raise_for_status()&#10;&#10;        result = response.json()&#10;        preview = result['choices'][0]['message']['content'].strip()&#10;&#10;        # Calculate latency&#10;        latency_ms = int((time.time() - start_time) * 1000)&#10;&#10;        # Extract token usage&#10;        token_usage = {}&#10;        if 'usage' in result:&#10;            usage = result['usage']&#10;            token_usage = {&#10;                'prompt_tokens': usage.get('prompt_tokens', 0),&#10;                'completion_tokens': usage.get('completion_tokens', 0),&#10;                'total_tokens': usage.get('total_tokens', 0)&#10;            }&#10;&#10;            # Estimate cost (approximate rates for gpt-4o-mini)&#10;            input_cost = (token_usage['prompt_tokens'] / 1000) * 0.00015  # $0.15 per 1K input tokens&#10;            output_cost = (token_usage['completion_tokens'] / 1000) * 0.0006  # $0.60 per 1K output tokens&#10;            token_usage['cost_usd'] = round(input_cost + output_cost, 6)&#10;&#10;        logger.info(f&quot;Generated case preview: {len(preview)} characters, {latency_ms}ms&quot;)&#10;        return preview, token_usage, latency_ms&#10;&#10;    except Exception as e:&#10;        latency_ms = int((time.time() - start_time) * 1000)&#10;        logger.error(f&quot;Failed to generate case preview: {str(e)}&quot;)&#10;        return f&quot;Error generating preview: {str(e)}&quot;, {}, latency_ms&#10;&#10;&#10;def save_case_preview(token: str, preview_text: str, doc_brief: Dict) -&gt; bool:&#10;    &quot;&quot;&quot;Save generated case preview to Supabase.&quot;&quot;&quot;&#10;    try:&#10;        url = f&quot;{SUPABASE_URL}/rest/v1/dmhoa_cases&quot;&#10;        params = {'token': f'eq.{token}'}&#10;        headers = supabase_headers()&#10;        headers['Prefer'] = 'return=representation'&#10;&#10;        update_data = {&#10;            'preview_text': preview_text[:10000],  # Limit size&#10;            'preview_generated_at': datetime.utcnow().isoformat(),&#10;            'doc_summary': doc_brief&#10;        }&#10;&#10;        response = requests.patch(url, params=params, headers=headers,&#10;                                json=update_data, timeout=TIMEOUT)&#10;        response.raise_for_status()&#10;&#10;        logger.info(f&quot;Saved case preview for token {token[:12]}...&quot;)&#10;        return True&#10;&#10;    except Exception as e:&#10;        logger.error(f&quot;Failed to save case preview for token {token[:12]}...: {str(e)}&quot;)&#10;        return False&#10;&#10;&#10;@app.route('/api/case-preview', methods=['POST'])&#10;def case_preview_endpoint():&#10;    &quot;&quot;&quot;Generate and return case preview with document analysis.&quot;&quot;&quot;&#10;    try:&#10;        data = request.get_json()&#10;        if not data:&#10;            return jsonify({'error': 'Invalid JSON body'}), 400&#10;&#10;        token = data.get('token')&#10;        if not token:&#10;            return jsonify({'error': 'Missing required field: token'}), 400&#10;&#10;        logger.info(f&quot;Processing case preview request for token: {token[:12]}...&quot;)&#10;&#10;        # Fetch case details&#10;        case = read_case_by_token(token)&#10;        if not case:&#10;            return jsonify({&#10;                'error': 'Case not found',&#10;                'token': token[:12] + '...'&#10;            }), 404&#10;&#10;        # Check if preview already exists and is recent&#10;        existing_preview = case.get('preview_text')&#10;        preview_generated_at = case.get('preview_generated_at')&#10;&#10;        if existing_preview and preview_generated_at:&#10;            try:&#10;                # If preview was generated less than 1 hour ago, return it&#10;                from datetime import datetime, timedelta&#10;                generated_time = datetime.fromisoformat(preview_generated_at.replace('Z', '+00:00'))&#10;                if datetime.now().replace(tzinfo=generated_time.tzinfo) - generated_time &lt; timedelta(hours=1):&#10;                    logger.info(f&quot;Returning existing preview for token {token[:12]}...&quot;)&#10;                    return jsonify({&#10;                        'preview': existing_preview,&#10;                        'cached': True,&#10;                        'generated_at': preview_generated_at&#10;                    }), 200&#10;            except:&#10;                pass  # If there's any issue with date parsing, continue with new generation&#10;&#10;        # Fetch and analyze documents&#10;        documents = fetch_ready_documents_by_token(token, limit=5)&#10;        doc_brief = build_doc_brief(documents)&#10;&#10;        # Generate case preview&#10;        preview_text, token_usage, latency_ms = generate_case_preview_with_openai(case, doc_brief)&#10;&#10;        # Save the preview to the new table using case_id&#10;        case_id = case.get('id')&#10;        if case_id and save_case_preview_to_new_table(case_id, preview_text, doc_brief, token_usage, latency_ms):&#10;            logger.info(f&quot;Successfully generated and saved preview to new table for token {token[:12]}...&quot;)&#10;        else:&#10;            # Fallback to old table for backward compatibility&#10;            if save_case_preview(token, preview_text, doc_brief):&#10;                logger.info(f&quot;Successfully generated and saved preview to old table for token {token[:12]}...&quot;)&#10;            else:&#10;                logger.warning(f&quot;Generated preview but failed to save for token {token[:12]}...&quot;)&#10;&#10;        return jsonify({&#10;            'preview': preview_text,&#10;            'doc_summary': doc_brief,&#10;            'cached': False,&#10;            'generated_at': datetime.utcnow().isoformat()&#10;        }), 200&#10;&#10;    except Exception as e:&#10;        logger.error(f&quot;Error in case preview endpoint: {str(e)}&quot;)&#10;        return jsonify({&#10;            'error': f'Internal server error: {str(e)}'&#10;        }), 500&#10;&#10;&#10;@app.route('/api/case/&lt;token&gt;/status', methods=['GET'])&#10;def case_status(token: str):&#10;    &quot;&quot;&quot;Get case and document status.&quot;&quot;&quot;&#10;    try:&#10;        # Fetch case&#10;        case = read_case_by_token(token)&#10;        if not case:&#10;            return jsonify({'error': 'Case not found'}), 404&#10;&#10;        # Fetch document status&#10;        documents = fetch_any_documents_status_by_token(token)&#10;&#10;        # Count documents by status&#10;        doc_counts = {}&#10;        for doc in documents:&#10;            status = doc.get('status', 'unknown')&#10;            doc_counts[status] = doc_counts.get(status, 0) + 1&#10;&#10;        return jsonify({&#10;            'case_id': case.get('id'),&#10;            'token': token[:12] + '...',&#10;            'hoa_name': case.get('hoa_name'),&#10;            'violation_type': case.get('violation_type'),&#10;            'documents': {&#10;                'total': len(documents),&#10;                'by_status': doc_counts&#10;            },&#10;            'preview_available': bool(case.get('preview_text'))&#10;        }), 200&#10;&#10;    except Exception as e:&#10;        logger.error(f&quot;Error in case status endpoint: {str(e)}&quot;)&#10;        return jsonify({'error': str(e)}), 500&#10;&#10;&#10;# Stripe webhook endpoints&#10;@app.route('/webhooks/stripe', methods=['POST'])&#10;def stripe_webhook():&#10;    &quot;&quot;&quot;Handle Stripe webhook events.&quot;&quot;&quot;&#10;    try:&#10;        payload = request.get_data()&#10;        sig_header = request.headers.get('Stripe-Signature')&#10;&#10;        if not STRIPE_WEBHOOK_SECRET:&#10;            logger.error(&quot;Stripe webhook secret not configured&quot;)&#10;            return jsonify({'error': 'Webhook not configured'}), 500&#10;&#10;        # Verify webhook signature&#10;        try:&#10;            event = stripe.Webhook.construct_event(&#10;                payload, sig_header, STRIPE_WEBHOOK_SECRET&#10;            )&#10;        except ValueError as e:&#10;            logger.error(f&quot;Invalid payload: {e}&quot;)&#10;            return jsonify({'error': 'Invalid payload'}), 400&#10;        except stripe.error.SignatureVerificationError as e:&#10;            logger.error(f&quot;Invalid signature: {e}&quot;)&#10;            return jsonify({'error': 'Invalid signature'}), 400&#10;&#10;        # Handle the event&#10;        if event['type'] == 'checkout.session.completed':&#10;            session = event['data']['object']&#10;            handle_successful_payment(session)&#10;        else:&#10;            logger.info(f&quot;Unhandled Stripe event type: {event['type']}&quot;)&#10;&#10;        return jsonify({'received': True}), 200&#10;&#10;    except Exception as e:&#10;        logger.error(f&quot;Error handling Stripe webhook: {str(e)}&quot;)&#10;        return jsonify({'error': str(e)}), 500&#10;&#10;&#10;def handle_successful_payment(session):&#10;    &quot;&quot;&quot;Handle successful Stripe payment.&quot;&quot;&quot;&#10;    try:&#10;        customer_email = session.get('customer_details', {}).get('email')&#10;        metadata = session.get('metadata', {})&#10;&#10;        logger.info(f&quot;Processing successful payment for email: {customer_email}&quot;)&#10;&#10;        # You can add logic here to update user permissions, send confirmation emails, etc.&#10;&#10;    except Exception as e:&#10;        logger.error(f&quot;Error handling successful payment: {str(e)}&quot;)&#10;&#10;&#10;@app.route('/create-checkout-session', methods=['POST'])&#10;def create_checkout_session():&#10;    &quot;&quot;&quot;Create Stripe checkout session.&quot;&quot;&quot;&#10;    try:&#10;        if not STRIPE_SECRET_KEY or not STRIPE_PRICE_ID:&#10;            return jsonify({'error': 'Stripe not configured'}), 500&#10;&#10;        data = request.get_json()&#10;&#10;        session = stripe.checkout.Session.create(&#10;            payment_method_types=['card'],&#10;            line_items=[{&#10;                'price': STRIPE_PRICE_ID,&#10;                'quantity': 1,&#10;            }],&#10;            mode='payment',&#10;            success_url=f&quot;{SITE_URL}/success?session_id={{CHECKOUT_SESSION_ID}}&quot;,&#10;            cancel_url=f&quot;{SITE_URL}/cancel&quot;,&#10;            customer_email=data.get('email') if data else None,&#10;        )&#10;&#10;        return jsonify({'checkout_url': session.url}), 200&#10;&#10;    except Exception as e:&#10;        logger.error(f&quot;Error creating checkout session: {str(e)}&quot;)&#10;        return jsonify({'error': str(e)}), 500&#10;&#10;&#10;# Email functionality&#10;def send_email(to_email: str, subject: str, body: str, html_body: str = None) -&gt; bool:&#10;    &quot;&quot;&quot;Send email via SMTP.&quot;&quot;&quot;&#10;    try:&#10;        if not all([SMTP_HOST, SMTP_USER, SMTP_PASS]):&#10;            logger.warning(&quot;SMTP not configured - cannot send email&quot;)&#10;            return False&#10;&#10;        msg = MIMEMultipart('alternative')&#10;        msg['From'] = SMTP_FROM&#10;        msg['To'] = to_email&#10;        msg['Subject'] = subject&#10;&#10;        # Add text part&#10;        text_part = MIMEText(body, 'plain')&#10;        msg.attach(text_part)&#10;&#10;        # Add HTML part if provided&#10;        if html_body:&#10;            html_part = MIMEText(html_body, 'html')&#10;            msg.attach(html_part)&#10;&#10;        # Send email&#10;        with smtplib.SMTP(SMTP_HOST, SMTP_PORT) as server:&#10;            server.starttls()&#10;            server.login(SMTP_USER, SMTP_PASS)&#10;            server.sendmail(SMTP_FROM, to_email, msg.as_string())&#10;&#10;        logger.info(f&quot;Email sent successfully to {to_email}&quot;)&#10;        return True&#10;&#10;    except Exception as e:&#10;        logger.error(f&quot;Failed to send email to {to_email}: {str(e)}&quot;)&#10;        return False&#10;&#10;&#10;@app.route('/api/save-case', methods=['POST', 'OPTIONS'])&#10;def save_case():&#10;    &quot;&quot;&quot;Save case endpoint for frontend case creation.&quot;&quot;&quot;&#10;    try:&#10;        # Parse request body&#10;        data = request.get_json()&#10;        if not data:&#10;            return jsonify({'error': 'Invalid JSON body'}), 400&#10;&#10;        token = data.get('token')&#10;        payload = data.get('payload')&#10;&#10;        if not token or not payload:&#10;            return jsonify({'error': 'Token and payload are required'}), 400&#10;&#10;        # Validate token format&#10;        if not str(token).startswith('case_'):&#10;            return jsonify({'error': 'Invalid token format'}), 400&#10;&#10;        # Check if case already exists&#10;        url = f&quot;{SUPABASE_URL}/rest/v1/dmhoa_cases&quot;&#10;        params = {&#10;            'token': f'eq.{token}',&#10;            'select': 'id,payload,created_at'&#10;        }&#10;        headers = supabase_headers()&#10;&#10;        response = requests.get(url, params=params, headers=headers, timeout=TIMEOUT)&#10;        response.raise_for_status()&#10;        &#10;        cases = response.json()&#10;        existing_case = cases[0] if cases else None&#10;&#10;        if existing_case:&#10;            # Case exists - update with merged payload&#10;            existing_payload = existing_case.get('payload') or {}&#10;            if isinstance(existing_payload, str):&#10;                try:&#10;                    existing_payload = json.loads(existing_payload)&#10;                except:&#10;                    existing_payload = {}&#10;&#10;            merged_payload = {**existing_payload, **payload}&#10;&#10;            update_data = {&#10;                'payload': merged_payload,&#10;                'updated_at': datetime.utcnow().isoformat()&#10;            }&#10;            headers['Prefer'] = 'return=representation'&#10;&#10;            update_response = requests.patch(url, params=params, headers=headers, &#10;                                           json=update_data, timeout=TIMEOUT)&#10;            update_response.raise_for_status()&#10;            result = update_response.json()&#10;            logger.info(f&quot;Case updated: {token}&quot;)&#10;&#10;        else:&#10;            # Case doesn't exist - create new&#10;            insert_data = {&#10;                'token': token,&#10;                'payload': payload,&#10;                'status': 'new',&#10;                'unlocked': False,&#10;                'created_at': datetime.utcnow().isoformat(),&#10;                'updated_at': datetime.utcnow().isoformat()&#10;            }&#10;            headers['Prefer'] = 'return=representation'&#10;&#10;            insert_response = requests.post(url, headers=headers, &#10;                                          json=insert_data, timeout=TIMEOUT)&#10;            insert_response.raise_for_status()&#10;            result = insert_response.json()&#10;            logger.info(f&quot;Case created: {token}&quot;)&#10;&#10;        case_id = result[0].get('id') if result and len(result) &gt; 0 else None&#10;        return jsonify({'success': True, 'case_id': case_id}), 200&#10;&#10;    except Exception as e:&#10;        logger.error(f&quot;Save case error: {str(e)}&quot;)&#10;        return jsonify({'error': str(e) or 'Internal server error'}), 500&#10;&#10;&#10;if __name__ == '__main__':&#10;    port = int(os.environ.get('PORT', 5000))&#10;    debug = os.environ.get('FLASK_DEBUG', 'False').lower() == 'true'&#10;    app.run(host='0.0.0.0', port=port, debug=debug)&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/requirements.txt">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/requirements.txt" />
              <option name="originalContent" value="flask&gt;=2.3.0&#10;gunicorn&gt;=21.2.0&#10;requests&gt;=2.31.0&#10;pypdf&gt;=3.17.0&#10;pillow&gt;=10.0.0&#10;pytesseract&gt;=0.3.10&#10;stripe&gt;=7.0.0&#10;&#10;&#10;&#10;&#10;" />
              <option name="updatedContent" value="flask&gt;=2.3.0&#10;flask-cors&gt;=4.0.0&#10;gunicorn&gt;=21.2.0&#10;requests&gt;=2.31.0&#10;pypdf&gt;=3.17.0&#10;pillow&gt;=10.0.0&#10;pytesseract&gt;=0.3.10&#10;stripe&gt;=7.0.0" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>