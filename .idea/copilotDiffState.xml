<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/app.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/app.py" />
              <option name="originalContent" value="# Flask app with CORS fixes deployed on January 18, 2026&#10;import os&#10;import io&#10;import json&#10;import logging&#10;from typing import Dict, Any, Optional, Tuple, List&#10;import re&#10;from datetime import datetime&#10;import threading&#10;import time&#10;&#10;import requests&#10;from flask import Flask, request, jsonify&#10;from flask_cors import CORS&#10;from pypdf import PdfReader&#10;import stripe&#10;&#10;import smtplib&#10;from email.mime.text import MIMEText&#10;from email.mime.multipart import MIMEMultipart&#10;&#10;# Image processing and OCR imports&#10;from PIL import Image&#10;import pytesseract&#10;&#10;&#10;# Configure logging&#10;logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')&#10;logger = logging.getLogger(__name__)&#10;&#10;app = Flask(__name__)&#10;# NEW: Updated CORS configuration with comprehensive headers and methods&#10;CORS(app, resources={r&quot;/*&quot;: {&quot;origins&quot;: &quot;*&quot;}},&#10;     supports_credentials=False,&#10;     allow_headers=[&quot;Content-Type&quot;, &quot;Authorization&quot;, &quot;apikey&quot;, &quot;x-client-info&quot;, &quot;x-supabase-api-version&quot;, &quot;X-Webhook-Secret&quot;],&#10;     methods=[&quot;GET&quot;, &quot;POST&quot;, &quot;PUT&quot;, &quot;DELETE&quot;, &quot;OPTIONS&quot;])&#10;&#10;# Configuration&#10;SUPABASE_URL = os.environ.get('SUPABASE_URL')&#10;SUPABASE_SERVICE_ROLE_KEY = os.environ.get('SUPABASE_SERVICE_ROLE_KEY')&#10;DOC_EXTRACT_WEBHOOK_SECRET = os.environ.get('DOC_EXTRACT_WEBHOOK_SECRET')&#10;&#10;# Stripe Configuration&#10;STRIPE_SECRET_KEY = os.environ.get('STRIPE_SECRET_KEY')&#10;STRIPE_WEBHOOK_SECRET = os.environ.get('STRIPE_WEBHOOK_SECRET')&#10;STRIPE_PRICE_ID = os.environ.get('STRIPE_PRICE_ID')&#10;SITE_URL = os.environ.get('SITE_URL', 'https://disputemyhoa.com')&#10;&#10;# OpenAI Configuration&#10;OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')&#10;&#10;# Configure Stripe&#10;if STRIPE_SECRET_KEY:&#10;    stripe.api_key = STRIPE_SECRET_KEY&#10;&#10;# SMTP Configuration - Optional, only needed for email functionality&#10;SMTP_HOST = os.environ.get(&quot;SMTP_HOST&quot;)&#10;SMTP_PORT = int(os.environ.get(&quot;SMTP_PORT&quot;, &quot;587&quot;))&#10;# Clean SMTP credentials to handle non-ASCII characters like non-breaking spaces&#10;SMTP_USER = (os.environ.get(&quot;SMTP_USER&quot;) or &quot;&quot;).strip().replace('\xa0', ' ')&#10;SMTP_PASS = (os.environ.get(&quot;SMTP_PASS&quot;) or &quot;&quot;).strip().replace('\xa0', ' ')&#10;SMTP_FROM = os.environ.get(&quot;SMTP_FROM&quot;, &quot;support@disputemyhoa.com&quot;)&#10;&#10;SMTP_SENDER_WEBHOOK_SECRET = os.environ.get(&quot;SMTP_SENDER_WEBHOOK_SECRET&quot;)&#10;SMTP_SENDER_WEBHOOK_URL = os.environ.get(&quot;SMTP_SENDER_WEBHOOK_URL&quot;)&#10;&#10;# Request timeouts&#10;TIMEOUT = (5, 60)  # (connect, read)&#10;&#10;# NEW: In-process lock to prevent duplicate preview generation&#10;preview_generation_locks = {}&#10;preview_lock = threading.Lock()&#10;&#10;def supabase_headers() -&gt; Dict[str, str]:&#10;    &quot;&quot;&quot;Return headers for Supabase API requests.&quot;&quot;&quot;&#10;    return {&#10;        'apikey': SUPABASE_SERVICE_ROLE_KEY,&#10;        'Authorization': f'Bearer {SUPABASE_SERVICE_ROLE_KEY}',&#10;        'Content-Type': 'application/json'&#10;    }&#10;&#10;def fetch_ready_documents_by_token(token: str, limit: int = 3) -&gt; List[Dict]:&#10;    &quot;&quot;&quot;Query Supabase for ready documents by case token.&quot;&quot;&quot;&#10;    try:&#10;        url = f&quot;{SUPABASE_URL}/rest/v1/dmhoa_documents&quot;&#10;        params = {&#10;            'token': f'eq.{token}',&#10;            'status': 'eq.ready',&#10;            'select': 'id,filename,mime_type,page_count,char_count,updated_at,extracted_text',&#10;            'order': 'updated_at.desc',&#10;            'limit': str(limit)&#10;        }&#10;        headers = supabase_headers()&#10;&#10;        response = requests.get(url, params=params, headers=headers, timeout=TIMEOUT)&#10;        response.raise_for_status()&#10;&#10;        documents = response.json()&#10;        logger.info(f&quot;Found {len(documents)} ready documents for token {token[:12]}...&quot;)&#10;        return documents&#10;&#10;    except Exception as e:&#10;        logger.error(f&quot;Failed to fetch ready documents for token {token[:12]}...: {str(e)}&quot;)&#10;        return []&#10;&#10;def fetch_any_documents_status_by_token(token: str) -&gt; List[Dict]:&#10;    &quot;&quot;&quot;Check for any documents (including processing/pending) by token.&quot;&quot;&quot;&#10;    try:&#10;        url = f&quot;{SUPABASE_URL}/rest/v1/dmhoa_documents&quot;&#10;        params = {&#10;            'token': f'eq.{token}',&#10;            'select': 'id,status,updated_at',&#10;            'order': 'updated_at.desc'&#10;        }&#10;        headers = supabase_headers()&#10;&#10;        response = requests.get(url, params=params, headers=headers, timeout=TIMEOUT)&#10;        response.raise_for_status()&#10;&#10;        return response.json()&#10;&#10;    except Exception as e:&#10;        logger.error(f&quot;Failed to fetch document status for token {token[:12]}...: {str(e)}&quot;)&#10;        return []&#10;&#10;def summarize_doc_text_with_openai(token: str, raw_text: str) -&gt; str:&#10;    &quot;&quot;&quot;Summarize document text using OpenAI gpt-4o-mini.&quot;&quot;&quot;&#10;    try:&#10;        # Clip text to avoid token limits&#10;        clipped_text = raw_text[:12000] if raw_text else &quot;&quot;&#10;&#10;        if not clipped_text.strip():&#10;            return &quot;&quot;&#10;&#10;        headers = {&#10;            'Authorization': f'Bearer {OPENAI_API_KEY}',&#10;            'Content-Type': 'application/json'&#10;        }&#10;&#10;        prompt = &quot;&quot;&quot;Summarize the HOA notice into: (a) alleged violation, (b) what HOA demands, (c) deadlines/fines/next actions, (d) evidence/rules cited. Be factual, quote exact deadlines/amounts when present. 8-12 bullets max.&quot;&quot;&quot;&#10;&#10;        data = {&#10;            &quot;model&quot;: &quot;gpt-4o-mini&quot;,&#10;            &quot;messages&quot;: [&#10;                {&#10;                    &quot;role&quot;: &quot;system&quot;,&#10;                    &quot;content&quot;: &quot;You are a document summarizer for HOA notices. Extract key facts concisely.&quot;&#10;                },&#10;                {&#10;                    &quot;role&quot;: &quot;user&quot;,&#10;                    &quot;content&quot;: f&quot;{prompt}\n\nDocument text:\n{clipped_text}&quot;&#10;                }&#10;            ],&#10;            &quot;temperature&quot;: 0.3,&#10;            &quot;max_tokens&quot;: 800&#10;        }&#10;&#10;        response = requests.post(&#10;            'https://api.openai.com/v1/chat/completions',&#10;            headers=headers,&#10;            json=data,&#10;            timeout=30&#10;        )&#10;        response.raise_for_status()&#10;&#10;        result = response.json()&#10;        summary = result['choices'][0]['message']['content'].strip()&#10;&#10;        # Limit summary length&#10;        if len(summary) &gt; 1200:&#10;            summary = summary[:1200] + &quot;...&quot;&#10;&#10;        logger.info(f&quot;Successfully summarized document for token {token[:12]}...&quot;)&#10;        return summary&#10;&#10;    except Exception as e:&#10;        logger.error(f&quot;Failed to summarize document text for token {token[:12]}...: {str(e)}&quot;)&#10;        # Fallback to clipped excerpt&#10;        return raw_text[:1200] + &quot;...&quot; if len(raw_text) &gt; 1200 else raw_text&#10;&#10;def build_doc_brief(docs: List[Dict]) -&gt; Dict:&#10;    &quot;&quot;&quot;Build document brief from ready documents.&quot;&quot;&quot;&#10;    if not docs:&#10;        return {&#10;            &quot;doc_status&quot;: &quot;none&quot;,&#10;            &quot;doc_count&quot;: 0,&#10;            &quot;sources&quot;: [],&#10;            &quot;brief_text&quot;: &quot;&quot;&#10;        }&#10;&#10;    sources = []&#10;    raw_texts = []&#10;&#10;    for doc in docs:&#10;        sources.append({&#10;            &quot;filename&quot;: doc.get(&quot;filename&quot;, &quot;unknown&quot;),&#10;            &quot;page_count&quot;: doc.get(&quot;page_count&quot;, 0),&#10;            &quot;char_count&quot;: doc.get(&quot;char_count&quot;, 0)&#10;        })&#10;&#10;        # Extract text with limits&#10;        extracted_text = doc.get(&quot;extracted_text&quot;, &quot;&quot;)&#10;        if extracted_text:&#10;            # Limit per document to 6000 chars&#10;            doc_text = extracted_text[:6000]&#10;            raw_texts.append(doc_text)&#10;&#10;    if not raw_texts:&#10;        return {&#10;            &quot;doc_status&quot;: &quot;ready&quot;,&#10;            &quot;doc_count&quot;: len(docs),&#10;            &quot;sources&quot;: sources,&#10;            &quot;brief_text&quot;: &quot;&quot;&#10;        }&#10;&#10;    # Combine texts with total limit of 12000 chars&#10;    combined_text = &quot; &quot;.join(raw_texts)&#10;    if len(combined_text) &gt; 12000:&#10;        combined_text = combined_text[:12000]&#10;&#10;    # Try to summarize with OpenAI&#10;    token = docs[0].get(&quot;token&quot;, &quot;unknown&quot;) if docs else &quot;unknown&quot;&#10;    brief_text = summarize_doc_text_with_openai(token, combined_text)&#10;&#10;    return {&#10;        &quot;doc_status&quot;: &quot;ready&quot;,&#10;        &quot;doc_count&quot;: len(docs),&#10;        &quot;sources&quot;: sources,&#10;        &quot;brief_text&quot;: brief_text&#10;    }&#10;&#10;&#10;def fetch_document_status(document_id: str) -&gt; Optional[Dict[str, Any]]:&#10;    &quot;&quot;&quot;Fetch current document status from Supabase.&quot;&quot;&quot;&#10;    try:&#10;        url = f&quot;{SUPABASE_URL}/rest/v1/dmhoa_documents&quot;&#10;        params = {&#10;            'id': f'eq.{document_id}',&#10;            'select': 'id,token,status'&#10;        }&#10;        headers = supabase_headers()&#10;&#10;        response = requests.get(url, params=params, headers=headers, timeout=TIMEOUT)&#10;        response.raise_for_status()&#10;&#10;        data = response.json()&#10;        return data[0] if data else None&#10;&#10;    except Exception as e:&#10;        logger.error(f&quot;Failed to fetch document status for {document_id}: {str(e)}&quot;)&#10;        return None&#10;&#10;def update_document(document_id: str, token: str, updates: Dict[str, Any]) -&gt; bool:&#10;    &quot;&quot;&quot;Update document in Supabase database.&quot;&quot;&quot;&#10;    try:&#10;        url = f&quot;{SUPABASE_URL}/rest/v1/dmhoa_documents&quot;&#10;        params = {&#10;            'id': f'eq.{document_id}',&#10;            'token': f'eq.{token}'&#10;        }&#10;        headers = supabase_headers()&#10;        headers['Prefer'] = 'return=representation'&#10;&#10;        response = requests.patch(url, params=params, headers=headers,&#10;                                json=updates, timeout=TIMEOUT)&#10;        response.raise_for_status()&#10;&#10;        logger.info(f&quot;Updated document {document_id} with: {updates}&quot;)&#10;        return True&#10;&#10;    except Exception as e:&#10;        logger.error(f&quot;Failed to update document {document_id}: {str(e)}&quot;)&#10;        return False&#10;&#10;def download_storage_object(bucket: str, path: str) -&gt; Optional[bytes]:&#10;    &quot;&quot;&quot;Download file from Supabase Storage.&quot;&quot;&quot;&#10;    try:&#10;        url = f&quot;{SUPABASE_URL}/storage/v1/object/{bucket}/{path}&quot;&#10;        headers = {&#10;            'apikey': SUPABASE_SERVICE_ROLE_KEY,&#10;            'Authorization': f'Bearer {SUPABASE_SERVICE_ROLE_KEY}'&#10;        }&#10;&#10;        response = requests.get(url, headers=headers, timeout=TIMEOUT)&#10;&#10;        if response.status_code == 404:&#10;            logger.error(f&quot;File not found: {bucket}/{path}&quot;)&#10;            return None&#10;&#10;        response.raise_for_status()&#10;&#10;        logger.info(f&quot;Downloaded {len(response.content)} bytes from {bucket}/{path}&quot;)&#10;        return response.content&#10;&#10;    except Exception as e:&#10;        logger.error(f&quot;Failed to download {bucket}/{path}: {str(e)}&quot;)&#10;        return None&#10;&#10;def extract_pdf_text(pdf_bytes: bytes) -&gt; Tuple[str, int, int, Optional[str]]:&#10;    &quot;&quot;&quot;&#10;    Extract text from PDF bytes.&#10;    Returns: (extracted_text, page_count, char_count, error_message)&#10;    &quot;&quot;&quot;&#10;    try:&#10;        reader = PdfReader(io.BytesIO(pdf_bytes))&#10;        page_count = len(reader.pages)&#10;&#10;        text_parts = []&#10;        for page in reader.pages:&#10;            try:&#10;                page_text = page.extract_text() or &quot;&quot;&#10;                text_parts.append(page_text)&#10;            except Exception as e:&#10;                logger.warning(f&quot;Failed to extract text from page: {str(e)}&quot;)&#10;                text_parts.append(&quot;&quot;)&#10;&#10;        extracted_text = &quot;\n\n&quot;.join(text_parts)&#10;        char_count = len(extracted_text)&#10;&#10;        # Check if text is empty or only whitespace&#10;        if not extracted_text or extracted_text.strip() == &quot;&quot;:&#10;            return &quot;&quot;, page_count, 0, &quot;No text layer found - document may be scanned and require OCR&quot;&#10;&#10;        logger.info(f&quot;Extracted {char_count} characters from {page_count} pages&quot;)&#10;        return extracted_text, page_count, char_count, None&#10;&#10;    except Exception as e:&#10;        error_msg = f&quot;Failed to extract text from PDF: {str(e)}&quot;&#10;        logger.error(error_msg)&#10;        return &quot;&quot;, 0, 0, error_msg&#10;&#10;&#10;def extract_image_text(image_bytes: bytes, filename: str = &quot;&quot;) -&gt; Tuple[str, int, int, Optional[str]]:&#10;    &quot;&quot;&quot;&#10;    Extract text from image bytes using OCR.&#10;    Returns: (extracted_text, page_count=1, char_count, error_message)&#10;    &quot;&quot;&quot;&#10;    try:&#10;        # Open image from bytes&#10;        image = Image.open(io.BytesIO(image_bytes))&#10;&#10;        # Convert to RGB if necessary (some formats like RGBA or P need conversion)&#10;        if image.mode not in ('RGB', 'L'):&#10;            image = image.convert('RGB')&#10;&#10;        logger.info(f&quot;Processing image: {image.size[0]}x{image.size[1]} pixels, mode: {image.mode}&quot;)&#10;&#10;        # Set TESSDATA_PREFIX if not already set (for Heroku compatibility)&#10;        if 'TESSDATA_PREFIX' not in os.environ:&#10;            possible_paths = [&#10;                '/usr/share/tesseract-ocr/5/tessdata',&#10;                '/usr/share/tesseract-ocr/tessdata',&#10;                '/usr/share/tesseract-ocr/4.00/tessdata',&#10;                '/usr/share/tesseract-ocr/4/tessdata',&#10;                '/usr/share/tesseract-ocr/tessdata',&#10;                '/usr/share/tessdata',&#10;                '/app/.apt/usr/share/tesseract-ocr/5/tessdata',&#10;                '/app/.apt/usr/share/tesseract-ocr/tessdata'&#10;            ]&#10;            for path in possible_paths:&#10;                if os.path.exists(path):&#10;                    os.environ['TESSDATA_PREFIX'] = path&#10;                    logger.info(f&quot;Set TESSDATA_PREFIX to: {path}&quot;)&#10;                    break&#10;            else:&#10;                logger.warning(&quot;Could not find tessdata directory in any expected location&quot;)&#10;&#10;        # Try multiple OCR configurations for better compatibility&#10;        configs_to_try = [&#10;            r'--oem 1 --psm 1 -l eng',   # Automatic page segmentation with OSD&#10;            r'--oem 1 --psm 3 -l eng',   # Fully automatic page segmentation, but no OSD&#10;            r'--oem 1 --psm 4 -l eng',   # Assume a single column of text of variable sizes&#10;            r'--oem 1 --psm 6 -l eng',   # Assume a single uniform block of text&#10;            r'--oem 3 --psm 1 -l eng',   # LSTM with automatic page segmentation&#10;            r'--oem 3 --psm 3 -l eng',   # LSTM with fully automatic page segmentation&#10;            r'--oem 3 --psm 6 -l eng',   # LSTM standard config&#10;            r'--oem 3 --psm 11 -l eng',  # Sparse text - find as much text as possible&#10;            r'--oem 3 --psm 12 -l eng',  # Sparse text with OSD&#10;            r'--psm 6',                  # No language specified fallback&#10;        ]&#10;&#10;        extracted_text = &quot;&quot;&#10;        best_text = &quot;&quot;&#10;        best_char_count = 0&#10;        last_error = None&#10;&#10;        for config in configs_to_try:&#10;            try:&#10;                logger.info(f&quot;Trying OCR with config: {config}&quot;)&#10;                current_text = pytesseract.image_to_string(image, config=config)&#10;                current_text = current_text.strip()&#10;                char_count = len(current_text);&#10;&#10;                # Keep track of the best result (most text that looks reasonable)&#10;                if char_count &gt; best_char_count:&#10;                    # Basic heuristic: prefer results with more alphanumeric content&#10;                    alphanumeric_ratio = sum(c.isalnum() or c.isspace() for c in current_text) / max(len(current_text), 1)&#10;                    if alphanumeric_ratio &gt; 0.3:  # At least 30% should be readable characters&#10;                        best_text = current_text&#10;                        best_char_count = char_count&#10;                        logger.info(f&quot;New best result: {char_count} chars, {alphanumeric_ratio:.2f} alphanumeric ratio&quot;)&#10;&#10;                # If we got a decent amount of readable text, we can stop&#10;                if char_count &gt; 50 and best_text:&#10;                    extracted_text = best_text&#10;                    break&#10;&#10;            except Exception as e:&#10;                last_error = str(e)&#10;                logger.warning(f&quot;OCR config failed: {config}, error: {str(e)}&quot;)&#10;                continue&#10;&#10;        # Use the best result we found&#10;        if not extracted_text and best_text:&#10;            extracted_text = best_text&#10;&#10;        # Clean up the extracted text&#10;        extracted_text = extracted_text.strip()&#10;        char_count = len(extracted_text)&#10;&#10;        if char_count == 0:&#10;            error_msg = f&quot;No text found in image - image may be blank or contain no readable text. Last OCR error: {last_error}&quot;&#10;            return &quot;&quot;, 1, 0, error_msg&#10;&#10;        logger.info(f&quot;OCR extracted {char_count} characters from image {filename}&quot;)&#10;        return extracted_text, 1, char_count, None&#10;&#10;    except Exception as e:&#10;        error_msg = f&quot;Failed to extract text from image: {str(e)}&quot;&#10;        logger.error(error_msg)&#10;        return &quot;&quot;, 0, 0, error_msg&#10;&#10;&#10;def is_image_file(filename: str, mime_type: str = &quot;&quot;) -&gt; bool:&#10;    &quot;&quot;&quot;Check if file is a supported image format.&quot;&quot;&quot;&#10;    image_extensions = {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.tif', '.webp'}&#10;    image_mime_types = {&#10;        'image/jpeg', 'image/jpg', 'image/png', 'image/gif',&#10;        'image/bmp', 'image/tiff', 'image/webp'&#10;    }&#10;&#10;    # Check by file extension&#10;    if filename:&#10;        ext = os.path.splitext(filename.lower())[1]&#10;        if ext in image_extensions:&#10;            return True&#10;&#10;    # Check by MIME type&#10;    if mime_type and mime_type.lower() in image_mime_types:&#10;        return True&#10;&#10;    return False&#10;&#10;&#10;def is_pdf_file(filename: str, mime_type: str = &quot;&quot;) -&gt; bool:&#10;    &quot;&quot;&quot;Check if file is a PDF.&quot;&quot;&quot;&#10;    if filename and filename.lower().endswith('.pdf'):&#10;        return True&#10;    if mime_type and mime_type.lower() == 'application/pdf':&#10;        return True&#10;    return False&#10;&#10;&#10;@app.route('/health', methods=['GET'])&#10;def health_check():&#10;    &quot;&quot;&quot;Health check endpoint.&quot;&quot;&quot;&#10;    return jsonify({'status': 'healthy'}), 200&#10;&#10;&#10;@app.route('/debug/env', methods=['GET'])&#10;def debug_env():&#10;    &quot;&quot;&quot;Return presence of critical env vars (gated by DOC_EXTRACT_WEBHOOK_SECRET).&#10;&#10;    This does NOT return any secret values, only boolean flags indicating whether each&#10;    required configuration item is set. Intended for quick diagnostics on deployed app.&#10;    &quot;&quot;&quot;&#10;    secret = request.headers.get('X-Webhook-Secret')&#10;    if not secret or secret != DOC_EXTRACT_WEBHOOK_SECRET:&#10;        logger.warning(&quot;Unauthorized request to /debug/env&quot;)&#10;        return jsonify({'error': 'Unauthorized'}), 401&#10;&#10;    keys = [&#10;        'SUPABASE_URL', 'SUPABASE_SERVICE_ROLE_KEY', 'DOC_EXTRACT_WEBHOOK_SECRET',&#10;        'SMTP_HOST', 'SMTP_PORT', 'SMTP_USER', 'SMTP_PASS', 'SMTP_FROM', 'SMTP_SENDER_WEBHOOK_SECRET'&#10;    ]&#10;    presence = {k: bool(os.environ.get(k)) for k in keys}&#10;    logger.info(f&quot;/debug/env requested; presence: { {k: presence[k] for k in presence} }&quot;)&#10;    return jsonify({'env_presence': presence}), 200&#10;&#10;@app.route('/webhooks/doc-extract', methods=['POST'])&#10;def doc_extract_webhook():&#10;    &quot;&quot;&quot;Main webhook endpoint for document extraction.&quot;&quot;&quot;&#10;    # Validate webhook secret&#10;    webhook_secret = request.headers.get('X-Webhook-Secret')&#10;    if not webhook_secret or webhook_secret != DOC_EXTRACT_WEBHOOK_SECRET:&#10;        logger.warning(&quot;Invalid or missing webhook secret&quot;)&#10;        return jsonify({'error': 'Unauthorized'}), 401&#10;&#10;    document_id = None&#10;    token = None&#10;&#10;    try:&#10;        # Parse JSON body&#10;        data = request.get_json()&#10;        if not data:&#10;            return jsonify({'error': 'Invalid JSON body'}), 400&#10;&#10;        # Validate required fields&#10;        required_fields = ['token', 'document_id', 'bucket', 'path']&#10;        missing_fields = [field for field in required_fields if not data.get(field)]&#10;        if missing_fields:&#10;            return jsonify({&#10;                'error': f'Missing required fields: {&quot;, &quot;.join(missing_fields)}'&#10;            }), 400&#10;&#10;        token = data['token']&#10;        document_id = data['document_id']&#10;        bucket = data['bucket']&#10;        path = data['path']&#10;        filename = data.get('filename', '') or ''  # Handle null values&#10;        mime_type = data.get('mime_type', '') or ''  # Handle null values&#10;&#10;        logger.info(f&quot;Processing document extraction - ID: {document_id}, Token: {token[:8]}...&quot;)&#10;&#10;        # Check if document is already processed&#10;        current_doc = fetch_document_status(document_id)&#10;        if current_doc and current_doc.get('status') == 'ready':&#10;            logger.info(f&quot;Document {document_id} already processed&quot;)&#10;            return jsonify({&#10;                'message': 'Document already processed',&#10;                'document_id': document_id,&#10;                'status': 'ready'&#10;            }), 200&#10;&#10;        # Mark document as processing&#10;        if not update_document(document_id, token, {'status': 'processing'}):&#10;            return jsonify({&#10;                'error': 'Failed to update document status to processing',&#10;                'document_id': document_id&#10;            }), 500&#10;&#10;        # Download file from Supabase Storage&#10;        file_bytes = download_storage_object(bucket, path)&#10;        if file_bytes is None:&#10;            error_msg = f&quot;Failed to download file from {bucket}/{path}&quot;&#10;            update_document(document_id, token, {&#10;                'status': 'failed',&#10;                'error': error_msg[:2000]&#10;            })&#10;            return jsonify({&#10;                'error': error_msg,&#10;                'document_id': document_id&#10;            }), 500&#10;&#10;        # Determine file type with multiple fallback strategies&#10;        logger.info(f&quot;Initial file detection - filename: '{filename}', mime_type: '{mime_type}'&quot;)&#10;&#10;        # Strategy 1: If filename is empty/null, extract from path&#10;        if not filename or filename.lower() == 'null':&#10;            filename = os.path.basename(path)&#10;            logger.info(f&quot;Extracted filename from path: '{filename}'&quot;)&#10;&#10;        # Strategy 2: If mime_type is empty/null, guess from filename&#10;        if not mime_type or mime_type.lower() == 'null':&#10;            if filename:&#10;                ext = os.path.splitext(filename.lower())[1]&#10;                mime_type_map = {&#10;                    '.pdf': 'application/pdf',&#10;                    '.jpg': 'image/jpeg', '.jpeg': 'image/jpeg',&#10;                    '.png': 'image/png', '.gif': 'image/gif',&#10;                    '.bmp': 'image/bmp', '.tiff': 'image/tiff', '.tif': 'image/tiff',&#10;                    '.webp': 'image/webp'&#10;                }&#10;                mime_type = mime_type_map.get(ext, '')&#10;                logger.info(f&quot;Guessed MIME type from extension '{ext}': '{mime_type}'&quot;)&#10;&#10;        # Strategy 3: If still no filename, try extracting just the filename from the full path&#10;        if not filename:&#10;            # Handle paths like &quot;dmhoa-docs/case_xxx/original/image.jpg&quot;&#10;            path_parts = path.split('/')&#10;            if path_parts:&#10;                filename = path_parts[-1]  # Get the last part&#10;                logger.info(f&quot;Extracted filename from path parts: '{filename}'&quot;)&#10;&#10;        # Strategy 4: If we still have no clear type, try to detect from file content&#10;        detected_type = None&#10;        if not (is_pdf_file(filename, mime_type) or is_image_file(filename, mime_type)):&#10;            # Check file magic bytes as last resort&#10;            if file_bytes and len(file_bytes) &gt;= 4:&#10;                # PDF magic bytes&#10;                if file_bytes.startswith(b'%PDF'):&#10;                    detected_type = 'pdf'&#10;                    logger.info(&quot;Detected PDF from file magic bytes&quot;)&#10;                # JPEG magic bytes&#10;                elif file_bytes.startswith(b'\xff\xd8\xff'):&#10;                    detected_type = 'image'&#10;                    mime_type = 'image/jpeg'&#10;                    logger.info(&quot;Detected JPEG from file magic bytes&quot;)&#10;                # PNG magic bytes&#10;                elif file_bytes.startswith(b'\x89PNG'):&#10;                    detected_type = 'image'&#10;                    mime_type = 'image/png'&#10;                    logger.info(&quot;Detected PNG from file magic bytes&quot;)&#10;&#10;        logger.info(f&quot;Final file detection - filename: '{filename}', mime_type: '{mime_type}', detected_type: {detected_type}&quot;)&#10;&#10;        # Process based on detected file type&#10;        if is_pdf_file(filename, mime_type) or detected_type == 'pdf':&#10;            logger.info(f&quot;Processing as PDF: {filename}&quot;)&#10;            extracted_text, page_count, char_count, extraction_error = extract_pdf_text(file_bytes)&#10;        elif is_image_file(filename, mime_type) or detected_type == 'image':&#10;            logger.info(f&quot;Processing as image: {filename}&quot;)&#10;            extracted_text, page_count, char_count, extraction_error = extract_image_text(file_bytes, filename)&#10;        else:&#10;            error_msg = f&quot;Unsupported file type: {filename} (MIME: {mime_type})&quot;&#10;            logger.error(error_msg)&#10;            update_document(document_id, token, {&#10;                'status': 'failed',&#10;                'error': error_msg[:2000]&#10;            })&#10;            return jsonify({&#10;                'error': error_msg,&#10;                'document_id': document_id&#10;            }), 400&#10;&#10;        # Update document with extraction results&#10;        update_data = {&#10;            'status': 'ready',&#10;            'extracted_text': extracted_text[:50000],  # Limit text size&#10;            'page_count': page_count,&#10;            'char_count': char_count&#10;        }&#10;&#10;        if extraction_error:&#10;            update_data['error'] = extraction_error[:2000]&#10;            logger.warning(f&quot;Document {document_id} processed with warning: {extraction_error}&quot;)&#10;        else:&#10;            logger.info(f&quot;Document {document_id} processed successfully: {char_count} chars, {page_count} pages&quot;)&#10;&#10;        if not update_document(document_id, token, update_data):&#10;            return jsonify({&#10;                'error': 'Failed to save extraction results',&#10;                'document_id': document_id&#10;            }), 500&#10;&#10;        # Trigger preview update now that document is ready&#10;        try:&#10;            trigger_preview_update_after_document_processing(token)&#10;        except Exception as e:&#10;            logger.warning(f&quot;Failed to trigger preview update after document processing: {str(e)}&quot;)&#10;            # Don't fail the response if preview update fails&#10;&#10;        return jsonify({&#10;            'message': 'Document processed successfully',&#10;            'document_id': document_id,&#10;            'status': 'ready',&#10;            'page_count': page_count,&#10;            'char_count': char_count,&#10;            'has_error': bool(extraction_error)&#10;        }), 200&#10;&#10;    except Exception as e:&#10;        error_msg = f&quot;Unexpected error processing document: {str(e)}&quot;&#10;        logger.error(error_msg)&#10;&#10;        # Try to update document status to failed if we have the IDs&#10;        if document_id and token:&#10;            try:&#10;                update_document(document_id, token, {&#10;                    'status': 'failed',&#10;                    'error': error_msg[:2000]&#10;                })&#10;            except:&#10;                pass  # Don't fail the response if we can't update status&#10;&#10;        return jsonify({&#10;            'error': error_msg,&#10;            'document_id': document_id&#10;        }), 500&#10;&#10;&#10;def trigger_preview_update_after_document_processing(token: str):&#10;    &quot;&quot;&quot;Trigger preview regeneration after a document becomes ready.&quot;&quot;&quot;&#10;    try:&#10;        case = read_case_by_token(token)&#10;        if not case:&#10;            logger.warning(f&quot;Cannot trigger preview update - case not found for token {token[:8]}...&quot;)&#10;            return&#10;&#10;        case_id = case.get('id')&#10;        if not case_id:&#10;            logger.warning(f&quot;Cannot trigger preview update - case ID not found for token {token[:8]}...&quot;)&#10;            return&#10;&#10;        # Trigger preview regeneration with force=True since we have new document data&#10;        success = auto_generate_case_preview(token, case_id, force_regenerate=True)&#10;&#10;        if success:&#10;            logger.info(f&quot;Successfully triggered preview update for case {token[:8]}... after document processing&quot;)&#10;        else:&#10;            logger.warning(f&quot;Failed to trigger preview update for case {token[:8]}... after document processing&quot;)&#10;&#10;    except Exception as e:&#10;        logger.error(f&quot;Error triggering preview update after document processing for {token[:8]}...: {str(e)}&quot;)&#10;&#10;&#10;def read_case_by_token(token: str) -&gt; Optional[Dict[str, Any]]:&#10;    &quot;&quot;&quot;Fetch case details from Supabase by token.&quot;&quot;&quot;&#10;    try:&#10;        url = f&quot;{SUPABASE_URL}/rest/v1/dmhoa_cases&quot;&#10;        params = {&#10;            'token': f'eq.{token}',&#10;            'select': '*'&#10;        }&#10;        headers = supabase_headers()&#10;&#10;        response = requests.get(url, params=params, headers=headers, timeout=TIMEOUT)&#10;        response.raise_for_status()&#10;&#10;        cases = response.json()&#10;        if not cases:&#10;            logger.warning(f&quot;No case found for token: {token[:12]}...&quot;)&#10;            return None&#10;&#10;        case = cases[0]&#10;        logger.info(f&quot;Found case for token {token[:12]}...: ID {case.get('id')}&quot;)&#10;        return case&#10;&#10;    except Exception as e:&#10;        logger.error(f&quot;Failed to fetch case for token {token[:12]}...: {str(e)}&quot;)&#10;        return None&#10;&#10;&#10;def read_case_by_id(case_id: str) -&gt; Optional[Dict[str, Any]]:&#10;    &quot;&quot;&quot;Fetch case details from Supabase by case ID.&quot;&quot;&quot;&#10;    try:&#10;        url = f&quot;{SUPABASE_URL}/rest/v1/dmhoa_cases&quot;&#10;        params = {&#10;            'id': f'eq.{case_id}',&#10;            'select': '*'&#10;        }&#10;        headers = supabase_headers()&#10;&#10;        response = requests.get(url, params=params, headers=headers, timeout=TIMEOUT)&#10;        response.raise_for_status()&#10;&#10;        cases = response.json()&#10;        if not cases:&#10;            logger.warning(f&quot;No case found for ID: {case_id}&quot;)&#10;            return None&#10;&#10;        case = cases[0]&#10;        logger.info(f&quot;Found case for ID {case_id}: Token {case.get('token', '')[:8]}...&quot;)&#10;        return case&#10;&#10;    except Exception as e:&#10;        logger.error(f&quot;Failed to fetch case for ID {case_id}: {str(e)}&quot;)&#10;        return None&#10;&#10;&#10;def read_active_preview(case_id: str) -&gt; Optional[Dict]:&#10;    &quot;&quot;&quot;Read the active preview for a case from dmhoa_case_previews table.&quot;&quot;&quot;&#10;    try:&#10;        url = f&quot;{SUPABASE_URL}/rest/v1/dmhoa_case_previews&quot;&#10;        params = {&#10;            'case_id': f'eq.{case_id}',&#10;            'is_active': 'eq.true',&#10;            'select': '*',&#10;            'order': 'created_at.desc',&#10;            'limit': '1'&#10;        }&#10;        headers = supabase_headers()&#10;&#10;        response = requests.get(url, params=params, headers=headers, timeout=TIMEOUT)&#10;        response.raise_for_status()&#10;&#10;        previews = response.json()&#10;        if previews:&#10;            logger.info(f&quot;Found active preview for case {case_id}&quot;)&#10;            return previews[0]&#10;&#10;        logger.info(f&quot;No active preview found for case {case_id}&quot;)&#10;        return None&#10;&#10;    except Exception as e:&#10;        logger.error(f&quot;Failed to read active preview for case {case_id}: {str(e)}&quot;)&#10;        return None&#10;&#10;&#10;def deactivate_previews(case_id: str) -&gt; bool:&#10;    &quot;&quot;&quot;Deactivate all existing previews for a case.&quot;&quot;&quot;&#10;    try:&#10;        url = f&quot;{SUPABASE_URL}/rest/v1/dmhoa_case_previews&quot;&#10;        params = {'case_id': f'eq.{case_id}'}&#10;        headers = supabase_headers()&#10;&#10;        update_data = {'is_active': False}&#10;&#10;        response = requests.patch(url, params=params, headers=headers,&#10;                                json=update_data, timeout=TIMEOUT)&#10;        response.raise_for_status()&#10;&#10;        logger.info(f&quot;Deactivated existing previews for case {case_id}&quot;)&#10;        return True&#10;&#10;    except Exception as e:&#10;        logger.error(f&quot;Failed to deactivate previews for case {case_id}: {str(e)}&quot;)&#10;        return False&#10;&#10;&#10;def insert_preview(case_id: str, preview_content: Dict, preview_snippet: str = None,&#10;                  prompt_version: str = None, model: str = &quot;gpt-4o-mini&quot;,&#10;                  token_input: int = None, token_output: int = None,&#10;                  cost_usd: float = None, latency_ms: int = None) -&gt; Optional[str]:&#10;    &quot;&quot;&quot;Insert a new preview into dmhoa_case_previews table.&quot;&quot;&quot;&#10;    try:&#10;        url = f&quot;{SUPABASE_URL}/rest/v1/dmhoa_case_previews&quot;&#10;        headers = supabase_headers()&#10;        headers['Prefer'] = 'return=representation'&#10;&#10;        preview_data = {&#10;            'case_id': case_id,&#10;            'preview_content': preview_content,&#10;            'preview_snippet': preview_snippet,&#10;            'prompt_version': prompt_version,&#10;            'model': model,&#10;            'token_input': token_input,&#10;            'token_output': token_output,&#10;            'cost_usd': cost_usd,&#10;            'latency_ms': latency_ms,&#10;            'is_active': True&#10;        }&#10;&#10;        # Remove None values&#10;        preview_data = {k: v for k, v in preview_data.items() if v is not None}&#10;&#10;        response = requests.post(url, headers=headers, json=preview_data, timeout=TIMEOUT)&#10;        response.raise_for_status()&#10;&#10;        result = response.json()&#10;        if result:&#10;            preview_id = result[0]['id']&#10;            logger.info(f&quot;Inserted new preview {preview_id} for case {case_id}&quot;)&#10;            return preview_id&#10;&#10;        return None&#10;&#10;    except Exception as e:&#10;        logger.error(f&quot;Failed to insert preview for case {case_id}: {str(e)}&quot;)&#10;        return None&#10;&#10;&#10;def save_case_preview_to_new_table(case_id: str, preview_text: str, doc_brief: Dict,&#10;                                  token_usage: Dict = None, latency_ms: int = None, preview_json: Optional[Dict] = None) -&gt; bool:&#10;    &quot;&quot;&quot;Save generated case preview to the new dmhoa_case_previews table.&quot;&quot;&quot;&#10;    try:&#10;        # Deactivate existing previews first&#10;        deactivate_previews(case_id)&#10;&#10;        # Prepare preview content as JSONB with preview_json&#10;        preview_content = {&#10;            'preview_text': preview_text,&#10;            'doc_summary': doc_brief,&#10;            'generated_at': datetime.utcnow().isoformat(),&#10;            'preview_json': preview_json&#10;        }&#10;&#10;        # Create preview_snippet from preview_json if available, otherwise fallback&#10;        if preview_json:&#10;            headline = preview_json.get('headline', 'HOA Case Analysis')&#10;            deadline = preview_json.get('your_situation', {}).get('deadline', 'Not stated')&#10;            risks = preview_json.get('risk_if_wrong', [])&#10;            first_risk = risks[0] if risks else 'Various consequences'&#10;&#10;            preview_snippet = f&quot;{headline} | Deadline: {deadline} | Risk: {first_risk}&quot;&#10;            # Limit snippet length&#10;            if len(preview_snippet) &gt; 200:&#10;                preview_snippet = preview_snippet[:197] + &quot;...&quot;&#10;        else:&#10;            # Fallback to old method&#10;            preview_snippet = preview_text[:200] + &quot;...&quot; if len(preview_text) &gt; 200 else preview_text&#10;&#10;        # Extract token usage if available&#10;        token_input = token_usage.get('prompt_tokens') if token_usage else None&#10;        token_output = token_usage.get('completion_tokens') if token_usage else None&#10;        cost_usd = token_usage.get('cost_usd') if token_usage else None&#10;&#10;        # Insert new preview with updated prompt version&#10;        preview_id = insert_preview(&#10;            case_id=case_id,&#10;            preview_content=preview_content,&#10;            preview_snippet=preview_snippet,&#10;            prompt_version=&quot;v2.0_sales&quot;,  # Updated to indicate new conversion-optimized format&#10;            model=&quot;gpt-4o-mini&quot;,&#10;            token_input=token_input,&#10;            token_output=token_output,&#10;            cost_usd=cost_usd,&#10;            latency_ms=latency_ms&#10;        )&#10;&#10;        return preview_id is not None&#10;&#10;    except Exception as e:&#10;        logger.error(f&quot;Failed to save case preview to new table for case {case_id}: {str(e)}&quot;)&#10;        return False&#10;&#10;&#10;def render_preview_markdown(preview_json: Dict) -&gt; str:&#10;    &quot;&quot;&quot;Convert preview_json into a clean markdown string for existing UI.&quot;&quot;&quot;&#10;    try:&#10;        markdown_parts = []&#10;&#10;        # Headline&#10;        headline = preview_json.get('headline', 'HOA Case Analysis')&#10;        markdown_parts.append(f&quot;# {headline}\n&quot;)&#10;&#10;        # Why Now section&#10;        why_now = preview_json.get('why_now', '')&#10;        if why_now:&#10;            markdown_parts.append(f&quot;## Urgent Situation\n{why_now}\n&quot;)&#10;&#10;        # Your Situation&#10;        situation = preview_json.get('your_situation', {})&#10;        if situation:&#10;            markdown_parts.append(&quot;## Your Current Situation\n&quot;)&#10;&#10;            alleged_violation = situation.get('alleged_violation')&#10;            if alleged_violation:&#10;                markdown_parts.append(f&quot;**Alleged Violation:** {alleged_violation}\n&quot;)&#10;&#10;            deadline = situation.get('deadline', 'Not stated')&#10;            markdown_parts.append(f&quot;**Deadline:** {deadline}\n&quot;)&#10;&#10;            hoa_demands = situation.get('hoa_demands', [])&#10;            if hoa_demands:&#10;                markdown_parts.append(&quot;**HOA Demands:**&quot;)&#10;                for demand in hoa_demands:&#10;                    markdown_parts.append(f&quot;- {demand}&quot;)&#10;                markdown_parts.append(&quot;&quot;)&#10;&#10;            rules_cited = situation.get('rules_cited', [])&#10;            if rules_cited:&#10;                markdown_parts.append(&quot;**Rules/Regulations Cited:**&quot;)&#10;                for rule in rules_cited:&#10;                    markdown_parts.append(f&quot;- {rule}&quot;)&#10;                markdown_parts.append(&quot;&quot;)&#10;&#10;        # NEW: Critical Detail (Locked) section&#10;        critical_detail = preview_json.get('critical_detail_locked', {})&#10;        if critical_detail:&#10;            title = critical_detail.get('title', 'Critical Detail (Locked)')&#10;            body = critical_detail.get('body', '')&#10;            if body:&#10;                markdown_parts.append(f&quot;## {title}\n{body}\n&quot;)&#10;&#10;        # Risk if wrong&#10;        risks = preview_json.get('risk_if_wrong', [])&#10;        if risks:&#10;            markdown_parts.append(&quot;## Risk If You Handle This Wrong\n&quot;)&#10;            for risk in risks:&#10;                markdown_parts.append(f&quot;- {risk}&quot;)&#10;            markdown_parts.append(&quot;&quot;)&#10;&#10;        # What you get when you unlock&#10;        unlock_items = preview_json.get('what_you_get_when_you_unlock', [])&#10;        if unlock_items:&#10;            markdown_parts.append(&quot;## What You Get When You Unlock Full Response\n&quot;)&#10;            for item in unlock_items:&#10;                markdown_parts.append(f&quot;- {item}&quot;)&#10;            markdown_parts.append(&quot;&quot;)&#10;&#10;        # Hard stop&#10;        hard_stop = preview_json.get('hard_stop', '')&#10;        if hard_stop:&#10;            markdown_parts.append(f&quot;## Next Steps\n{hard_stop}\n&quot;)&#10;&#10;        # CTA&#10;        cta = preview_json.get('cta', {})&#10;        if cta:&#10;            primary = cta.get('primary', 'Unlock full response package')&#10;            secondary = cta.get('secondary', 'See exactly what proof the HOA will accept')&#10;            markdown_parts.append(f&quot;**{primary}**\n*{secondary}*\n&quot;)&#10;&#10;        # Join all parts and limit to ~600 words&#10;        full_markdown = '\n'.join(markdown_parts)&#10;&#10;        # Simple word count check - if too long, truncate at reasonable point&#10;        words = full_markdown.split()&#10;        if len(words) &gt; 600:&#10;            truncated_words = words[:600]&#10;            full_markdown = ' '.join(truncated_words) + &quot;...&quot;&#10;&#10;        return full_markdown&#10;&#10;    except Exception as e:&#10;        logger.error(f&quot;Error rendering preview markdown: {str(e)}&quot;)&#10;        return &quot;Error rendering preview. Please try again.&quot;&#10;&#10;&#10;# NEW: Helper function to clean up rules_cited narrative text&#10;def clean_rules_cited(rules_cited_list):&#10;    &quot;&quot;&quot;Clean up rules_cited to prefer citations over narrative.&quot;&quot;&quot;&#10;    if not rules_cited_list:&#10;        return rules_cited_list&#10;&#10;    cleaned = []&#10;    for rule in rules_cited_list:&#10;        if isinstance(rule, str):&#10;            # Convert narrative &quot;previous notices&quot; to compressed form&#10;            if &quot;previous notices&quot; in rule.lower():&#10;                # Extract year if present, otherwise use generic&#10;                import re&#10;                year_match = re.search(r'\b(20\d{2})\b', rule)&#10;                if year_match:&#10;                    cleaned.append(f&quot;Prior notices ({year_match.group(1)})&quot;)&#10;                else:&#10;                    cleaned.append(&quot;Prior notices (2024)&quot;)&#10;            else:&#10;                cleaned.append(rule)&#10;        else:&#10;            cleaned.append(rule)&#10;&#10;    return cleaned&#10;&#10;def generate_case_preview_with_openai(case_data: Dict, doc_brief: Dict) -&gt; Tuple[str, Dict, int, Optional[Dict]]:&#10;    &quot;&quot;&quot;Generate case preview using OpenAI and return preview, token usage, latency, and preview_json.&quot;&quot;&quot;&#10;    start_time = time.time()&#10;&#10;    try:&#10;        if not OPENAI_API_KEY:&#10;            logger.warning(&quot;OpenAI API key not configured&quot;)&#10;            return &quot;Preview generation unavailable - OpenAI not configured&quot;, {}, 0, None&#10;&#10;        # Extract case information from payload first, then fallback to top level&#10;        payload = case_data.get('payload', {})&#10;        if isinstance(payload, str):&#10;            try:&#10;                payload = json.loads(payload)&#10;            except:&#10;                payload = {}&#10;&#10;        # Try payload first, then case_data directly&#10;        hoa_name = (payload.get('hoaName') or&#10;                   payload.get('hoa_name') or&#10;                   case_data.get('hoa_name') or&#10;                   case_data.get('hoaName') or&#10;                   'Unknown HOA')&#10;&#10;        violation_type = (payload.get('violationType') or&#10;                         payload.get('violation_type') or&#10;                         payload.get('noticeType') or&#10;                         case_data.get('case_description') or&#10;                         case_data.get('violationType') or&#10;                         'Unknown violation')&#10;&#10;        case_description = (payload.get('caseDescription') or&#10;                           payload.get('case_description') or&#10;                           payload.get('description') or&#10;                           case_data.get('case_description') or&#10;                           case_data.get('caseDescription') or&#10;                           'No description provided')&#10;&#10;        property_address = (payload.get('propertyAddress') or&#10;                           payload.get('property_address') or&#10;                           case_data.get('property_address') or&#10;                           case_data.get('propertyAddress') or&#10;                           '')&#10;&#10;        owner_name = (payload.get('ownerName') or&#10;                     payload.get('owner_name') or&#10;                     case_data.get('owner_name') or&#10;                     case_data.get('ownerName') or&#10;                     '')&#10;&#10;        # Get document brief&#10;        doc_text = doc_brief.get('brief_text', '')&#10;        doc_count = doc_brief.get('doc_count', 0)&#10;        doc_status = doc_brief.get('doc_status', 'none')&#10;&#10;        # Prepare the conversion-optimized prompt based on document availability&#10;        if doc_status == &quot;ready&quot; and doc_text:&#10;            # NEW: Updated prompt for documents ready - improved headline, why_now, rules_cited, hard_stop, and critical_detail_locked&#10;            user_prompt = f&quot;&quot;&quot;Create a conversion-optimized preview for this HOA dispute case. You must extract specific facts from the document analysis and output ONLY valid JSON.&#10;&#10;Case Details:&#10;- HOA: {hoa_name}&#10;- Violation Type: {violation_type}&#10;- Property Address: {property_address}&#10;- Owner: {owner_name}&#10;- Case Description: {case_description}&#10;&#10;Document Analysis ({doc_count} documents ready):&#10;{doc_text[:3000] if doc_text else 'No document content available'}&#10;&#10;Output ONLY valid JSON with this exact structure:&#10;{{&#10;  &quot;version&quot;: &quot;preview_v2_sales&quot;,&#10;  &quot;headline&quot;: &quot;string (8-14 words, specific to this case. If a deadline is present, headline must include it as 'X-Day Deadline' or the exact date)&quot;,&#10;  &quot;why_now&quot;: &quot;string (1-2 sentences, tie urgency to the required action: inspection + written report + video, and the deadline)&quot;,&#10;  &quot;your_situation&quot;: {{&#10;    &quot;alleged_violation&quot;: &quot;string (extracted from documents)&quot;,&#10;    &quot;hoa_demands&quot;: [&quot;string&quot;, &quot;string&quot;, &quot;...&quot;],&#10;    &quot;deadline&quot;: &quot;string (use exact date if present; else 'Not stated')&quot;,&#10;    &quot;rules_cited&quot;: [&quot;string (each entry must be either 'Paragraph &lt;...&gt;' / 'Section &lt;...&gt;' / 'Article &lt;...&gt;' if present, otherwise 'Not stated'. Do NOT include vague narrative like 'previous notices' unless no citations exist; if included, compress to 'Prior notices (YYYY)')&quot;, &quot;...&quot;]&#10;  }},&#10;  &quot;critical_detail_locked&quot;: {{&#10;    &quot;title&quot;: &quot;Critical Response Wording (Locked)&quot;,&#10;    &quot;body&quot;: &quot;The exact clause/paragraph language to cite, proof checklist (what the HOA will accept: report + video format), and extension request wording (preserves rights without admitting fault) are locked. Our analysis shows specific language about [evidence acceptance/rule interpretation/compliance deadlines] that could weaken your position if worded incorrectly. This critical phrasing is included in the unlock package.&quot;&#10;  }},&#10;  &quot;risk_if_wrong&quot;: [&#10;    &quot;string (specific consequence)&quot;,&#10;    &quot;string&quot;,&#10;    &quot;string&quot;&#10;  ],&#10;  &quot;what_you_get_when_you_unlock&quot;: [&#10;    &quot;Professional response letter tailored to your specific violation&quot;,&#10;    &quot;Certified mail template with proper legal language&quot;,&#10;    &quot;Evidence checklist to document your defense&quot;,&#10;    &quot;Extension request template if deadline is tight&quot;,&#10;    &quot;Negotiation strategies for your specific situation&quot;&#10;  ],&#10;  &quot;hard_stop&quot;: &quot;string (1-2 lines that create unfinished business - must mention 2-3 concrete items such as: exact paragraph language to quote, proof checklist, extension request template)&quot;,&#10;  &quot;cta&quot;: {{&#10;    &quot;primary&quot;: &quot;Unlock full response package&quot;,&#10;    &quot;secondary&quot;: &quot;See exactly what proof the HOA will accept&quot;&#10;  }}&#10;}}&#10;&#10;RULES:&#10;- Extract specific facts from document analysis for your_situation fields&#10;- Use &quot;you&quot; voice throughout&#10;- Be concrete and specific to this case&#10;- Include at least 5 concrete deliverables in what_you_get_when_you_unlock&#10;- Make hard_stop create genuine unfinished business with concrete locked items&#10;- For critical_detail_locked body, reference exact clause language, proof checklist, extension request wording&#10;- Avoid 'admit liability' language unless phrased as 'without admitting fault' or 'without admitting liability'&quot;&quot;&quot;&#10;        else:&#10;            # NEW: Updated prompt for documents pending - improved hard_stop and critical_detail_locked&#10;            docs_pending_text = &quot;docs are still being processed&quot; if doc_status == &quot;processing&quot; else &quot;no documents uploaded yet&quot;&#10;            user_prompt = f&quot;&quot;&quot;Create a conversion-optimized preview for this HOA dispute case. Documents are pending but you must still create a persuasive preview. Output ONLY valid JSON.&#10;&#10;Case Details:&#10;- HOA: {hoa_name}&#10;- Violation Type: {violation_type}&#10;- Property Address: {property_address}&#10;- Owner: {owner_name}&#10;- Case Description: {case_description}&#10;- Document Status: {docs_pending_text}&#10;&#10;Output ONLY valid JSON with this exact structure:&#10;{{&#10;  &quot;version&quot;: &quot;preview_v2_sales&quot;,&#10;  &quot;headline&quot;: &quot;string (8-14 words, specific to this case type)&quot;,&#10;  &quot;why_now&quot;: &quot;string (1-2 sentences, mention documents pending but emphasize urgency)&quot;,&#10;  &quot;your_situation&quot;: {{&#10;    &quot;alleged_violation&quot;: &quot;{violation_type} (details pending document analysis)&quot;,&#10;    &quot;hoa_demands&quot;: [&quot;Pending document analysis&quot;],&#10;    &quot;deadline&quot;: &quot;Not stated - will be extracted from documents&quot;,&#10;    &quot;rules_cited&quot;: [&quot;Pending document analysis&quot;]&#10;  }},&#10;  &quot;critical_detail_locked&quot;: {{&#10;    &quot;title&quot;: &quot;Critical Response Wording (Locked)&quot;,&#10;    &quot;body&quot;: &quot;Exact rule language, deadline extraction, proof checklist, and extension request template are being extracted from your documents. The precise response phrasing that avoids admitting liability and preserves your rights will be available after processing. This includes the exact language needed for compliance responses and extension requests.&quot;&#10;  }},&#10;  &quot;risk_if_wrong&quot;: [&#10;    &quot;Missing critical response deadlines&quot;,&#10;    &quot;Accepting invalid HOA demands without challenge&quot;,&#10;    &quot;Paying unnecessary fines or agreeing to unreasonable compliance&quot;&#10;  ],&#10;  &quot;what_you_get_when_you_unlock&quot;: [&#10;    &quot;Professional response letter once documents are analyzed&quot;,&#10;    &quot;Certified mail template with proper legal language&quot;,&#10;    &quot;Evidence checklist for your specific violation type&quot;,&#10;    &quot;Deadline tracking and extension strategies&quot;,&#10;    &quot;Complete document analysis and defense strategy&quot;&#10;  ],&#10;  &quot;hard_stop&quot;: &quot;Your documents are being analyzed to identify exact rule language, deadline extraction, proof checklist, and extension request template. Once complete, you'll get these specific locked items needed for your response.&quot;,&#10;  &quot;cta&quot;: {{&#10;    &quot;primary&quot;: &quot;Unlock full response package&quot;,&#10;    &quot;secondary&quot;: &quot;Get complete analysis once documents are ready&quot;&#10;  }}&#10;}}&#10;&#10;RULES:&#10;- Use &quot;you&quot; voice throughout&#10;- Be specific about what the unlock will provide once docs are ready&#10;- Make it clear docs are pending but tool is still valuable&#10;- Create urgency around not missing opportunities&#10;- For critical_detail_locked, emphasize that exact response phrasing comes after document analysis&quot;&quot;&quot;&#10;&#10;        headers = {&#10;            'Authorization': f'Bearer {OPENAI_API_KEY}',&#10;            'Content-Type': 'application/json'&#10;        }&#10;&#10;        data = {&#10;            &quot;model&quot;: &quot;gpt-4o-mini&quot;,&#10;            &quot;messages&quot;: [&#10;                {&#10;                    &quot;role&quot;: &quot;system&quot;,&#10;                    &quot;content&quot;: &quot;You write conversion-optimized previews for HOA dispute tool. Output ONLY valid JSON. No explanation, no markdown, just the JSON object.&quot;&#10;                },&#10;                {&#10;                    &quot;role&quot;: &quot;user&quot;,&#10;                    &quot;content&quot;: user_prompt&#10;                }&#10;            ],&#10;            &quot;temperature&quot;: 0.3,&#10;            &quot;max_tokens&quot;: 1200&#10;        }&#10;&#10;        response = requests.post(&#10;            'https://api.openai.com/v1/chat/completions',&#10;            headers=headers,&#10;            json=data,&#10;            timeout=30&#10;        )&#10;        response.raise_for_status()&#10;&#10;        result = response.json()&#10;        json_response = result['choices'][0]['message']['content'].strip()&#10;&#10;        # Calculate latency&#10;        latency_ms = int((time.time() - start_time) * 1000)&#10;&#10;        # Extract token usage&#10;        token_usage = {}&#10;        if 'usage' in result:&#10;            usage = result['usage']&#10;            token_usage = {&#10;                'prompt_tokens': usage.get('prompt_tokens', 0),&#10;                'completion_tokens': usage.get('completion_tokens', 0),&#10;                'total_tokens': usage.get('total_tokens', 0)&#10;            }&#10;&#10;            # Estimate cost (approximate rates for gpt-4o-mini)&#10;            input_cost = (token_usage['prompt_tokens'] / 1000) * 0.00015  # $0.15 per 1K input tokens&#10;            output_cost = (token_usage['completion_tokens'] / 1000) * 0.0006  # $0.60 per 1K output tokens&#10;            token_usage['cost_usd'] = round(input_cost + output_cost, 6)&#10;&#10;        # Parse JSON safely and create markdown&#10;        preview_json = None&#10;        try:&#10;            # Clean the response in case there's extra text&#10;            json_start = json_response.find('{')&#10;            json_end = json_response.rfind('}') + 1&#10;            if json_start &gt;= 0 and json_end &gt; json_start:&#10;                clean_json = json_response[json_start:json_end]&#10;                preview_json = json.loads(clean_json)&#10;&#10;                # NEW: Post-processing to clean up rules_cited narrative&#10;                if preview_json and 'your_situation' in preview_json and 'rules_cited' in preview_json['your_situation']:&#10;                    preview_json['your_situation']['rules_cited'] = clean_rules_cited(preview_json['your_situation']['rules_cited'])&#10;&#10;                # Create markdown from JSON&#10;                markdown_preview = render_preview_markdown(preview_json)&#10;&#10;                logger.info(f&quot;Generated case preview: {len(markdown_preview)} characters, {latency_ms}ms&quot;)&#10;                return markdown_preview, token_usage, latency_ms, preview_json&#10;&#10;            else:&#10;                raise json.JSONDecodeError(&quot;No valid JSON found&quot;, json_response, 0)&#10;&#10;        except json.JSONDecodeError as e:&#10;            logger.warning(f&quot;Failed to parse JSON response: {str(e)}&quot;)&#10;            logger.warning(f&quot;Raw response: {json_response}&quot;)&#10;&#10;            # Fallback to original response as markdown but no preview_json&#10;            markdown_preview = f&quot;# HOA Case Analysis\n\n{json_response}&quot;&#10;            logger.info(f&quot;Generated fallback case preview: {len(markdown_preview)} characters, {latency_ms}ms&quot;)&#10;            return markdown_preview, token_usage, latency_ms, None&#10;&#10;    except Exception as e:&#10;        latency_ms = int((time.time() - start_time) * 1000)&#10;        logger.error(f&quot;Failed to generate case preview: {str(e)}&quot;)&#10;        return f&quot;Error generating preview: {str(e)}&quot;, {}, latency_ms, None&#10;&#10;&#10;def generate_preview_without_documents(case_data: Dict) -&gt; Tuple[str, Dict, int, Optional[Dict]]:&#10;    &quot;&quot;&quot;Generate a conversion-optimized case preview when no documents are available yet.&quot;&quot;&quot;&#10;    start_time = time.time()&#10;&#10;    try:&#10;        if not OPENAI_API_KEY:&#10;            logger.warning(&quot;OpenAI API key not configured&quot;)&#10;            return &quot;Preview generation unavailable - OpenAI not configured&quot;, {}, 0, None&#10;&#10;        # Extract case information from payload&#10;        payload = case_data.get('payload', {})&#10;        if isinstance(payload, str):&#10;            try:&#10;                payload = json.loads(payload)&#10;            except:&#10;                payload = {}&#10;&#10;        hoa_name = payload.get('hoaName', payload.get('hoa_name', 'Unknown HOA'))&#10;        violation_type = payload.get('violationType', payload.get('noticeType', 'Unknown violation'))&#10;        case_description = payload.get('caseDescription', payload.get('case_description', 'No description provided'))&#10;        property_address = payload.get('propertyAddress', payload.get('property_address', ''))&#10;        owner_name = payload.get('ownerName', payload.get('owner_name', ''))&#10;&#10;        # NEW: Updated prompt for no documents case - improved hard_stop and critical_detail_locked&#10;        user_prompt = f&quot;&quot;&quot;Create a conversion-optimized preview for this HOA dispute case. No documents have been uploaded yet, but you must still create a persuasive preview. Output ONLY valid JSON.&#10;&#10;Case Details:&#10;- HOA: {hoa_name}&#10;- Violation Type: {violation_type}&#10;- Property Address: {property_address}&#10;- Owner: {owner_name}&#10;- Case Description: {case_description}&#10;- Document Status: No documents uploaded yet&#10;&#10;Output ONLY valid JSON with this exact structure:&#10;{{&#10;  &quot;version&quot;: &quot;preview_v2_sales&quot;,&#10;  &quot;headline&quot;: &quot;string (8-14 words, specific to this {violation_type} case)&quot;,&#10;  &quot;why_now&quot;: &quot;string (1-2 sentences, create urgency about acting before it's too late)&quot;,&#10;  &quot;your_situation&quot;: {{&#10;    &quot;alleged_violation&quot;: &quot;{violation_type} by {hoa_name}&quot;,&#10;    &quot;hoa_demands&quot;: [&quot;Upload documents to see specific demands&quot;],&#10;    &quot;deadline&quot;: &quot;Unknown - upload documents to identify deadlines&quot;,&#10;    &quot;rules_cited&quot;: [&quot;Upload documents to see specific rules cited&quot;]&#10;  }},&#10;  &quot;critical_detail_locked&quot;: {{&#10;    &quot;title&quot;: &quot;Critical Response Wording (Locked)&quot;,&#10;    &quot;body&quot;: &quot;Exact rule language, deadline extraction, proof checklist, and extension request template will be extracted from your documents. The precise response phrasing that avoids admitting liability and preserves your rights will be available after processing. This includes the exact language needed for compliance responses and extension requests.&quot;&#10;  }},&#10;  &quot;risk_if_wrong&quot;: [&#10;    &quot;Missing critical response deadlines that could escalate penalties&quot;,&#10;    &quot;Accepting invalid HOA demands without proper challenge&quot;,&#10;    &quot;Paying unnecessary fines or agreeing to unreasonable compliance&quot;&#10;  ],&#10;  &quot;what_you_get_when_you_unlock&quot;: [&#10;    &quot;Professional response letter template for {violation_type} cases&quot;,&#10;    &quot;Certified mail template with proper legal language&quot;,&#10;    &quot;Evidence checklist for {violation_type} violations&quot;,&#10;    &quot;Extension request strategies if deadlines are tight&quot;,&#10;    &quot;Complete analysis once you upload your HOA documents&quot;&#10;  ],&#10;  &quot;hard_stop&quot;: &quot;Upload your HOA notice and we'll analyze exact rule language, deadline extraction, proof checklist, and extension request template. You'll get these specific locked items needed for your response.&quot;,&#10;  &quot;cta&quot;: {{&#10;    &quot;primary&quot;: &quot;Unlock full response package&quot;,&#10;    &quot;secondary&quot;: &quot;Upload documents for complete analysis&quot;&#10;  }}&#10;}}&#10;&#10;RULES:&#10;- Use &quot;you&quot; voice throughout&#10;- Be specific about {violation_type} violations&#10;- Create urgency around not delaying action&#10;- Make it clear uploading documents unlocks much more value&#10;- For critical_detail_locked, emphasize that exact response phrasing comes after document analysis&quot;&quot;&quot;&#10;&#10;        headers = {&#10;            'Authorization': f'Bearer {OPENAI_API_KEY}',&#10;            'Content-Type': 'application/json'&#10;        }&#10;&#10;        data = {&#10;            &quot;model&quot;: &quot;gpt-4o-mini&quot;,&#10;            &quot;messages&quot;: [&#10;                {&#10;                    &quot;role&quot;: &quot;system&quot;,&#10;                    &quot;content&quot;: &quot;You write conversion-optimized previews for HOA dispute tool. Output ONLY valid JSON. No explanation, no markdown, just the JSON object.&quot;&#10;                },&#10;                {&#10;                    &quot;role&quot;: &quot;user&quot;,&#10;                    &quot;content&quot;: user_prompt&#10;                }&#10;            ],&#10;            &quot;temperature&quot;: 0.3,&#10;            &quot;max_tokens&quot;: 1000&#10;        }&#10;&#10;        response = requests.post(&#10;            'https://api.openai.com/v1/chat/completions',&#10;            headers=headers,&#10;            json=data,&#10;            timeout=30&#10;        )&#10;        response.raise_for_status()&#10;&#10;        result = response.json()&#10;        json_response = result['choices'][0]['message']['content'].strip()&#10;&#10;        # Calculate latency&#10;        latency_ms = int((time.time() - start_time) * 1000)&#10;&#10;        # Extract token usage&#10;        token_usage = {}&#10;        if 'usage' in result:&#10;            usage = result['usage']&#10;            token_usage = {&#10;                'prompt_tokens': usage.get('prompt_tokens', 0),&#10;                'completion_tokens': usage.get('completion_tokens', 0),&#10;                'total_tokens': usage.get('total_tokens', 0)&#10;            }&#10;&#10;            # Estimate cost (approximate rates for gpt-4o-mini)&#10;            input_cost = (token_usage['prompt_tokens'] / 1000) * 0.00015  # $0.15 per 1K input tokens&#10;            output_cost = (token_usage['completion_tokens'] / 1000) * 0.0006  # $0.60 per 1K output tokens&#10;            token_usage['cost_usd'] = round(input_cost + output_cost, 6)&#10;&#10;        # Parse JSON safely and create markdown&#10;        preview_json = None&#10;        try:&#10;            # Clean the response in case there's extra text&#10;            json_start = json_response.find('{')&#10;            json_end = json_response.rfind('}') + 1&#10;            if json_start &gt;= 0 and json_end &gt; json_start:&#10;                clean_json = json_response[json_start:json_end]&#10;                preview_json = json.loads(clean_json)&#10;&#10;                # NEW: Post-processing to clean up rules_cited narrative&#10;                if preview_json and 'your_situation' in preview_json and 'rules_cited' in preview_json['your_situation']:&#10;                    preview_json['your_situation']['rules_cited'] = clean_rules_cited(preview_json['your_situation']['rules_cited'])&#10;&#10;                # Create markdown from JSON&#10;                markdown_preview = render_preview_markdown(preview_json)&#10;&#10;                logger.info(f&quot;Generated preliminary case preview: {len(markdown_preview)} characters, {latency_ms}ms&quot;)&#10;                return markdown_preview, token_usage, latency_ms, preview_json&#10;&#10;            else:&#10;                raise json.JSONDecodeError(&quot;No valid JSON found&quot;, json_response, 0)&#10;&#10;        except json.JSONDecodeError as e:&#10;            logger.warning(f&quot;Failed to parse JSON response: {str(e)}&quot;)&#10;            logger.warning(f&quot;Raw response: {json_response}&quot;)&#10;&#10;            # Fallback to original response as markdown but no preview_json&#10;            markdown_preview = f&quot;# HOA Case Analysis\n\n{json_response}&quot;&#10;            logger.info(f&quot;Generated fallback preliminary case preview: {len(markdown_preview)} characters, {latency_ms}ms&quot;)&#10;            return markdown_preview, token_usage, latency_ms, None&#10;&#10;    except Exception as e:&#10;        latency_ms = int((time.time() - start_time) * 1000)&#10;        logger.error(f&quot;Failed to generate case preview: {str(e)}&quot;)&#10;        return f&quot;Error generating preview: {str(e)}&quot;, {}, latency_ms, None&#10;&#10;&#10;def auto_generate_case_preview(token: str, case_id: str, force_regenerate: bool = False) -&gt; bool:&#10;    &quot;&quot;&quot;Automatically generate case preview - immediate or deferred based on document status.&quot;&quot;&quot;&#10;    try:&#10;        # NEW: Use improved concurrency guard to prevent duplicate active previews&#10;        if not force_regenerate and not upsert_active_preview_lock(case_id):&#10;            return True  # Skip generation, already handled or in progress&#10;&#10;        try:&#10;            # Fetch case data first&#10;            case = read_case_by_token(token)&#10;            if not case:&#10;                logger.error(f&quot;Case not found for token {token[:12]}...&quot;)&#10;                return False&#10;&#10;            # Check document status&#10;            documents = fetch_ready_documents_by_token(token, limit=5)&#10;            all_documents = fetch_any_documents_status_by_token(token)&#10;&#10;            has_processing_documents = any(doc.get('status') in ['pending', 'processing'] for doc in all_documents)&#10;&#10;            # Check existing preview&#10;            existing_preview = read_active_preview(case_id)&#10;&#10;            # Determine if we need to generate/upgrade preview&#10;            should_generate = False&#10;            preview_type = &quot;preliminary&quot;&#10;&#10;            if not existing_preview:&#10;                # No preview exists - always generate&#10;                should_generate = True&#10;                logger.info(f&quot;No preview exists for case {token[:12]}... - generating new preview&quot;)&#10;            elif force_regenerate:&#10;                # Forced regeneration&#10;                should_generate = True&#10;                logger.info(f&quot;Force regenerating preview for case {token[:12]}...&quot;)&#10;            else:&#10;                # Check if we can upgrade from preliminary to full&#10;                existing_content = existing_preview.get('preview_content', {})&#10;                existing_doc_summary = existing_content.get('doc_summary', {})&#10;                existing_doc_status = existing_doc_summary.get('doc_status', 'none')&#10;&#10;                if documents and existing_doc_status in ['none', 'processing']:&#10;                    # We have ready documents but existing preview doesn't - upgrade to full&#10;                    should_generate = True&#10;                    preview_type = &quot;upgrade&quot;&#10;                    logger.info(f&quot;Upgrading preview from {existing_doc_status} to full for case {token[:12]}...&quot;)&#10;                else:&#10;                    logger.info(f&quot;Preview already exists and up-to-date for case {token[:12]}... - skipping generation&quot;)&#10;                    return True&#10;&#10;            if not should_generate:&#10;                return True&#10;&#10;            # Generate preview based on available data&#10;            preview_json = None&#10;            if documents:&#10;                # Documents are ready - generate full preview with document analysis&#10;                logger.info(f&quot;Generating full preview with {len(documents)} ready documents for case {token[:12]}...&quot;)&#10;                doc_brief = build_doc_brief(documents)&#10;                preview_text, token_usage, latency_ms, preview_json = generate_case_preview_with_openai(case, doc_brief)&#10;                preview_type = &quot;full&quot;&#10;&#10;            elif has_processing_documents:&#10;                # Documents are still processing - generate preliminary preview&#10;                logger.info(f&quot;Documents still processing for case {token[:12]}... - generating preliminary preview&quot;)&#10;                doc_brief = {&#10;                    &quot;doc_status&quot;: &quot;processing&quot;,&#10;                    &quot;doc_count&quot;: len(all_documents),&#10;                    &quot;sources&quot;: [],&#10;                    &quot;brief_text&quot;: &quot;Documents are currently being processed and analyzed.&quot;&#10;                }&#10;                preview_text, token_usage, latency_ms, preview_json = generate_preview_without_documents(case)&#10;                preview_type = &quot;preliminary&quot;&#10;&#10;            else:&#10;                # No documents at all - generate basic preview&#10;                logger.info(f&quot;No documents found for case {token[:12]}... - generating basic preview&quot;)&#10;                doc_brief = {&#10;                    &quot;doc_status&quot;: &quot;none&quot;,&#10;                    &quot;doc_count&quot;: 0,&#10;                    &quot;sources&quot;: [],&#10;                    &quot;brief_text&quot;: &quot;No documents have been uploaded for analysis.&quot;&#10;                }&#10;                preview_text, token_usage, latency_ms, preview_json = generate_preview_without_documents(case)&#10;                preview_type = &quot;basic&quot;&#10;&#10;            # Save preview (this will deactivate existing ones automatically)&#10;            success = save_case_preview_to_new_table(case_id, preview_text, doc_brief, token_usage, latency_ms, preview_json)&#10;&#10;            if success:&#10;                logger.info(f&quot;Successfully generated {preview_type} preview for case {token[:12]}...&quot;)&#10;            else:&#10;                logger.error(f&quot;Failed to save {preview_type} preview for case {token[:12]}...&quot;)&#10;&#10;            return success&#10;&#10;        finally:&#10;            # NEW: Always release the lock when done&#10;            release_preview_lock(case_id)&#10;&#10;    except Exception as e:&#10;        # NEW: Release lock on exception&#10;        release_preview_lock(case_id)&#10;        logger.error(f&quot;Error auto-generating preview for case {token[:12]}...: {str(e)}&quot;)&#10;        return False&#10;&#10;&#10;&#10;@app.route('/webhooks/generate-preview', methods=['POST'])&#10;def generate_preview_webhook():&#10;    &quot;&quot;&quot;Webhook endpoint to generate case preview - can be called after case creation.&quot;&quot;&quot;&#10;    # Validate webhook secret&#10;    webhook_secret = request.headers.get('X-Webhook-Secret')&#10;    if not webhook_secret or webhook_secret != DOC_EXTRACT_WEBHOOK_SECRET:&#10;        logger.warning(&quot;Invalid or missing webhook secret for generate-preview&quot;)&#10;        return jsonify({'error': 'Unauthorized'}), 401&#10;&#10;    try:&#10;        # Parse JSON body&#10;        data = request.get_json()&#10;        if not data:&#10;            return jsonify({'error': 'Invalid JSON body'}), 400&#10;&#10;        # Validate required fields&#10;        token = data.get('token')&#10;        case_id = data.get('case_id')&#10;&#10;        if not token:&#10;            return jsonify({'error': 'Missing required field: token'}), 400&#10;&#10;        logger.info(f&quot;Generating preview for case - Token: {token[:8]}..., Case ID: {case_id}&quot;)&#10;&#10;        # If case_id not provided, look it up by token&#10;        if not case_id:&#10;            case = read_case_by_token(token)&#10;            if not case:&#10;                return jsonify({'error': 'Case not found for token'}), 404&#10;            case_id = case.get('id')&#10;            if not case_id:&#10;                return jsonify({'error': 'Case ID not found'}), 404&#10;&#10;        # Generate preview&#10;        success = auto_generate_case_preview(token, case_id)&#10;&#10;        if success:&#10;            return jsonify({&#10;                'message': 'Preview generated successfully',&#10;                'token': token,&#10;                'case_id': case_id&#10;            }), 200&#10;        else:&#10;            return jsonify({&#10;                'error': 'Failed to generate preview',&#10;                'token': token,&#10;                'case_id': case_id&#10;            }), 500&#10;&#10;    except Exception as e:&#10;        error_msg = f&quot;Unexpected error generating preview: {str(e)}&quot;&#10;        logger.error(error_msg)&#10;        return jsonify({'error': error_msg}), 500&#10;&#10;&#10;def schedule_delayed_preview_generation(token: str, case_id: str, delay_seconds: int = 30):&#10;    &quot;&quot;&quot;Schedule preview generation with a delay to allow documents to be uploaded and processed.&quot;&quot;&quot;&#10;    def delayed_preview():&#10;        time.sleep(delay_seconds)&#10;        try:&#10;            logger.info(f&quot;Executing delayed preview generation for case {token[:8]}...&quot;)&#10;            success = auto_generate_case_preview(token, case_id)&#10;            if success:&#10;                logger.info(f&quot;Delayed preview generation successful for case {token[:8]}...&quot;)&#10;            else:&#10;                logger.warning(f&quot;Delayed preview generation failed for case {token[:8]}...&quot;)&#10;        except Exception as e:&#10;            logger.error(f&quot;Error in delayed preview generation for case {token[:8]}...: {str(e)}&quot;)&#10;&#10;    # Start the delayed task in a background thread&#10;    thread = threading.Thread(target=delayed_preview, daemon=True)&#10;    thread.start()&#10;    logger.info(f&quot;Scheduled delayed preview generation for case {token[:8]}... in {delay_seconds} seconds&quot;)&#10;&#10;&#10;@app.route('/webhooks/case-created', methods=['POST'])&#10;def case_created_webhook():&#10;    &quot;&quot;&quot;Webhook to handle case creation and trigger initial preview generation.&quot;&quot;&quot;&#10;    # Validate webhook secret&#10;    webhook_secret = request.headers.get('X-Webhook-Secret')&#10;    if not webhook_secret or webhook_secret != DOC_EXTRACT_WEBHOOK_SECRET:&#10;        logger.warning(&quot;Invalid or missing webhook secret for case-created&quot;)&#10;        return jsonify({'error': 'Unauthorized'}), 401&#10;&#10;    try:&#10;        # Parse JSON body&#10;        data = request.get_json()&#10;        if not data:&#10;            return jsonify({'error': 'Invalid JSON body'}), 400&#10;&#10;        token = data.get('token')&#10;        case_id = data.get('case_id')&#10;&#10;        if not token:&#10;            return jsonify({'error': 'Missing required field: token'}), 400&#10;&#10;        logger.info(f&quot;Case created - Token: {token[:8]}..., Case ID: {case_id}&quot;)&#10;&#10;        # If case_id not provided, look it up by token&#10;        if not case_id:&#10;            case = read_case_by_token(token)&#10;            if not case:&#10;                return jsonify({'error': 'Case not found for token'}), 404&#10;            case_id = case.get('id')&#10;&#10;        # Generate immediate preview&#10;        immediate_success = auto_generate_case_preview(token, case_id)&#10;&#10;        # Check if there are any documents that might need processing&#10;        all_documents = fetch_any_documents_status_by_token(token)&#10;        pending_or_processing_docs = [doc for doc in all_documents if doc.get('status') in ['pending', 'processing']]&#10;&#10;        # Only schedule delayed jobs if there are documents that might become ready&#10;        if pending_or_processing_docs:&#10;            logger.info(f&quot;Found {len(pending_or_processing_docs)} pending/processing documents - scheduling delayed preview generations&quot;)&#10;            # Delayed: Give time for documents to be uploaded and processed, then regenerate&#10;            schedule_delayed_preview_generation(token, case_id, delay_seconds=60)&#10;            # Also schedule a longer delay for cases where document processing might take longer&#10;            schedule_delayed_preview_generation(token, case_id, delay_seconds=300)  # 5 minutes&#10;        else:&#10;            logger.info(f&quot;No pending/processing documents found - skipping delayed preview generations&quot;)&#10;&#10;        return jsonify({&#10;            'message': 'Case creation handled and preview generation scheduled',&#10;            'token': token,&#10;            'case_id': case_id,&#10;            'immediate_preview': immediate_success,&#10;            'delayed_jobs_scheduled': len(pending_or_processing_docs) &gt; 0&#10;        }), 200&#10;&#10;    except Exception as e:&#10;        error_msg = f&quot;Unexpected error handling case creation: {str(e)}&quot;&#10;        logger.error(error_msg)&#10;        return jsonify({'error': error_msg}), 500&#10;&#10;&#10;def create_case_in_supabase(case_data: Dict) -&gt; Tuple[bool, Optional[str], Optional[str]]:&#10;    &quot;&quot;&quot;Create a new case in Supabase database or update existing one.&quot;&quot;&quot;&#10;    try:&#10;        token = case_data.get('token')&#10;        if not token:&#10;            logger.error(&quot;No token provided for case creation&quot;)&#10;            return False, None, None&#10;&#10;        # First, check if case already exists&#10;        check_url = f&quot;{SUPABASE_URL}/rest/v1/dmhoa_cases&quot;&#10;        check_params = {'token': f'eq.{token}', 'select': 'id,token,status'}&#10;        check_headers = supabase_headers()&#10;&#10;        check_response = requests.get(check_url, params=check_params, headers=check_headers, timeout=TIMEOUT)&#10;        check_response.raise_for_status()&#10;        existing_cases = check_response.json()&#10;&#10;        # Get email from the case data (check multiple possible field names)&#10;        email = (case_data.get('email') or&#10;                case_data.get('ownerEmail') or&#10;                case_data.get('owner_email') or&#10;                case_data.get('userEmail') or&#10;                case_data.get('user_email'))&#10;&#10;        # Prepare case data according to the actual table structure&#10;        case_payload = {&#10;            'token': token,&#10;            'email': email,&#10;            'payload': case_data,  # Store the entire case data as JSONB in payload column&#10;            'status': 'preview'  # Set default status&#10;        }&#10;&#10;        # Remove None/empty values except for payload which should always be included&#10;        case_payload = {k: v for k, v in case_payload.items() if v is not None and (k == 'payload' or v)}&#10;&#10;        if existing_cases and len(existing_cases) &gt; 0:&#10;            # Case exists, update it&#10;            existing_case = existing_cases[0]&#10;            case_id = existing_case.get('id')&#10;&#10;            logger.info(f&quot;Case with token {token[:8]}... already exists, updating with ID: {case_id}&quot;)&#10;&#10;            # Update existing case&#10;            update_url = f&quot;{SUPABASE_URL}/rest/v1/dmhoa_cases&quot;&#10;            update_params = {'id': f'eq.{case_id}'}&#10;            update_headers = supabase_headers()&#10;            update_headers['Prefer'] = 'return=representation'&#10;&#10;            # Only update payload and email, preserve other fields&#10;            update_payload = {&#10;                'payload': case_data,&#10;                'email': email&#10;            }&#10;            update_payload = {k: v for k, v in update_payload.items() if v is not None}&#10;&#10;            update_response = requests.patch(update_url, params=update_params, headers=update_headers, json=update_payload, timeout=TIMEOUT)&#10;            update_response.raise_for_status()&#10;&#10;            result = update_response.json()&#10;            if result and len(result) &gt; 0:&#10;                logger.info(f&quot;Updated existing case successfully - ID: {case_id}, Token: {token[:8]}...&quot;)&#10;                return True, case_id, token&#10;            else:&#10;                logger.warning(f&quot;Update succeeded but no data returned for case {case_id}&quot;)&#10;                return True, case_id, token&#10;&#10;        else:&#10;            # Case doesn't exist, create new one&#10;            logger.info(f&quot;Creating new case with token {token[:8]}...&quot;)&#10;&#10;            url = f&quot;{SUPABASE_URL}/rest/v1/dmhoa_cases&quot;&#10;            headers = supabase_headers()&#10;            headers['Prefer'] = 'return=representation'&#10;&#10;            logger.info(f&quot;Creating case with payload keys: {list(case_payload.keys())}&quot;)&#10;&#10;            response = requests.post(url, headers=headers, json=case_payload, timeout=TIMEOUT)&#10;            response.raise_for_status()&#10;&#10;            result = response.json()&#10;            if result and len(result) &gt; 0:&#10;                case_id = result[0].get('id')&#10;                token_returned = result[0].get('token')&#10;                logger.info(f&quot;Created case successfully - ID: {case_id}, Token: {token_returned[:8]}...&quot;)&#10;                return True, case_id, token_returned&#10;            else:&#10;                logger.error(&quot;No case data returned from Supabase&quot;)&#10;                return False, None, None&#10;&#10;    except Exception as e:&#10;        logger.error(f&quot;Failed to create/update case in Supabase: {str(e)}&quot;)&#10;        return False, None, None&#10;&#10;&#10;@app.route('/api/save-case', methods=['POST', 'OPTIONS'])&#10;def save_case():&#10;    &quot;&quot;&quot;Save a new case and trigger preview generation.&quot;&quot;&quot;&#10;    # Handle CORS preflight&#10;    if request.method == 'OPTIONS':&#10;        response = jsonify({'message': 'OK'})&#10;        return response&#10;&#10;    try:&#10;        # Parse JSON body&#10;        case_data = request.get_json()&#10;        if not case_data:&#10;            return jsonify({'error': 'Invalid JSON body'}), 400&#10;&#10;        # Log the incoming data for debugging&#10;        logger.info(f&quot;Received case data: {json.dumps(case_data, indent=2)}&quot;)&#10;&#10;        # Handle nested payload structure - check both top level and inside payload&#10;        payload_data = case_data.get('payload', {})&#10;&#10;        # Combine top-level fields and payload fields for field extraction&#10;        combined_data = {**case_data, **payload_data}&#10;&#10;        logger.info(f&quot;Combined data fields: {list(combined_data.keys())}&quot;)&#10;&#10;        # Handle multiple possible field name variations from combined data&#10;        hoa_name = (combined_data.get('hoaName') or&#10;                   combined_data.get('hoa_name') or&#10;                   combined_data.get('HOAName') or&#10;                   combined_data.get('organizationName') or&#10;                   combined_data.get('organization_name'))&#10;&#10;        violation_type = (combined_data.get('violationType') or&#10;                         combined_data.get('violation_type') or&#10;                         combined_data.get('ViolationType') or&#10;                         combined_data.get('noticeType') or&#10;                         combined_data.get('notice_type') or&#10;                         combined_data.get('violationCategory'))&#10;&#10;        token = combined_data.get('token')&#10;&#10;        # Log extracted values&#10;        logger.info(f&quot;Extracted values - Token: {token}, HOA: {hoa_name}, Violation: {violation_type}&quot;)&#10;&#10;        # If we don't have HOA name or violation type, try to infer from other fields&#10;        if not hoa_name:&#10;            # Try to get organization info from other fields&#10;            hoa_name = (combined_data.get('organization') or&#10;                       combined_data.get('company') or&#10;                       combined_data.get('entity') or&#10;                       &quot;Unknown HOA&quot;)&#10;            logger.info(f&quot;Inferred HOA name: {hoa_name}&quot;)&#10;&#10;        if not violation_type:&#10;            # Use noticeType or infer from context&#10;            violation_type = (combined_data.get('noticeType') or&#10;                             combined_data.get('issueType') or&#10;                             combined_data.get('category') or&#10;                             &quot;violation&quot;)  # Default fallback&#10;            logger.info(f&quot;Using violation type: {violation_type}&quot;)&#10;&#10;        # Validate required fields with more flexible requirements&#10;        missing_fields = []&#10;        if not token:&#10;            missing_fields.append('token')&#10;&#10;        if missing_fields:&#10;            error_msg = f'Missing required fields: {&quot;, &quot;.join(missing_fields)}'&#10;            logger.error(f&quot;Validation failed: {error_msg}&quot;)&#10;            logger.error(f&quot;Available fields in combined data: {list(combined_data.keys())}&quot;)&#10;            return jsonify({'error': error_msg}), 400&#10;&#10;        # Create the final case data structure&#10;        if 'payload' in case_data and isinstance(case_data['payload'], dict):&#10;            # If payload exists, use it as the base and add normalized fields&#10;            final_case_data = case_data['payload'].copy()&#10;            final_case_data['token'] = token&#10;            final_case_data['hoaName'] = hoa_name&#10;            final_case_data['violationType'] = violation_type&#10;        else:&#10;            # Otherwise use the combined data&#10;            final_case_data = combined_data.copy()&#10;            final_case_data['token'] = token&#10;            final_case_data['hoaName'] = hoa_name&#10;            final_case_data['violationType'] = violation_type&#10;&#10;        logger.info(f&quot;Saving case - Token: {token[:8]}..., HOA: {hoa_name}, Violation: {violation_type}&quot;)&#10;        logger.info(f&quot;Final case data keys: {list(final_case_data.keys())}&quot;)&#10;&#10;        # Create case in database&#10;        success, case_id, returned_token = create_case_in_supabase(final_case_data)&#10;&#10;        if not success:&#10;            return jsonify({&#10;                'error': 'Failed to create case in database'&#10;            }), 500&#10;&#10;        # NEW: Direct preview generation trigger instead of HTTP call to localhost&#10;        try:&#10;            if case_id:&#10;                logger.info(f&quot;Triggering direct preview generation for case {token[:8]}...&quot;)&#10;&#10;                # Call auto_generate_case_preview directly instead of making HTTP request&#10;                immediate_success = auto_generate_case_preview(token, case_id, force_regenerate=False)&#10;&#10;                # Check if there are any documents that might need processing&#10;                all_documents = fetch_any_documents_status_by_token(token)&#10;                pending_or_processing_docs = [doc for doc in all_documents if doc.get('status') in ['pending', 'processing']]&#10;&#10;                # Only schedule delayed jobs if there are documents that might become ready&#10;                if pending_or_processing_docs:&#10;                    logger.info(f&quot;Found {len(pending_or_processing_docs)} pending/processing documents - scheduling delayed preview generations&quot;)&#10;                    # Delayed: Give time for documents to be uploaded and processed, then regenerate&#10;                    schedule_delayed_preview_generation(token, case_id, delay_seconds=60)&#10;                    # Also schedule a longer delay for cases where document processing might take longer&#10;                    schedule_delayed_preview_generation(token, case_id, delay_seconds=300)  # 5 minutes&#10;                else:&#10;                    logger.info(f&quot;No pending/processing documents found - skipping delayed preview generations&quot;)&#10;&#10;                if immediate_success:&#10;                    logger.info(f&quot;Successfully generated immediate preview for case {token[:8]}...&quot;)&#10;                else:&#10;                    logger.warning(f&quot;Failed to generate immediate preview for case {token[:8]}...&quot;)&#10;        except Exception as e:&#10;            logger.warning(f&quot;Failed to trigger preview generation: {str(e)}&quot;)&#10;            # Don't fail the main request if preview generation fails&#10;&#10;        response_data = {&#10;            'success': True,&#10;            'message': 'Case saved successfully',&#10;            'token': token,&#10;            'case_id': case_id&#10;        }&#10;&#10;        return jsonify(response_data), 200&#10;&#10;    except Exception as e:&#10;        error_msg = f&quot;Unexpected error saving case: {str(e)}&quot;&#10;        logger.error(error_msg)&#10;        return jsonify({'error': error_msg}), 500&#10;&#10;&#10;@app.route('/api/case-preview/&lt;case_id&gt;', methods=['GET', 'OPTIONS'])&#10;def get_case_preview(case_id):&#10;    &quot;&quot;&quot;Get the active case preview for frontend display.&quot;&quot;&quot;&#10;    # Handle CORS preflight&#10;    if request.method == 'OPTIONS':&#10;        response = jsonify({'message': 'OK'})&#10;        return response&#10;&#10;    try:&#10;        # Validate case_id format (basic UUID check)&#10;        if not case_id or len(case_id) &lt; 10:&#10;            return jsonify({'error': 'Invalid case ID format'}), 400&#10;&#10;        logger.info(f&quot;Fetching preview for case ID: {case_id}&quot;)&#10;&#10;        # Get the active preview from database&#10;        preview_data = read_active_preview(case_id)&#10;&#10;        if not preview_data:&#10;            return jsonify({&#10;                'error': 'No active preview found for this case',&#10;                'case_id': case_id&#10;            }), 404&#10;&#10;        # Extract the structured data for frontend&#10;        preview_content = preview_data.get('preview_content', {})&#10;        preview_json = preview_content.get('preview_json', {})&#10;&#10;        # Build frontend response with all the data the frontend needs&#10;        frontend_response = {&#10;            'case_id': case_id,&#10;            'preview_id': preview_data.get('id'),&#10;            'preview_snippet': preview_data.get('preview_snippet'),&#10;            'generated_at': preview_content.get('generated_at'),&#10;            'doc_status': preview_content.get('doc_summary', {}).get('doc_status', 'none'),&#10;            'doc_count': preview_content.get('doc_summary', {}).get('doc_count', 0),&#10;&#10;            # Main preview content from preview_json&#10;            'headline': preview_json.get('headline', 'HOA Case Analysis'),&#10;            'why_now': preview_json.get('why_now', ''),&#10;            'your_situation': preview_json.get('your_situation', {}),&#10;            'risk_if_wrong': preview_json.get('risk_if_wrong', []),&#10;            'what_you_get_when_you_unlock': preview_json.get('what_you_get_when_you_unlock', []),&#10;            'critical_detail_locked': preview_json.get('critical_detail_locked', {}),&#10;            'cta': preview_json.get('cta', {&#10;                'primary': 'Unlock full response package',&#10;                'secondary': 'See detailed analysis'&#10;            }),&#10;&#10;            # Full preview text (markdown)&#10;            'preview_text': preview_content.get('preview_text', ''),&#10;&#10;            # Metadata&#10;            'model': preview_data.get('model', 'gpt-4o-mini'),&#10;            'prompt_version': preview_data.get('prompt_version', 'v2.0_sales'),&#10;            'is_active': preview_data.get('is_active', True)&#10;        }&#10;&#10;        logger.info(f&quot;Successfully retrieved preview for case {case_id}&quot;)&#10;        return jsonify(frontend_response), 200&#10;&#10;    except Exception as e:&#10;        error_msg = f&quot;Error fetching case preview: {str(e)}&quot;&#10;        logger.error(error_msg)&#10;        return jsonify({'error': error_msg}), 500&#10;&#10;&#10;@app.route('/api/case-preview/by-token/&lt;token&gt;', methods=['GET', 'OPTIONS'])&#10;def get_case_preview_by_token(token):&#10;    &quot;&quot;&quot;Get case preview by token (alternative endpoint for frontend convenience).&quot;&quot;&quot;&#10;    # Handle CORS preflight&#10;    if request.method == 'OPTIONS':&#10;        response = jsonify({'message': 'OK'})&#10;        return response&#10;&#10;    try:&#10;        # Validate token format&#10;        if not token or len(token) &lt; 10:&#10;            return jsonify({'error': 'Invalid token format'}), 400&#10;&#10;        logger.info(f&quot;Fetching case by token: {token[:8]}...&quot;)&#10;&#10;        # First get the case to find the case_id&#10;        case = read_case_by_token(token)&#10;        if not case:&#10;            return jsonify({&#10;                'error': 'Case not found for token',&#10;                'token': token[:8] + '...'&#10;            }), 404&#10;&#10;        case_id = case.get('id')&#10;        if not case_id:&#10;            return jsonify({'error': 'Case ID not found'}), 404&#10;&#10;        # Now get the preview using the case_id&#10;        return get_case_preview(case_id)&#10;&#10;    except Exception as e:&#10;        error_msg = f&quot;Error fetching case preview by token: {str(e)}&quot;&#10;        logger.error(error_msg)&#10;        return jsonify({'error': error_msg}), 500&#10;&#10;&#10;# NEW: Concurrency guard function to prevent duplicate preview generation&#10;def upsert_active_preview_lock(case_id: str) -&gt; bool:&#10;    &quot;&quot;&quot;&#10;    Check if preview generation should proceed to prevent duplicates.&#10;    Returns True if generation should proceed, False if should skip.&#10;    &quot;&quot;&quot;&#10;    with preview_lock:&#10;        # Check if another thread is already generating for this case&#10;        if case_id in preview_generation_locks:&#10;            logger.info(f&quot;Preview generation already in progress for case {case_id}&quot;)&#10;            return False&#10;&#10;        # Check existing active preview and doc status&#10;        existing_preview = read_active_preview(case_id)&#10;        if existing_preview:&#10;            existing_content = existing_preview.get('preview_content', {})&#10;            existing_doc_summary = existing_content.get('doc_summary', {})&#10;            existing_doc_status = existing_doc_summary.get('doc_status', 'none')&#10;&#10;            # If we already have a &quot;ready&quot; status preview, skip generation&#10;            if existing_doc_status == &quot;ready&quot;:&#10;                logger.info(f&quot;Preview with ready docs already exists for case {case_id}&quot;)&#10;                return False&#10;&#10;        # Mark this case as being processed&#10;        preview_generation_locks[case_id] = True&#10;        return True&#10;&#10;def release_preview_lock(case_id: str):&#10;    &quot;&quot;&quot;Release the preview generation lock for a case.&quot;&quot;&quot;&#10;    with preview_lock:&#10;        preview_generation_locks.pop(case_id, None)&#10;&#10;&#10;@app.route('/api/create-checkout-session', methods=['POST', 'OPTIONS'])&#10;def create_checkout_session():&#10;    &quot;&quot;&quot;Create a Stripe checkout session for case purchase.&quot;&quot;&quot;&#10;    # Handle CORS preflight&#10;    if request.method == 'OPTIONS':&#10;        response = jsonify({'message': 'OK'})&#10;        return response&#10;&#10;    try:&#10;        # Validate required environment variables&#10;        if not STRIPE_SECRET_KEY:&#10;            logger.error(&quot;STRIPE_SECRET_KEY not configured&quot;)&#10;            return jsonify({'error': 'Payment system not configured'}), 500&#10;&#10;        if not STRIPE_PRICE_ID:&#10;            logger.error(&quot;STRIPE_PRICE_ID not configured&quot;)&#10;            return jsonify({'error': 'Product pricing not configured'}), 500&#10;&#10;        # Get request data&#10;        data = request.get_json()&#10;        if not data:&#10;            return jsonify({'error': 'No data provided'}), 400&#10;&#10;        # Accept either case_id or case_token for flexibility&#10;        case_id = data.get('case_id')&#10;        case_token = data.get('case_token') or data.get('token')&#10;&#10;        logger.info(f&quot;Checkout request received - case_id: {case_id}, case_token: {case_token}&quot;)&#10;&#10;        if not case_id and not case_token:&#10;            return jsonify({'error': 'case_id or case_token is required'}), 400&#10;&#10;        # If we have a token but no case_id, look up the case_id by token&#10;        if case_token and not case_id:&#10;            case = read_case_by_token(case_token)&#10;            if not case:&#10;                return jsonify({'error': 'Case not found for token'}), 404&#10;            case_id = case.get('id')&#10;            if not case_id:&#10;                return jsonify({'error': 'Case ID not found for token'}), 404&#10;        elif case_id:&#10;            # Validate case exists by case_id&#10;            case = read_case_by_id(case_id)&#10;            if not case:&#10;                return jsonify({'error': 'Case not found'}), 404&#10;        else:&#10;            return jsonify({'error': 'Unable to identify case'}), 400&#10;&#10;        # Create Stripe checkout session&#10;        try:&#10;            # Use the correct frontend URL for development/staging&#10;            frontend_url = &quot;https://dmhoadev.netlify.app&quot;  # Use your actual frontend domain&#10;&#10;            checkout_session = stripe.checkout.Session.create(&#10;                payment_method_types=['card'],&#10;                line_items=[{&#10;                    'price': STRIPE_PRICE_ID,&#10;                    'quantity': 1,&#10;                }],&#10;                mode='payment',&#10;                success_url=f&quot;{frontend_url}/success?case_id={case_id}&amp;session_id={{CHECKOUT_SESSION_ID}}&quot;,&#10;                cancel_url=f&quot;{frontend_url}/case-preview?case={case_id}&quot;,&#10;                metadata={&#10;                    'case_id': case_id,&#10;                    'case_token': case.get('token', ''),&#10;                }&#10;            )&#10;&#10;            logger.info(f&quot;Created checkout session {checkout_session.id} for case {case_id}&quot;)&#10;&#10;            # Create response with multiple field names for frontend compatibility&#10;            response_data = {&#10;                'checkout_url': checkout_session.url,&#10;                'url': checkout_session.url,  # Alternative field name&#10;                'session_id': checkout_session.id,&#10;                'id': checkout_session.id,  # Alternative field name&#10;                'success': True&#10;            }&#10;&#10;            logger.info(f&quot;Returning checkout response: {response_data}&quot;)&#10;            return jsonify(response_data), 200&#10;&#10;        except stripe.error.StripeError as e:&#10;            logger.error(f&quot;Stripe error creating checkout session: {str(e)}&quot;)&#10;            return jsonify({'error': 'Payment system error'}), 500&#10;&#10;    except Exception as e:&#10;        error_msg = f&quot;Error creating checkout session: {str(e)}&quot;&#10;        logger.error(error_msg)&#10;        return jsonify({'error': error_msg}), 500&#10;&#10;&#10;@app.route('/webhooks/stripe', methods=['POST'])&#10;def stripe_webhook():&#10;    &quot;&quot;&quot;Handle Stripe webhook events, particularly checkout.session.completed.&quot;&quot;&quot;&#10;    payload = request.get_data()&#10;    sig_header = request.headers.get('Stripe-Signature')&#10;&#10;    try:&#10;        # Verify webhook signature&#10;        if not STRIPE_WEBHOOK_SECRET:&#10;            logger.error(&quot;STRIPE_WEBHOOK_SECRET not configured&quot;)&#10;            return jsonify({'error': 'Webhook secret not configured'}), 500&#10;&#10;        event = stripe.Webhook.construct_event(&#10;            payload, sig_header, STRIPE_WEBHOOK_SECRET&#10;        )&#10;    except ValueError as e:&#10;        logger.error(f&quot;Invalid payload: {e}&quot;)&#10;        return jsonify({'error': 'Invalid payload'}), 400&#10;    except stripe.error.SignatureVerificationError as e:&#10;        logger.error(f&quot;Invalid signature: {e}&quot;)&#10;        return jsonify({'error': 'Invalid signature'}), 400&#10;&#10;    # Handle the event&#10;    if event['type'] == 'checkout.session.completed':&#10;        session = event['data']['object']&#10;        logger.info(f&quot;Payment completed for session: {session['id']}&quot;)&#10;&#10;        # Extract metadata&#10;        metadata = session.get('metadata', {})&#10;        case_id = metadata.get('case_id')&#10;        case_token = metadata.get('case_token')&#10;&#10;        if not case_id:&#10;            logger.error(f&quot;No case_id in session metadata: {session['id']}&quot;)&#10;            return jsonify({'error': 'No case_id in metadata'}), 400&#10;&#10;        # Get case info&#10;        case = read_case_by_id(case_id)&#10;        if not case:&#10;            logger.error(f&quot;Case not found for ID: {case_id}&quot;)&#10;            return jsonify({'error': 'Case not found'}), 404&#10;&#10;        # Get the token from case if not in metadata&#10;        if not case_token:&#10;            case_token = case.get('token')&#10;&#10;        if not case_token:&#10;            logger.error(f&quot;No token found for case: {case_id}&quot;)&#10;            return jsonify({'error': 'No token found for case'}), 400&#10;&#10;        # STEP 1: Mark the case as unlocked/paid in the database&#10;        try:&#10;            case_update_url = f&quot;{SUPABASE_URL}/rest/v1/dmhoa_cases&quot;&#10;            case_update_params = {'id': f'eq.{case_id}'}&#10;            # Only update fields that exist in the database schema&#10;            case_update_data = {&#10;                'status': 'paid',&#10;                'updated_at': datetime.utcnow().isoformat()&#10;            }&#10;            case_update_headers = supabase_headers()&#10;&#10;            update_response = requests.patch(case_update_url, params=case_update_params,&#10;                                           headers=case_update_headers, json=case_update_data, timeout=TIMEOUT)&#10;            update_response.raise_for_status()&#10;&#10;            logger.info(f&quot;Successfully marked case {case_id} as paid&quot;)&#10;&#10;        except Exception as e:&#10;            logger.error(f&quot;Failed to mark case {case_id} as paid: {str(e)}&quot;)&#10;            # Log the full response for debugging&#10;            try:&#10;                logger.error(f&quot;Response status: {update_response.status_code}&quot;)&#10;                logger.error(f&quot;Response text: {update_response.text}&quot;)&#10;            except:&#10;                pass&#10;            return jsonify({'error': 'Failed to update case status'}), 500&#10;&#10;        # STEP 2: Generate the actual full case analysis using GPT (asynchronously)&#10;        logger.info(f&quot;Scheduling full case analysis for paid case {case_token}&quot;)&#10;&#10;        try:&#10;            # Schedule the analysis generation in a background thread to avoid webhook timeout&#10;            def generate_analysis_async():&#10;                try:&#10;                    logger.info(f&quot;Starting background analysis generation for {case_token}&quot;)&#10;                    full_analysis_result = generate_full_case_analysis_internal(case_token)&#10;&#10;                    if not full_analysis_result['success']:&#10;                        logger.error(f&quot;Background analysis failed for {case_token}: {full_analysis_result['error']}&quot;)&#10;                        return&#10;&#10;                    logger.info(f&quot;Successfully generated background analysis for {case_token}&quot;)&#10;&#10;                    # Add payment metadata to the existing output&#10;                    case_url = f&quot;{SITE_URL}/case.html?case={case_token}&amp;session_id={session['id']}&quot;&#10;&#10;                    # Update the case outputs with payment information&#10;                    try:&#10;                        outputs_update_url = f&quot;{SUPABASE_URL}/rest/v1/dmhoa_case_outputs&quot;&#10;                        outputs_update_params = {'case_token': f'eq.{case_token}'}&#10;&#10;                        # Get the current outputs to merge with payment info&#10;                        current_outputs = full_analysis_result['outputs']&#10;&#10;                        # Add payment metadata to the outputs&#10;                        enhanced_outputs = {&#10;                            **current_outputs,&#10;                            'payment_info': {&#10;                                'case_url': case_url,&#10;                                'session_id': session['id'],&#10;                                'payment_amount': session.get('amount_total'),&#10;                                'currency': session.get('currency'),&#10;                                'customer_email': session.get('customer_details', {}).get('email'),&#10;                                'payment_completed_at': datetime.utcnow().isoformat()&#10;                            }&#10;                        }&#10;&#10;                        outputs_update_data = {&#10;                            'outputs': enhanced_outputs,&#10;                            'updated_at': datetime.utcnow().isoformat()&#10;                        }&#10;                        outputs_update_headers = supabase_headers()&#10;&#10;                        outputs_update_response = requests.patch(outputs_update_url, params=outputs_update_params,&#10;                                                                headers=outputs_update_headers, json=outputs_update_data, timeout=TIMEOUT)&#10;                        outputs_update_response.raise_for_status()&#10;&#10;                        logger.info(f&quot;Successfully enhanced case outputs with payment info for {case_token}&quot;)&#10;                        logger.info(f&quot;Case URL: {case_url}&quot;)&#10;&#10;                    except Exception as e:&#10;                        logger.error(f&quot;Failed to enhance outputs with payment info: {str(e)}&quot;)&#10;                        # Don't fail the whole process if this fails - the analysis is already generated&#10;&#10;                except Exception as e:&#10;                    logger.error(f&quot;Error in background analysis generation for {case_token}: {str(e)}&quot;)&#10;&#10;            # Start the analysis in a background thread&#10;            analysis_thread = threading.Thread(target=generate_analysis_async, daemon=True)&#10;            analysis_thread.start()&#10;&#10;            logger.info(f&quot;Successfully scheduled background analysis generation for {case_token}&quot;)&#10;&#10;        except Exception as e:&#10;            logger.error(f&quot;Failed to schedule analysis generation for {case_token}: {str(e)}&quot;)&#10;            # Don't fail the webhook response - we can retry later&#10;&#10;    else:&#10;        logger.info(f&quot;Unhandled event type: {event['type']}&quot;)&#10;&#10;    return jsonify({'status': 'success'}), 200&#10;&#10;&#10;def generate_full_case_analysis_internal(case_token: str) -&gt; Dict[str, Any]:&#10;    &quot;&quot;&quot;&#10;    Generate a comprehensive GPT-powered case analysis for a paid case.&#10;    This replaces the preview with a full, detailed analysis.&#10;    &quot;&quot;&quot;&#10;    logger.info(f&quot;Starting internal case analysis for token: {case_token}&quot;)&#10;&#10;    try:&#10;        # Fetch documents for this case&#10;        docs = fetch_ready_documents_by_token(case_token, limit=5)&#10;        if not docs:&#10;            logger.error(f&quot;No ready documents found for token: {case_token}&quot;)&#10;            return {'success': False, 'error': 'No documents available for analysis'}&#10;&#10;        # Get case information&#10;        case_info = read_case_by_token(case_token)&#10;        if not case_info:&#10;            logger.error(f&quot;Case not found for token: {case_token}&quot;)&#10;            return {'success': False, 'error': 'Case not found'}&#10;&#10;        # Prepare document content for GPT&#10;        usable_docs = [d for d in docs if d.get('extracted_text') and len(d.get('extracted_text', '').strip()) &gt; 100]&#10;&#10;        if not usable_docs:&#10;            logger.error(f&quot;No usable documents with extracted text for token: {case_token}&quot;)&#10;            return {'success': False, 'error': 'No usable documents with text content'}&#10;&#10;        docs_newest = max(d.get('updated_at', '') for d in docs)&#10;        docs_block = &quot;\n\n&quot;.join([&#10;            f&quot;=== Document {i+1} ===\n&quot;&#10;            f&quot;Filename: {d.get('filename', 'Unknown')}\n&quot;&#10;            f&quot;Pages: {d.get('page_count', 'Unknown')}\n&quot;&#10;            f&quot;Characters: {d.get('char_count', 'Unknown')}\n\n&quot;&#10;            f&quot;{d.get('extracted_text', '').strip()}&quot;&#10;            for i, d in enumerate(usable_docs)&#10;        ])&#10;&#10;        # Prepare case payload for GPT&#10;        payload = {&#10;            'case_token': case_token,&#10;            'customer_email': case_info.get('customer_email'),&#10;            'documents_count': len(docs),&#10;            'usable_documents_count': len(usable_docs)&#10;        }&#10;&#10;        # Define draft titles - these are the three main response types we generate&#10;        draft_titles = {&#10;            'clarification': 'Request for Clarification and Documentation',&#10;            'extension': 'Request for Deadline Extension',&#10;            'compliance': 'Compliance Response and Action Plan'&#10;        }&#10;&#10;        # Define the JSON schema for structured output&#10;        schema = {&#10;            &quot;type&quot;: &quot;object&quot;,&#10;            &quot;additionalProperties&quot;: False,&#10;            &quot;properties&quot;: {&#10;                &quot;summary_html&quot;: {&quot;type&quot;: &quot;string&quot;},&#10;                &quot;letter_summary&quot;: {&quot;type&quot;: &quot;string&quot;},&#10;                &quot;draft_titles&quot;: {&#10;                    &quot;type&quot;: &quot;object&quot;,&#10;                    &quot;additionalProperties&quot;: False,&#10;                    &quot;properties&quot;: {&#10;                        &quot;clarification&quot;: {&quot;type&quot;: &quot;string&quot;},&#10;                        &quot;extension&quot;: {&quot;type&quot;: &quot;string&quot;},&#10;                        &quot;compliance&quot;: {&quot;type&quot;: &quot;string&quot;}&#10;                    },&#10;                    &quot;required&quot;: [&quot;clarification&quot;, &quot;extension&quot;, &quot;compliance&quot;]&#10;                },&#10;                &quot;risks_and_deadlines&quot;: {&#10;                    &quot;type&quot;: &quot;object&quot;,&#10;                    &quot;additionalProperties&quot;: False,&#10;                    &quot;properties&quot;: {&#10;                        &quot;deadlines&quot;: {&quot;type&quot;: &quot;array&quot;, &quot;items&quot;: {&quot;type&quot;: &quot;string&quot;}, &quot;minItems&quot;: 1},&#10;                        &quot;risks&quot;: {&quot;type&quot;: &quot;array&quot;, &quot;items&quot;: {&quot;type&quot;: &quot;string&quot;}, &quot;minItems&quot;: 3}&#10;                    },&#10;                    &quot;required&quot;: [&quot;deadlines&quot;, &quot;risks&quot;]&#10;                },&#10;                &quot;action_plan&quot;: {&quot;type&quot;: &quot;array&quot;, &quot;items&quot;: {&quot;type&quot;: &quot;string&quot;}, &quot;minItems&quot;: 6},&#10;                &quot;drafts&quot;: {&#10;                    &quot;type&quot;: &quot;object&quot;,&#10;                    &quot;additionalProperties&quot;: False,&#10;                    &quot;properties&quot;: {&#10;                        &quot;clarification&quot;: {&quot;type&quot;: &quot;string&quot;},&#10;                        &quot;extension&quot;: {&quot;type&quot;: &quot;string&quot;},&#10;                        &quot;compliance&quot;: {&quot;type&quot;: &quot;string&quot;}&#10;                    },&#10;                    &quot;required&quot;: [&quot;clarification&quot;, &quot;extension&quot;, &quot;compliance&quot;]&#10;                },&#10;                &quot;questions_to_ask&quot;: {&quot;type&quot;: &quot;array&quot;, &quot;items&quot;: {&quot;type&quot;: &quot;string&quot;}, &quot;minItems&quot;: 6},&#10;                &quot;lowest_cost_path&quot;: {&quot;type&quot;: &quot;array&quot;, &quot;items&quot;: {&quot;type&quot;: &quot;string&quot;}, &quot;minItems&quot;: 4}&#10;            },&#10;            &quot;required&quot;: [&#10;                &quot;summary_html&quot;, &quot;letter_summary&quot;, &quot;draft_titles&quot;, &quot;risks_and_deadlines&quot;,&#10;                &quot;action_plan&quot;, &quot;drafts&quot;, &quot;questions_to_ask&quot;, &quot;lowest_cost_path&quot;&#10;            ]&#10;        }&#10;&#10;        doc_fingerprint = {&#10;            'count': len(docs),&#10;            'usableCount': len(usable_docs),&#10;            'newestUpdatedAt': docs_newest,&#10;            'ids': [d['id'] for d in docs],&#10;            'statuses': [d.get('status') for d in docs],&#10;            'charCounts': [d.get('char_count') for d in docs]&#10;        }&#10;&#10;        # Prepare messages for GPT&#10;        messages = [&#10;            {&#10;                &quot;role&quot;: &quot;system&quot;,&#10;                &quot;content&quot;: &quot;&quot;&quot;You are an expert HOA dispute assistant helping homeowners respond to violations and disputes.&#10;&#10;Your job is to analyze the homeowner's situation and provide comprehensive, actionable guidance including:&#10;1. A detailed HTML summary of their situation&#10;2. Three complete draft response letters (clarification, extension, compliance)  &#10;3. Risk assessment with specific deadlines&#10;4. Step-by-step action plan&#10;5. Strategic questions to ask the HOA&#10;6. Lowest-cost resolution path&#10;&#10;OUTPUT RULES (CRITICAL):&#10;- ONLY &quot;summary_html&quot; may contain HTML tags&#10;- summary_html must use ONLY these HTML tags: &lt;div&gt;, &lt;strong&gt;, &lt;ul&gt;, &lt;li&gt;, &lt;p&gt;&#10;- ALL draft letters MUST be PLAIN TEXT with \\n for line breaks&#10;- Each draft must be a complete, professional letter ready to send&#10;- Include specific facts, dates, amounts, and references from the documents&#10;- Be professional but firm in tone&#10;&#10;TONE AND POSITIONING RULES (VERY IMPORTANT):&#10;- NEVER admit fault, liability, or acknowledge violations in any draft&#10;- Use non-admitting language: &quot;responding to your notice&quot; instead of &quot;acknowledging the violation&quot;&#10;- For compliance drafts, say &quot;addressing the Association's stated concerns&quot; not &quot;acknowledging the violation&quot;&#10;- For extension requests, provide specific, factual reasons (vendor availability, scheduling constraints) not vague excuses&#10;- Preserve homeowner's rights while showing cooperation&#10;- Frame responses as addressing concerns, not admitting wrongdoing&#10;&#10;DRAFT REQUIREMENTS:&#10;Each draft letter must include:&#10;- Professional subject line&#10;- Proper greeting and introduction&#10;- Specific references to HOA rules/violations mentioned (without admission)&#10;- Clear requests with deadlines&#10;- Professional closing requesting written confirmation&#10;- Homeowner's rights and procedural protections&#10;&#10;SPECIFIC LANGUAGE GUIDANCE:&#10;- Compliance Draft: &quot;I am responding to the notice regarding [issue] and am taking steps to address the Association's stated concerns&quot;&#10;- Extension Draft: &quot;Due to vendor availability and inspection scheduling constraints&quot; (not &quot;unforeseen circumstances&quot;)&#10;- Never use phrases like &quot;I acknowledge the violation&quot; or &quot;I admit fault&quot;&#10;- Always frame as &quot;responding to notice&quot; or &quot;addressing concerns&quot;&#10;&#10;Make this comprehensive - worth the $29 the customer paid.&quot;&quot;&quot;&#10;            },&#10;            {&#10;                &quot;role&quot;: &quot;user&quot;,&#10;                &quot;content&quot;: f&quot;&quot;&quot;Please analyze this HOA case and provide comprehensive assistance.&#10;&#10;CASE INFORMATION:&#10;{json.dumps(payload, indent=2)}&#10;&#10;DOCUMENT ANALYSIS:&#10;{docs_block}&#10;&#10;REQUIRED DRAFT TYPES:&#10;- Clarification: &quot;{draft_titles['clarification']}&quot;&#10;- Extension: &quot;{draft_titles['extension']}&quot;  &#10;- Compliance: &quot;{draft_titles['compliance']}&quot;&#10;&#10;Please provide a thorough analysis with actionable advice, specific draft letters, and strategic guidance. Focus on protecting the homeowner's rights while working toward resolution.&quot;&quot;&quot;&#10;            }&#10;        ]&#10;&#10;        # Make OpenAI API call using the correct chat completions endpoint&#10;        openai_payload = {&#10;            &quot;model&quot;: &quot;gpt-4o-mini&quot;,&#10;            &quot;messages&quot;: messages,&#10;            &quot;response_format&quot;: {&#10;                &quot;type&quot;: &quot;json_schema&quot;,&#10;                &quot;json_schema&quot;: {&#10;                    &quot;name&quot;: &quot;dmhoa_case_analysis&quot;,&#10;                    &quot;strict&quot;: True,&#10;                    &quot;schema&quot;: schema&#10;                }&#10;            },&#10;            &quot;temperature&quot;: 0.7&#10;        }&#10;&#10;        logger.info(f&quot;Making OpenAI API call for case analysis: {case_token}&quot;)&#10;&#10;        try:&#10;            openai_response = requests.post(&#10;                'https://api.openai.com/v1/chat/completions',&#10;                headers={&#10;                    'Authorization': f'Bearer {OPENAI_API_KEY}',&#10;                    'Content-Type': 'application/json'&#10;                },&#10;                json=openai_payload,&#10;                timeout=(10, 180)  # 3 minute timeout for complex analysis&#10;            )&#10;        except requests.exceptions.Timeout as e:&#10;            logger.error(f&quot;OpenAI API call timed out for case {case_token}: {str(e)}&quot;)&#10;            return {'success': False, 'error': f'OpenAI API call timed out: {str(e)}'}&#10;        except requests.exceptions.RequestException as e:&#10;            logger.error(f&quot;OpenAI API call failed for case {case_token}: {str(e)}&quot;)&#10;            return {'success': False, 'error': f'OpenAI API call failed: {str(e)}'}&#10;&#10;        if not openai_response.ok:&#10;            error_text = openai_response.text&#10;            logger.error(f'OpenAI API call failed for {case_token}: {error_text}')&#10;            return {'success': False, 'error': f'OpenAI API failed: {error_text}'}&#10;&#10;        openai_json = openai_response.json()&#10;&#10;        # Extract the structured result&#10;        try:&#10;            message_content = openai_json['choices'][0]['message']['content']&#10;            structured_result = json.loads(message_content)&#10;        except (KeyError, json.JSONDecodeError) as e:&#10;            logger.error(f'Failed to parse OpenAI response for {case_token}: {str(e)}')&#10;            return {'success': False, 'error': f'Failed to parse AI response: {str(e)}'}&#10;&#10;        # Prepare final output&#10;        outputs_to_store = {&#10;            **structured_result,&#10;            'draft_titles': structured_result.get('draft_titles', draft_titles),&#10;            'doc_fingerprint': doc_fingerprint,&#10;            'generated_at': datetime.utcnow().isoformat(),&#10;            'generation_source': 'post_payment_analysis'&#10;        }&#10;&#10;        # Save the full analysis to dmhoa_case_outputs&#10;        upsert_url = f&quot;{SUPABASE_URL}/rest/v1/dmhoa_case_outputs&quot;&#10;        upsert_headers = supabase_headers()&#10;        upsert_headers['Prefer'] = 'resolution=merge-duplicates'&#10;&#10;        success_data = {&#10;            'case_token': case_token,&#10;            'status': 'ready',&#10;            'outputs': outputs_to_store,&#10;            'error': None,&#10;            'model': 'gpt-4o-mini',&#10;            'prompt_version': 'v4_post_payment',&#10;            'updated_at': datetime.utcnow().isoformat()&#10;        }&#10;&#10;        try:&#10;            save_response = requests.post(upsert_url, headers=upsert_headers, json=success_data, timeout=TIMEOUT)&#10;            save_response.raise_for_status()&#10;            logger.info(f'Successfully saved full case analysis for {case_token}')&#10;        except Exception as e:&#10;            logger.error(f'Failed to save case outputs for {case_token}: {str(e)}')&#10;            return {'success': False, 'error': f'Failed to save analysis: {str(e)}'}&#10;&#10;        # Update case timestamp&#10;        try:&#10;            case_update_url = f&quot;{SUPABASE_URL}/rest/v1/dmhoa_cases&quot;&#10;            case_update_params = {'token': f'eq.{case_token}'}&#10;            case_update_data = {'updated_at': datetime.utcnow().isoformat()}&#10;            case_update_headers = supabase_headers()&#10;            requests.patch(case_update_url, params=case_update_params,&#10;                         headers=case_update_headers, json=case_update_data, timeout=TIMEOUT)&#10;        except Exception:&#10;            pass  # Best effort&#10;&#10;        return {'success': True, 'outputs': outputs_to_store}&#10;&#10;    except Exception as e:&#10;        logger.error(f'Error generating full case analysis for {case_token}: {str(e)}')&#10;        return {'success': False, 'error': f'Analysis generation failed: {str(e)}'}&#10;" />
              <option name="updatedContent" value="# Flask app with CORS fixes deployed on January 18, 2026&#10;import os&#10;import io&#10;import json&#10;import logging&#10;from typing import Dict, Any, Optional, Tuple, List&#10;import re&#10;from datetime import datetime&#10;import threading&#10;import time&#10;&#10;import requests&#10;from flask import Flask, request, jsonify&#10;from flask_cors import CORS&#10;from pypdf import PdfReader&#10;import stripe&#10;&#10;import smtplib&#10;from email.mime.text import MIMEText&#10;from email.mime.multipart import MIMEMultipart&#10;&#10;# Image processing and OCR imports&#10;from PIL import Image&#10;import pytesseract&#10;&#10;&#10;# Configure logging&#10;logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')&#10;logger = logging.getLogger(__name__)&#10;&#10;app = Flask(__name__)&#10;# NEW: Updated CORS configuration with comprehensive headers and methods&#10;CORS(app, resources={r&quot;/*&quot;: {&quot;origins&quot;: &quot;*&quot;}},&#10;     supports_credentials=False,&#10;     allow_headers=[&quot;Content-Type&quot;, &quot;Authorization&quot;, &quot;apikey&quot;, &quot;x-client-info&quot;, &quot;x-supabase-api-version&quot;, &quot;X-Webhook-Secret&quot;],&#10;     methods=[&quot;GET&quot;, &quot;POST&quot;, &quot;PUT&quot;, &quot;DELETE&quot;, &quot;OPTIONS&quot;])&#10;&#10;# Configuration&#10;SUPABASE_URL = os.environ.get('SUPABASE_URL')&#10;SUPABASE_SERVICE_ROLE_KEY = os.environ.get('SUPABASE_SERVICE_ROLE_KEY')&#10;DOC_EXTRACT_WEBHOOK_SECRET = os.environ.get('DOC_EXTRACT_WEBHOOK_SECRET')&#10;&#10;# Stripe Configuration&#10;STRIPE_SECRET_KEY = os.environ.get('STRIPE_SECRET_KEY')&#10;STRIPE_WEBHOOK_SECRET = os.environ.get('STRIPE_WEBHOOK_SECRET')&#10;STRIPE_PRICE_ID = os.environ.get('STRIPE_PRICE_ID')&#10;SITE_URL = os.environ.get('SITE_URL', 'https://disputemyhoa.com')&#10;&#10;# OpenAI Configuration&#10;OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')&#10;&#10;# Configure Stripe&#10;if STRIPE_SECRET_KEY:&#10;    stripe.api_key = STRIPE_SECRET_KEY&#10;&#10;# SMTP Configuration - Optional, only needed for email functionality&#10;SMTP_HOST = os.environ.get(&quot;SMTP_HOST&quot;)&#10;SMTP_PORT = int(os.environ.get(&quot;SMTP_PORT&quot;, &quot;587&quot;))&#10;# Clean SMTP credentials to handle non-ASCII characters like non-breaking spaces&#10;SMTP_USER = (os.environ.get(&quot;SMTP_USER&quot;) or &quot;&quot;).strip().replace('\xa0', ' ')&#10;SMTP_PASS = (os.environ.get(&quot;SMTP_PASS&quot;) or &quot;&quot;).strip().replace('\xa0', ' ')&#10;SMTP_FROM = os.environ.get(&quot;SMTP_FROM&quot;, &quot;support@disputemyhoa.com&quot;)&#10;&#10;SMTP_SENDER_WEBHOOK_SECRET = os.environ.get(&quot;SMTP_SENDER_WEBHOOK_SECRET&quot;)&#10;SMTP_SENDER_WEBHOOK_URL = os.environ.get(&quot;SMTP_SENDER_WEBHOOK_URL&quot;)&#10;&#10;# Request timeouts&#10;TIMEOUT = (5, 60)  # (connect, read)&#10;&#10;# NEW: In-process lock to prevent duplicate preview generation&#10;preview_generation_locks = {}&#10;preview_lock = threading.Lock()&#10;&#10;def supabase_headers() -&gt; Dict[str, str]:&#10;    &quot;&quot;&quot;Return headers for Supabase API requests.&quot;&quot;&quot;&#10;    return {&#10;        'apikey': SUPABASE_SERVICE_ROLE_KEY,&#10;        'Authorization': f'Bearer {SUPABASE_SERVICE_ROLE_KEY}',&#10;        'Content-Type': 'application/json'&#10;    }&#10;&#10;def fetch_ready_documents_by_token(token: str, limit: int = 3) -&gt; List[Dict]:&#10;    &quot;&quot;&quot;Query Supabase for ready documents by case token.&quot;&quot;&quot;&#10;    try:&#10;        url = f&quot;{SUPABASE_URL}/rest/v1/dmhoa_documents&quot;&#10;        params = {&#10;            'token': f'eq.{token}',&#10;            'status': 'eq.ready',&#10;            'select': 'id,filename,mime_type,page_count,char_count,updated_at,extracted_text',&#10;            'order': 'updated_at.desc',&#10;            'limit': str(limit)&#10;        }&#10;        headers = supabase_headers()&#10;&#10;        response = requests.get(url, params=params, headers=headers, timeout=TIMEOUT)&#10;        response.raise_for_status()&#10;&#10;        documents = response.json()&#10;        logger.info(f&quot;Found {len(documents)} ready documents for token {token[:12]}...&quot;)&#10;        return documents&#10;&#10;    except Exception as e:&#10;        logger.error(f&quot;Failed to fetch ready documents for token {token[:12]}...: {str(e)}&quot;)&#10;        return []&#10;&#10;def fetch_any_documents_status_by_token(token: str) -&gt; List[Dict]:&#10;    &quot;&quot;&quot;Check for any documents (including processing/pending) by token.&quot;&quot;&quot;&#10;    try:&#10;        url = f&quot;{SUPABASE_URL}/rest/v1/dmhoa_documents&quot;&#10;        params = {&#10;            'token': f'eq.{token}',&#10;            'select': 'id,status,updated_at',&#10;            'order': 'updated_at.desc'&#10;        }&#10;        headers = supabase_headers()&#10;&#10;        response = requests.get(url, params=params, headers=headers, timeout=TIMEOUT)&#10;        response.raise_for_status()&#10;&#10;        return response.json()&#10;&#10;    except Exception as e:&#10;        logger.error(f&quot;Failed to fetch document status for token {token[:12]}...: {str(e)}&quot;)&#10;        return []&#10;&#10;def summarize_doc_text_with_openai(token: str, raw_text: str) -&gt; str:&#10;    &quot;&quot;&quot;Summarize document text using OpenAI gpt-4o-mini.&quot;&quot;&quot;&#10;    try:&#10;        # Clip text to avoid token limits&#10;        clipped_text = raw_text[:12000] if raw_text else &quot;&quot;&#10;&#10;        if not clipped_text.strip():&#10;            return &quot;&quot;&#10;&#10;        headers = {&#10;            'Authorization': f'Bearer {OPENAI_API_KEY}',&#10;            'Content-Type': 'application/json'&#10;        }&#10;&#10;        prompt = &quot;&quot;&quot;Summarize the HOA notice into: (a) alleged violation, (b) what HOA demands, (c) deadlines/fines/next actions, (d) evidence/rules cited. Be factual, quote exact deadlines/amounts when present. 8-12 bullets max.&quot;&quot;&quot;&#10;&#10;        data = {&#10;            &quot;model&quot;: &quot;gpt-4o-mini&quot;,&#10;            &quot;messages&quot;: [&#10;                {&#10;                    &quot;role&quot;: &quot;system&quot;,&#10;                    &quot;content&quot;: &quot;You are a document summarizer for HOA notices. Extract key facts concisely.&quot;&#10;                },&#10;                {&#10;                    &quot;role&quot;: &quot;user&quot;,&#10;                    &quot;content&quot;: f&quot;{prompt}\n\nDocument text:\n{clipped_text}&quot;&#10;                }&#10;            ],&#10;            &quot;temperature&quot;: 0.3,&#10;            &quot;max_tokens&quot;: 800&#10;        }&#10;&#10;        response = requests.post(&#10;            'https://api.openai.com/v1/chat/completions',&#10;            headers=headers,&#10;            json=data,&#10;            timeout=30&#10;        )&#10;        response.raise_for_status()&#10;&#10;        result = response.json()&#10;        summary = result['choices'][0]['message']['content'].strip()&#10;&#10;        # Limit summary length&#10;        if len(summary) &gt; 1200:&#10;            summary = summary[:1200] + &quot;...&quot;&#10;&#10;        logger.info(f&quot;Successfully summarized document for token {token[:12]}...&quot;)&#10;        return summary&#10;&#10;    except Exception as e:&#10;        logger.error(f&quot;Failed to summarize document text for token {token[:12]}...: {str(e)}&quot;)&#10;        # Fallback to clipped excerpt&#10;        return raw_text[:1200] + &quot;...&quot; if len(raw_text) &gt; 1200 else raw_text&#10;&#10;def build_doc_brief(docs: List[Dict]) -&gt; Dict:&#10;    &quot;&quot;&quot;Build document brief from ready documents.&quot;&quot;&quot;&#10;    if not docs:&#10;        return {&#10;            &quot;doc_status&quot;: &quot;none&quot;,&#10;            &quot;doc_count&quot;: 0,&#10;            &quot;sources&quot;: [],&#10;            &quot;brief_text&quot;: &quot;&quot;&#10;        }&#10;&#10;    sources = []&#10;    raw_texts = []&#10;&#10;    for doc in docs:&#10;        sources.append({&#10;            &quot;filename&quot;: doc.get(&quot;filename&quot;, &quot;unknown&quot;),&#10;            &quot;page_count&quot;: doc.get(&quot;page_count&quot;, 0),&#10;            &quot;char_count&quot;: doc.get(&quot;char_count&quot;, 0)&#10;        })&#10;&#10;        # Extract text with limits&#10;        extracted_text = doc.get(&quot;extracted_text&quot;, &quot;&quot;)&#10;        if extracted_text:&#10;            # Limit per document to 6000 chars&#10;            doc_text = extracted_text[:6000]&#10;            raw_texts.append(doc_text)&#10;&#10;    if not raw_texts:&#10;        return {&#10;            &quot;doc_status&quot;: &quot;ready&quot;,&#10;            &quot;doc_count&quot;: len(docs),&#10;            &quot;sources&quot;: sources,&#10;            &quot;brief_text&quot;: &quot;&quot;&#10;        }&#10;&#10;    # Combine texts with total limit of 12000 chars&#10;    combined_text = &quot; &quot;.join(raw_texts)&#10;    if len(combined_text) &gt; 12000:&#10;        combined_text = combined_text[:12000]&#10;&#10;    # Try to summarize with OpenAI&#10;    token = docs[0].get(&quot;token&quot;, &quot;unknown&quot;) if docs else &quot;unknown&quot;&#10;    brief_text = summarize_doc_text_with_openai(token, combined_text)&#10;&#10;    return {&#10;        &quot;doc_status&quot;: &quot;ready&quot;,&#10;        &quot;doc_count&quot;: len(docs),&#10;        &quot;sources&quot;: sources,&#10;        &quot;brief_text&quot;: brief_text&#10;    }&#10;&#10;&#10;def fetch_document_status(document_id: str) -&gt; Optional[Dict[str, Any]]:&#10;    &quot;&quot;&quot;Fetch current document status from Supabase.&quot;&quot;&quot;&#10;    try:&#10;        url = f&quot;{SUPABASE_URL}/rest/v1/dmhoa_documents&quot;&#10;        params = {&#10;            'id': f'eq.{document_id}',&#10;            'select': 'id,token,status'&#10;        }&#10;        headers = supabase_headers()&#10;&#10;        response = requests.get(url, params=params, headers=headers, timeout=TIMEOUT)&#10;        response.raise_for_status()&#10;&#10;        data = response.json()&#10;        return data[0] if data else None&#10;&#10;    except Exception as e:&#10;        logger.error(f&quot;Failed to fetch document status for {document_id}: {str(e)}&quot;)&#10;        return None&#10;&#10;def update_document(document_id: str, token: str, updates: Dict[str, Any]) -&gt; bool:&#10;    &quot;&quot;&quot;Update document in Supabase database.&quot;&quot;&quot;&#10;    try:&#10;        url = f&quot;{SUPABASE_URL}/rest/v1/dmhoa_documents&quot;&#10;        params = {&#10;            'id': f'eq.{document_id}',&#10;            'token': f'eq.{token}'&#10;        }&#10;        headers = supabase_headers()&#10;        headers['Prefer'] = 'return=representation'&#10;&#10;        response = requests.patch(url, params=params, headers=headers,&#10;                                json=updates, timeout=TIMEOUT)&#10;        response.raise_for_status()&#10;&#10;        logger.info(f&quot;Updated document {document_id} with: {updates}&quot;)&#10;        return True&#10;&#10;    except Exception as e:&#10;        logger.error(f&quot;Failed to update document {document_id}: {str(e)}&quot;)&#10;        return False&#10;&#10;def download_storage_object(bucket: str, path: str) -&gt; Optional[bytes]:&#10;    &quot;&quot;&quot;Download file from Supabase Storage.&quot;&quot;&quot;&#10;    try:&#10;        url = f&quot;{SUPABASE_URL}/storage/v1/object/{bucket}/{path}&quot;&#10;        headers = {&#10;            'apikey': SUPABASE_SERVICE_ROLE_KEY,&#10;            'Authorization': f'Bearer {SUPABASE_SERVICE_ROLE_KEY}'&#10;        }&#10;&#10;        response = requests.get(url, headers=headers, timeout=TIMEOUT)&#10;&#10;        if response.status_code == 404:&#10;            logger.error(f&quot;File not found: {bucket}/{path}&quot;)&#10;            return None&#10;&#10;        response.raise_for_status()&#10;&#10;        logger.info(f&quot;Downloaded {len(response.content)} bytes from {bucket}/{path}&quot;)&#10;        return response.content&#10;&#10;    except Exception as e:&#10;        logger.error(f&quot;Failed to download {bucket}/{path}: {str(e)}&quot;)&#10;        return None&#10;&#10;def extract_pdf_text(pdf_bytes: bytes) -&gt; Tuple[str, int, int, Optional[str]]:&#10;    &quot;&quot;&quot;&#10;    Extract text from PDF bytes.&#10;    Returns: (extracted_text, page_count, char_count, error_message)&#10;    &quot;&quot;&quot;&#10;    try:&#10;        reader = PdfReader(io.BytesIO(pdf_bytes))&#10;        page_count = len(reader.pages)&#10;&#10;        text_parts = []&#10;        for page in reader.pages:&#10;            try:&#10;                page_text = page.extract_text() or &quot;&quot;&#10;                text_parts.append(page_text)&#10;            except Exception as e:&#10;                logger.warning(f&quot;Failed to extract text from page: {str(e)}&quot;)&#10;                text_parts.append(&quot;&quot;)&#10;&#10;        extracted_text = &quot;\n\n&quot;.join(text_parts)&#10;        char_count = len(extracted_text)&#10;&#10;        # Check if text is empty or only whitespace&#10;        if not extracted_text or extracted_text.strip() == &quot;&quot;:&#10;            return &quot;&quot;, page_count, 0, &quot;No text layer found - document may be scanned and require OCR&quot;&#10;&#10;        logger.info(f&quot;Extracted {char_count} characters from {page_count} pages&quot;)&#10;        return extracted_text, page_count, char_count, None&#10;&#10;    except Exception as e:&#10;        error_msg = f&quot;Failed to extract text from PDF: {str(e)}&quot;&#10;        logger.error(error_msg)&#10;        return &quot;&quot;, 0, 0, error_msg&#10;&#10;&#10;def extract_image_text(image_bytes: bytes, filename: str = &quot;&quot;) -&gt; Tuple[str, int, int, Optional[str]]:&#10;    &quot;&quot;&quot;&#10;    Extract text from image bytes using OCR.&#10;    Returns: (extracted_text, page_count=1, char_count, error_message)&#10;    &quot;&quot;&quot;&#10;    try:&#10;        # Open image from bytes&#10;        image = Image.open(io.BytesIO(image_bytes))&#10;&#10;        # Convert to RGB if necessary (some formats like RGBA or P need conversion)&#10;        if image.mode not in ('RGB', 'L'):&#10;            image = image.convert('RGB')&#10;&#10;        logger.info(f&quot;Processing image: {image.size[0]}x{image.size[1]} pixels, mode: {image.mode}&quot;)&#10;&#10;        # Set TESSDATA_PREFIX if not already set (for Heroku compatibility)&#10;        if 'TESSDATA_PREFIX' not in os.environ:&#10;            possible_paths = [&#10;                '/usr/share/tesseract-ocr/5/tessdata',&#10;                '/usr/share/tesseract-ocr/tessdata',&#10;                '/usr/share/tesseract-ocr/4.00/tessdata',&#10;                '/usr/share/tesseract-ocr/4/tessdata',&#10;                '/usr/share/tesseract-ocr/tessdata',&#10;                '/usr/share/tessdata',&#10;                '/app/.apt/usr/share/tesseract-ocr/5/tessdata',&#10;                '/app/.apt/usr/share/tesseract-ocr/tessdata'&#10;            ]&#10;            for path in possible_paths:&#10;                if os.path.exists(path):&#10;                    os.environ['TESSDATA_PREFIX'] = path&#10;                    logger.info(f&quot;Set TESSDATA_PREFIX to: {path}&quot;)&#10;                    break&#10;            else:&#10;                logger.warning(&quot;Could not find tessdata directory in any expected location&quot;)&#10;&#10;        # Try multiple OCR configurations for better compatibility&#10;        configs_to_try = [&#10;            r'--oem 1 --psm 1 -l eng',   # Automatic page segmentation with OSD&#10;            r'--oem 1 --psm 3 -l eng',   # Fully automatic page segmentation, but no OSD&#10;            r'--oem 1 --psm 4 -l eng',   # Assume a single column of text of variable sizes&#10;            r'--oem 1 --psm 6 -l eng',   # Assume a single uniform block of text&#10;            r'--oem 3 --psm 1 -l eng',   # LSTM with automatic page segmentation&#10;            r'--oem 3 --psm 3 -l eng',   # LSTM with fully automatic page segmentation&#10;            r'--oem 3 --psm 6 -l eng',   # LSTM standard config&#10;            r'--oem 3 --psm 11 -l eng',  # Sparse text - find as much text as possible&#10;            r'--oem 3 --psm 12 -l eng',  # Sparse text with OSD&#10;            r'--psm 6',                  # No language specified fallback&#10;        ]&#10;&#10;        extracted_text = &quot;&quot;&#10;        best_text = &quot;&quot;&#10;        best_char_count = 0&#10;        last_error = None&#10;&#10;        for config in configs_to_try:&#10;            try:&#10;                logger.info(f&quot;Trying OCR with config: {config}&quot;)&#10;                current_text = pytesseract.image_to_string(image, config=config)&#10;                current_text = current_text.strip()&#10;                char_count = len(current_text);&#10;&#10;                # Keep track of the best result (most text that looks reasonable)&#10;                if char_count &gt; best_char_count:&#10;                    # Basic heuristic: prefer results with more alphanumeric content&#10;                    alphanumeric_ratio = sum(c.isalnum() or c.isspace() for c in current_text) / max(len(current_text), 1)&#10;                    if alphanumeric_ratio &gt; 0.3:  # At least 30% should be readable characters&#10;                        best_text = current_text&#10;                        best_char_count = char_count&#10;                        logger.info(f&quot;New best result: {char_count} chars, {alphanumeric_ratio:.2f} alphanumeric ratio&quot;)&#10;&#10;                # If we got a decent amount of readable text, we can stop&#10;                if char_count &gt; 50 and best_text:&#10;                    extracted_text = best_text&#10;                    break&#10;&#10;            except Exception as e:&#10;                last_error = str(e)&#10;                logger.warning(f&quot;OCR config failed: {config}, error: {str(e)}&quot;)&#10;                continue&#10;&#10;        # Use the best result we found&#10;        if not extracted_text and best_text:&#10;            extracted_text = best_text&#10;&#10;        # Clean up the extracted text&#10;        extracted_text = extracted_text.strip()&#10;        char_count = len(extracted_text)&#10;&#10;        if char_count == 0:&#10;            error_msg = f&quot;No text found in image - image may be blank or contain no readable text. Last OCR error: {last_error}&quot;&#10;            return &quot;&quot;, 1, 0, error_msg&#10;&#10;        logger.info(f&quot;OCR extracted {char_count} characters from image {filename}&quot;)&#10;        return extracted_text, 1, char_count, None&#10;&#10;    except Exception as e:&#10;        error_msg = f&quot;Failed to extract text from image: {str(e)}&quot;&#10;        logger.error(error_msg)&#10;        return &quot;&quot;, 0, 0, error_msg&#10;&#10;&#10;def is_image_file(filename: str, mime_type: str = &quot;&quot;) -&gt; bool:&#10;    &quot;&quot;&quot;Check if file is a supported image format.&quot;&quot;&quot;&#10;    image_extensions = {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.tif', '.webp'}&#10;    image_mime_types = {&#10;        'image/jpeg', 'image/jpg', 'image/png', 'image/gif',&#10;        'image/bmp', 'image/tiff', 'image/webp'&#10;    }&#10;&#10;    # Check by file extension&#10;    if filename:&#10;        ext = os.path.splitext(filename.lower())[1]&#10;        if ext in image_extensions:&#10;            return True&#10;&#10;    # Check by MIME type&#10;    if mime_type and mime_type.lower() in image_mime_types:&#10;        return True&#10;&#10;    return False&#10;&#10;&#10;def is_pdf_file(filename: str, mime_type: str = &quot;&quot;) -&gt; bool:&#10;    &quot;&quot;&quot;Check if file is a PDF.&quot;&quot;&quot;&#10;    if filename and filename.lower().endswith('.pdf'):&#10;        return True&#10;    if mime_type and mime_type.lower() == 'application/pdf':&#10;        return True&#10;    return False&#10;&#10;&#10;@app.route('/health', methods=['GET'])&#10;def health_check():&#10;    &quot;&quot;&quot;Health check endpoint.&quot;&quot;&quot;&#10;    return jsonify({'status': 'healthy'}), 200&#10;&#10;&#10;@app.route('/debug/env', methods=['GET'])&#10;def debug_env():&#10;    &quot;&quot;&quot;Return presence of critical env vars (gated by DOC_EXTRACT_WEBHOOK_SECRET).&#10;&#10;    This does NOT return any secret values, only boolean flags indicating whether each&#10;    required configuration item is set. Intended for quick diagnostics on deployed app.&#10;    &quot;&quot;&quot;&#10;    secret = request.headers.get('X-Webhook-Secret')&#10;    if not secret or secret != DOC_EXTRACT_WEBHOOK_SECRET:&#10;        logger.warning(&quot;Unauthorized request to /debug/env&quot;)&#10;        return jsonify({'error': 'Unauthorized'}), 401&#10;&#10;    keys = [&#10;        'SUPABASE_URL', 'SUPABASE_SERVICE_ROLE_KEY', 'DOC_EXTRACT_WEBHOOK_SECRET',&#10;        'SMTP_HOST', 'SMTP_PORT', 'SMTP_USER', 'SMTP_PASS', 'SMTP_FROM', 'SMTP_SENDER_WEBHOOK_SECRET'&#10;    ]&#10;    presence = {k: bool(os.environ.get(k)) for k in keys}&#10;    logger.info(f&quot;/debug/env requested; presence: { {k: presence[k] for k in presence} }&quot;)&#10;    return jsonify({'env_presence': presence}), 200&#10;&#10;@app.route('/webhooks/doc-extract', methods=['POST'])&#10;def doc_extract_webhook():&#10;    &quot;&quot;&quot;Main webhook endpoint for document extraction.&quot;&quot;&quot;&#10;    # Validate webhook secret&#10;    webhook_secret = request.headers.get('X-Webhook-Secret')&#10;    if not webhook_secret or webhook_secret != DOC_EXTRACT_WEBHOOK_SECRET:&#10;        logger.warning(&quot;Invalid or missing webhook secret&quot;)&#10;        return jsonify({'error': 'Unauthorized'}), 401&#10;&#10;    document_id = None&#10;    token = None&#10;&#10;    try:&#10;        # Parse JSON body&#10;        data = request.get_json()&#10;        if not data:&#10;            return jsonify({'error': 'Invalid JSON body'}), 400&#10;&#10;        # Validate required fields&#10;        required_fields = ['token', 'document_id', 'bucket', 'path']&#10;        missing_fields = [field for field in required_fields if not data.get(field)]&#10;        if missing_fields:&#10;            return jsonify({&#10;                'error': f'Missing required fields: {&quot;, &quot;.join(missing_fields)}'&#10;            }), 400&#10;&#10;        token = data['token']&#10;        document_id = data['document_id']&#10;        bucket = data['bucket']&#10;        path = data['path']&#10;        filename = data.get('filename', '') or ''  # Handle null values&#10;        mime_type = data.get('mime_type', '') or ''  # Handle null values&#10;&#10;        logger.info(f&quot;Processing document extraction - ID: {document_id}, Token: {token[:8]}...&quot;)&#10;&#10;        # Check if document is already processed&#10;        current_doc = fetch_document_status(document_id)&#10;        if current_doc and current_doc.get('status') == 'ready':&#10;            logger.info(f&quot;Document {document_id} already processed&quot;)&#10;            return jsonify({&#10;                'message': 'Document already processed',&#10;                'document_id': document_id,&#10;                'status': 'ready'&#10;            }), 200&#10;&#10;        # Mark document as processing&#10;        if not update_document(document_id, token, {'status': 'processing'}):&#10;            return jsonify({&#10;                'error': 'Failed to update document status to processing',&#10;                'document_id': document_id&#10;            }), 500&#10;&#10;        # Download file from Supabase Storage&#10;        file_bytes = download_storage_object(bucket, path)&#10;        if file_bytes is None:&#10;            error_msg = f&quot;Failed to download file from {bucket}/{path}&quot;&#10;            update_document(document_id, token, {&#10;                'status': 'failed',&#10;                'error': error_msg[:2000]&#10;            })&#10;            return jsonify({&#10;                'error': error_msg,&#10;                'document_id': document_id&#10;            }), 500&#10;&#10;        # Determine file type with multiple fallback strategies&#10;        logger.info(f&quot;Initial file detection - filename: '{filename}', mime_type: '{mime_type}'&quot;)&#10;&#10;        # Strategy 1: If filename is empty/null, extract from path&#10;        if not filename or filename.lower() == 'null':&#10;            filename = os.path.basename(path)&#10;            logger.info(f&quot;Extracted filename from path: '{filename}'&quot;)&#10;&#10;        # Strategy 2: If mime_type is empty/null, guess from filename&#10;        if not mime_type or mime_type.lower() == 'null':&#10;            if filename:&#10;                ext = os.path.splitext(filename.lower())[1]&#10;                mime_type_map = {&#10;                    '.pdf': 'application/pdf',&#10;                    '.jpg': 'image/jpeg', '.jpeg': 'image/jpeg',&#10;                    '.png': 'image/png', '.gif': 'image/gif',&#10;                    '.bmp': 'image/bmp', '.tiff': 'image/tiff', '.tif': 'image/tiff',&#10;                    '.webp': 'image/webp'&#10;                }&#10;                mime_type = mime_type_map.get(ext, '')&#10;                logger.info(f&quot;Guessed MIME type from extension '{ext}': '{mime_type}'&quot;)&#10;&#10;        # Strategy 3: If still no filename, try extracting just the filename from the full path&#10;        if not filename:&#10;            # Handle paths like &quot;dmhoa-docs/case_xxx/original/image.jpg&quot;&#10;            path_parts = path.split('/')&#10;            if path_parts:&#10;                filename = path_parts[-1]  # Get the last part&#10;                logger.info(f&quot;Extracted filename from path parts: '{filename}'&quot;)&#10;&#10;        # Strategy 4: If we still have no clear type, try to detect from file content&#10;        detected_type = None&#10;        if not (is_pdf_file(filename, mime_type) or is_image_file(filename, mime_type)):&#10;            # Check file magic bytes as last resort&#10;            if file_bytes and len(file_bytes) &gt;= 4:&#10;                # PDF magic bytes&#10;                if file_bytes.startswith(b'%PDF'):&#10;                    detected_type = 'pdf'&#10;                    logger.info(&quot;Detected PDF from file magic bytes&quot;)&#10;                # JPEG magic bytes&#10;                elif file_bytes.startswith(b'\xff\xd8\xff'):&#10;                    detected_type = 'image'&#10;                    mime_type = 'image/jpeg'&#10;                    logger.info(&quot;Detected JPEG from file magic bytes&quot;)&#10;                # PNG magic bytes&#10;                elif file_bytes.startswith(b'\x89PNG'):&#10;                    detected_type = 'image'&#10;                    mime_type = 'image/png'&#10;                    logger.info(&quot;Detected PNG from file magic bytes&quot;)&#10;&#10;        logger.info(f&quot;Final file detection - filename: '{filename}', mime_type: '{mime_type}', detected_type: {detected_type}&quot;)&#10;&#10;        # Process based on detected file type&#10;        if is_pdf_file(filename, mime_type) or detected_type == 'pdf':&#10;            logger.info(f&quot;Processing as PDF: {filename}&quot;)&#10;            extracted_text, page_count, char_count, extraction_error = extract_pdf_text(file_bytes)&#10;        elif is_image_file(filename, mime_type) or detected_type == 'image':&#10;            logger.info(f&quot;Processing as image: {filename}&quot;)&#10;            extracted_text, page_count, char_count, extraction_error = extract_image_text(file_bytes, filename)&#10;        else:&#10;            error_msg = f&quot;Unsupported file type: {filename} (MIME: {mime_type})&quot;&#10;            logger.error(error_msg)&#10;            update_document(document_id, token, {&#10;                'status': 'failed',&#10;                'error': error_msg[:2000]&#10;            })&#10;            return jsonify({&#10;                'error': error_msg,&#10;                'document_id': document_id&#10;            }), 400&#10;&#10;        # Update document with extraction results&#10;        update_data = {&#10;            'status': 'ready',&#10;            'extracted_text': extracted_text[:50000],  # Limit text size&#10;            'page_count': page_count,&#10;            'char_count': char_count&#10;        }&#10;&#10;        if extraction_error:&#10;            update_data['error'] = extraction_error[:2000]&#10;            logger.warning(f&quot;Document {document_id} processed with warning: {extraction_error}&quot;)&#10;        else:&#10;            logger.info(f&quot;Document {document_id} processed successfully: {char_count} chars, {page_count} pages&quot;)&#10;&#10;        if not update_document(document_id, token, update_data):&#10;            return jsonify({&#10;                'error': 'Failed to save extraction results',&#10;                'document_id': document_id&#10;            }), 500&#10;&#10;        # Trigger preview update now that document is ready&#10;        try:&#10;            trigger_preview_update_after_document_processing(token)&#10;        except Exception as e:&#10;            logger.warning(f&quot;Failed to trigger preview update after document processing: {str(e)}&quot;)&#10;            # Don't fail the response if preview update fails&#10;&#10;        return jsonify({&#10;            'message': 'Document processed successfully',&#10;            'document_id': document_id,&#10;            'status': 'ready',&#10;            'page_count': page_count,&#10;            'char_count': char_count,&#10;            'has_error': bool(extraction_error)&#10;        }), 200&#10;&#10;    except Exception as e:&#10;        error_msg = f&quot;Unexpected error processing document: {str(e)}&quot;&#10;        logger.error(error_msg)&#10;&#10;        # Try to update document status to failed if we have the IDs&#10;        if document_id and token:&#10;            try:&#10;                update_document(document_id, token, {&#10;                    'status': 'failed',&#10;                    'error': error_msg[:2000]&#10;                })&#10;            except:&#10;                pass  # Don't fail the response if we can't update status&#10;&#10;        return jsonify({&#10;            'error': error_msg,&#10;            'document_id': document_id&#10;        }), 500&#10;&#10;&#10;def trigger_preview_update_after_document_processing(token: str):&#10;    &quot;&quot;&quot;Trigger preview regeneration after a document becomes ready.&quot;&quot;&quot;&#10;    try:&#10;        case = read_case_by_token(token)&#10;        if not case:&#10;            logger.warning(f&quot;Cannot trigger preview update - case not found for token {token[:8]}...&quot;)&#10;            return&#10;&#10;        case_id = case.get('id')&#10;        if not case_id:&#10;            logger.warning(f&quot;Cannot trigger preview update - case ID not found for token {token[:8]}...&quot;)&#10;            return&#10;&#10;        # Trigger preview regeneration with force=True since we have new document data&#10;        success = auto_generate_case_preview(token, case_id, force_regenerate=True)&#10;&#10;        if success:&#10;            logger.info(f&quot;Successfully triggered preview update for case {token[:8]}... after document processing&quot;)&#10;        else:&#10;            logger.warning(f&quot;Failed to trigger preview update for case {token[:8]}... after document processing&quot;)&#10;&#10;    except Exception as e:&#10;        logger.error(f&quot;Error triggering preview update after document processing for {token[:8]}...: {str(e)}&quot;)&#10;&#10;&#10;def read_case_by_token(token: str) -&gt; Optional[Dict[str, Any]]:&#10;    &quot;&quot;&quot;Fetch case details from Supabase by token.&quot;&quot;&quot;&#10;    try:&#10;        url = f&quot;{SUPABASE_URL}/rest/v1/dmhoa_cases&quot;&#10;        params = {&#10;            'token': f'eq.{token}',&#10;            'select': '*'&#10;        }&#10;        headers = supabase_headers()&#10;&#10;        response = requests.get(url, params=params, headers=headers, timeout=TIMEOUT)&#10;        response.raise_for_status()&#10;&#10;        cases = response.json()&#10;        if not cases:&#10;            logger.warning(f&quot;No case found for token: {token[:12]}...&quot;)&#10;            return None&#10;&#10;        case = cases[0]&#10;        logger.info(f&quot;Found case for token {token[:12]}...: ID {case.get('id')}&quot;)&#10;        return case&#10;&#10;    except Exception as e:&#10;        logger.error(f&quot;Failed to fetch case for token {token[:12]}...: {str(e)}&quot;)&#10;        return None&#10;&#10;&#10;def read_case_by_id(case_id: str) -&gt; Optional[Dict[str, Any]]:&#10;    &quot;&quot;&quot;Fetch case details from Supabase by case ID.&quot;&quot;&quot;&#10;    try:&#10;        url = f&quot;{SUPABASE_URL}/rest/v1/dmhoa_cases&quot;&#10;        params = {&#10;            'id': f'eq.{case_id}',&#10;            'select': '*'&#10;        }&#10;        headers = supabase_headers()&#10;&#10;        response = requests.get(url, params=params, headers=headers, timeout=TIMEOUT)&#10;        response.raise_for_status()&#10;&#10;        cases = response.json()&#10;        if not cases:&#10;            logger.warning(f&quot;No case found for ID: {case_id}&quot;)&#10;            return None&#10;&#10;        case = cases[0]&#10;        logger.info(f&quot;Found case for ID {case_id}: Token {case.get('token', '')[:8]}...&quot;)&#10;        return case&#10;&#10;    except Exception as e:&#10;        logger.error(f&quot;Failed to fetch case for ID {case_id}: {str(e)}&quot;)&#10;        return None&#10;&#10;&#10;def read_active_preview(case_id: str) -&gt; Optional[Dict]:&#10;    &quot;&quot;&quot;Read the active preview for a case from dmhoa_case_previews table.&quot;&quot;&quot;&#10;    try:&#10;        url = f&quot;{SUPABASE_URL}/rest/v1/dmhoa_case_previews&quot;&#10;        params = {&#10;            'case_id': f'eq.{case_id}',&#10;            'is_active': 'eq.true',&#10;            'select': '*',&#10;            'order': 'created_at.desc',&#10;            'limit': '1'&#10;        }&#10;        headers = supabase_headers()&#10;&#10;        response = requests.get(url, params=params, headers=headers, timeout=TIMEOUT)&#10;        response.raise_for_status()&#10;&#10;        previews = response.json()&#10;        if previews:&#10;            logger.info(f&quot;Found active preview for case {case_id}&quot;)&#10;            return previews[0]&#10;&#10;        logger.info(f&quot;No active preview found for case {case_id}&quot;)&#10;        return None&#10;&#10;    except Exception as e:&#10;        logger.error(f&quot;Failed to read active preview for case {case_id}: {str(e)}&quot;)&#10;        return None&#10;&#10;&#10;def deactivate_previews(case_id: str) -&gt; bool:&#10;    &quot;&quot;&quot;Deactivate all existing previews for a case.&quot;&quot;&quot;&#10;    try:&#10;        url = f&quot;{SUPABASE_URL}/rest/v1/dmhoa_case_previews&quot;&#10;        params = {'case_id': f'eq.{case_id}'}&#10;        headers = supabase_headers()&#10;&#10;        update_data = {'is_active': False}&#10;&#10;        response = requests.patch(url, params=params, headers=headers,&#10;                                json=update_data, timeout=TIMEOUT)&#10;        response.raise_for_status()&#10;&#10;        logger.info(f&quot;Deactivated existing previews for case {case_id}&quot;)&#10;        return True&#10;&#10;    except Exception as e:&#10;        logger.error(f&quot;Failed to deactivate previews for case {case_id}: {str(e)}&quot;)&#10;        return False&#10;&#10;&#10;def insert_preview(case_id: str, preview_content: Dict, preview_snippet: str = None,&#10;                  prompt_version: str = None, model: str = &quot;gpt-4o-mini&quot;,&#10;                  token_input: int = None, token_output: int = None,&#10;                  cost_usd: float = None, latency_ms: int = None) -&gt; Optional[str]:&#10;    &quot;&quot;&quot;Insert a new preview into dmhoa_case_previews table.&quot;&quot;&quot;&#10;    try:&#10;        url = f&quot;{SUPABASE_URL}/rest/v1/dmhoa_case_previews&quot;&#10;        headers = supabase_headers()&#10;        headers['Prefer'] = 'return=representation'&#10;&#10;        preview_data = {&#10;            'case_id': case_id,&#10;            'preview_content': preview_content,&#10;            'preview_snippet': preview_snippet,&#10;            'prompt_version': prompt_version,&#10;            'model': model,&#10;            'token_input': token_input,&#10;            'token_output': token_output,&#10;            'cost_usd': cost_usd,&#10;            'latency_ms': latency_ms,&#10;            'is_active': True&#10;        }&#10;&#10;        # Remove None values&#10;        preview_data = {k: v for k, v in preview_data.items() if v is not None}&#10;&#10;        response = requests.post(url, headers=headers, json=preview_data, timeout=TIMEOUT)&#10;        response.raise_for_status()&#10;&#10;        result = response.json()&#10;        if result:&#10;            preview_id = result[0]['id']&#10;            logger.info(f&quot;Inserted new preview {preview_id} for case {case_id}&quot;)&#10;            return preview_id&#10;&#10;        return None&#10;&#10;    except Exception as e:&#10;        logger.error(f&quot;Failed to insert preview for case {case_id}: {str(e)}&quot;)&#10;        return None&#10;&#10;&#10;def save_case_preview_to_new_table(case_id: str, preview_text: str, doc_brief: Dict,&#10;                                  token_usage: Dict = None, latency_ms: int = None, preview_json: Optional[Dict] = None) -&gt; bool:&#10;    &quot;&quot;&quot;Save generated case preview to the new dmhoa_case_previews table.&quot;&quot;&quot;&#10;    try:&#10;        # Deactivate existing previews first&#10;        deactivate_previews(case_id)&#10;&#10;        # Prepare preview content as JSONB with preview_json&#10;        preview_content = {&#10;            'preview_text': preview_text,&#10;            'doc_summary': doc_brief,&#10;            'generated_at': datetime.utcnow().isoformat(),&#10;            'preview_json': preview_json&#10;        }&#10;&#10;        # Create preview_snippet from preview_json if available, otherwise fallback&#10;        if preview_json:&#10;            headline = preview_json.get('headline', 'HOA Case Analysis')&#10;            deadline = preview_json.get('your_situation', {}).get('deadline', 'Not stated')&#10;            risks = preview_json.get('risk_if_wrong', [])&#10;            first_risk = risks[0] if risks else 'Various consequences'&#10;&#10;            preview_snippet = f&quot;{headline} | Deadline: {deadline} | Risk: {first_risk}&quot;&#10;            # Limit snippet length&#10;            if len(preview_snippet) &gt; 200:&#10;                preview_snippet = preview_snippet[:197] + &quot;...&quot;&#10;        else:&#10;            # Fallback to old method&#10;            preview_snippet = preview_text[:200] + &quot;...&quot; if len(preview_text) &gt; 200 else preview_text&#10;&#10;        # Extract token usage if available&#10;        token_input = token_usage.get('prompt_tokens') if token_usage else None&#10;        token_output = token_usage.get('completion_tokens') if token_usage else None&#10;        cost_usd = token_usage.get('cost_usd') if token_usage else None&#10;&#10;        # Insert new preview with updated prompt version&#10;        preview_id = insert_preview(&#10;            case_id=case_id,&#10;            preview_content=preview_content,&#10;            preview_snippet=preview_snippet,&#10;            prompt_version=&quot;v2.0_sales&quot;,  # Updated to indicate new conversion-optimized format&#10;            model=&quot;gpt-4o-mini&quot;,&#10;            token_input=token_input,&#10;            token_output=token_output,&#10;            cost_usd=cost_usd,&#10;            latency_ms=latency_ms&#10;        )&#10;&#10;        return preview_id is not None&#10;&#10;    except Exception as e:&#10;        logger.error(f&quot;Failed to save case preview to new table for case {case_id}: {str(e)}&quot;)&#10;        return False&#10;&#10;&#10;def render_preview_markdown(preview_json: Dict) -&gt; str:&#10;    &quot;&quot;&quot;Convert preview_json into a clean markdown string for existing UI.&quot;&quot;&quot;&#10;    try:&#10;        markdown_parts = []&#10;&#10;        # Headline&#10;        headline = preview_json.get('headline', 'HOA Case Analysis')&#10;        markdown_parts.append(f&quot;# {headline}\n&quot;)&#10;&#10;        # Why Now section&#10;        why_now = preview_json.get('why_now', '')&#10;        if why_now:&#10;            markdown_parts.append(f&quot;## Urgent Situation\n{why_now}\n&quot;)&#10;&#10;        # Your Situation&#10;        situation = preview_json.get('your_situation', {})&#10;        if situation:&#10;            markdown_parts.append(&quot;## Your Current Situation\n&quot;)&#10;&#10;            alleged_violation = situation.get('alleged_violation')&#10;            if alleged_violation:&#10;                markdown_parts.append(f&quot;**Alleged Violation:** {alleged_violation}\n&quot;)&#10;&#10;            deadline = situation.get('deadline', 'Not stated')&#10;            markdown_parts.append(f&quot;**Deadline:** {deadline}\n&quot;)&#10;&#10;            hoa_demands = situation.get('hoa_demands', [])&#10;            if hoa_demands:&#10;                markdown_parts.append(&quot;**HOA Demands:**&quot;)&#10;                for demand in hoa_demands:&#10;                    markdown_parts.append(f&quot;- {demand}&quot;)&#10;                markdown_parts.append(&quot;&quot;)&#10;&#10;            rules_cited = situation.get('rules_cited', [])&#10;            if rules_cited:&#10;                markdown_parts.append(&quot;**Rules/Regulations Cited:**&quot;)&#10;                for rule in rules_cited:&#10;                    markdown_parts.append(f&quot;- {rule}&quot;)&#10;                markdown_parts.append(&quot;&quot;)&#10;&#10;        # NEW: Critical Detail (Locked) section&#10;        critical_detail = preview_json.get('critical_detail_locked', {})&#10;        if critical_detail:&#10;            title = critical_detail.get('title', 'Critical Detail (Locked)')&#10;            body = critical_detail.get('body', '')&#10;            if body:&#10;                markdown_parts.append(f&quot;## {title}\n{body}\n&quot;)&#10;&#10;        # Risk if wrong&#10;        risks = preview_json.get('risk_if_wrong', [])&#10;        if risks:&#10;            markdown_parts.append(&quot;## Risk If You Handle This Wrong\n&quot;)&#10;            for risk in risks:&#10;                markdown_parts.append(f&quot;- {risk}&quot;)&#10;            markdown_parts.append(&quot;&quot;)&#10;&#10;        # What you get when you unlock&#10;        unlock_items = preview_json.get('what_you_get_when_you_unlock', [])&#10;        if unlock_items:&#10;            markdown_parts.append(&quot;## What You Get When You Unlock Full Response\n&quot;)&#10;            for item in unlock_items:&#10;                markdown_parts.append(f&quot;- {item}&quot;)&#10;            markdown_parts.append(&quot;&quot;)&#10;&#10;        # Hard stop&#10;        hard_stop = preview_json.get('hard_stop', '')&#10;        if hard_stop:&#10;            markdown_parts.append(f&quot;## Next Steps\n{hard_stop}\n&quot;)&#10;&#10;        # CTA&#10;        cta = preview_json.get('cta', {})&#10;        if cta:&#10;            primary = cta.get('primary', 'Unlock full response package')&#10;            secondary = cta.get('secondary', 'See exactly what proof the HOA will accept')&#10;            markdown_parts.append(f&quot;**{primary}**\n*{secondary}*\n&quot;)&#10;&#10;        # Join all parts and limit to ~600 words&#10;        full_markdown = '\n'.join(markdown_parts)&#10;&#10;        # Simple word count check - if too long, truncate at reasonable point&#10;        words = full_markdown.split()&#10;        if len(words) &gt; 600:&#10;            truncated_words = words[:600]&#10;            full_markdown = ' '.join(truncated_words) + &quot;...&quot;&#10;&#10;        return full_markdown&#10;&#10;    except Exception as e:&#10;        logger.error(f&quot;Error rendering preview markdown: {str(e)}&quot;)&#10;        return &quot;Error rendering preview. Please try again.&quot;&#10;&#10;&#10;# NEW: Helper function to clean up rules_cited narrative text&#10;def clean_rules_cited(rules_cited_list):&#10;    &quot;&quot;&quot;Clean up rules_cited to prefer citations over narrative.&quot;&quot;&quot;&#10;    if not rules_cited_list:&#10;        return rules_cited_list&#10;&#10;    cleaned = []&#10;    for rule in rules_cited_list:&#10;        if isinstance(rule, str):&#10;            # Convert narrative &quot;previous notices&quot; to compressed form&#10;            if &quot;previous notices&quot; in rule.lower():&#10;                # Extract year if present, otherwise use generic&#10;                import re&#10;                year_match = re.search(r'\b(20\d{2})\b', rule)&#10;                if year_match:&#10;                    cleaned.append(f&quot;Prior notices ({year_match.group(1)})&quot;)&#10;                else:&#10;                    cleaned.append(&quot;Prior notices (2024)&quot;)&#10;            else:&#10;                cleaned.append(rule)&#10;        else:&#10;            cleaned.append(rule)&#10;&#10;    return cleaned&#10;&#10;def generate_case_preview_with_openai(case_data: Dict, doc_brief: Dict) -&gt; Tuple[str, Dict, int, Optional[Dict]]:&#10;    &quot;&quot;&quot;Generate case preview using OpenAI and return preview, token usage, latency, and preview_json.&quot;&quot;&quot;&#10;    start_time = time.time()&#10;&#10;    try:&#10;        if not OPENAI_API_KEY:&#10;            logger.warning(&quot;OpenAI API key not configured&quot;)&#10;            return &quot;Preview generation unavailable - OpenAI not configured&quot;, {}, 0, None&#10;&#10;        # Extract case information from payload first, then fallback to top level&#10;        payload = case_data.get('payload', {})&#10;        if isinstance(payload, str):&#10;            try:&#10;                payload = json.loads(payload)&#10;            except:&#10;                payload = {}&#10;&#10;        # Try payload first, then case_data directly&#10;        hoa_name = (payload.get('hoaName') or&#10;                   payload.get('hoa_name') or&#10;                   case_data.get('hoa_name') or&#10;                   case_data.get('hoaName') or&#10;                   'Unknown HOA')&#10;&#10;        violation_type = (payload.get('violationType') or&#10;                         payload.get('violation_type') or&#10;                         payload.get('noticeType') or&#10;                         case_data.get('case_description') or&#10;                         case_data.get('violationType') or&#10;                         'Unknown violation')&#10;&#10;        case_description = (payload.get('caseDescription') or&#10;                           payload.get('case_description') or&#10;                           payload.get('description') or&#10;                           case_data.get('case_description') or&#10;                           case_data.get('caseDescription') or&#10;                           'No description provided')&#10;&#10;        property_address = (payload.get('propertyAddress') or&#10;                           payload.get('property_address') or&#10;                           case_data.get('property_address') or&#10;                           case_data.get('propertyAddress') or&#10;                           '')&#10;&#10;        owner_name = (payload.get('ownerName') or&#10;                     payload.get('owner_name') or&#10;                     case_data.get('owner_name') or&#10;                     case_data.get('ownerName') or&#10;                     '')&#10;&#10;        # Get document brief&#10;        doc_text = doc_brief.get('brief_text', '')&#10;        doc_count = doc_brief.get('doc_count', 0)&#10;        doc_status = doc_brief.get('doc_status', 'none')&#10;&#10;        # Prepare the conversion-optimized prompt based on document availability&#10;        if doc_status == &quot;ready&quot; and doc_text:&#10;            # NEW: Updated prompt for documents ready - improved headline, why_now, rules_cited, hard_stop, and critical_detail_locked&#10;            user_prompt = f&quot;&quot;&quot;Create a conversion-optimized preview for this HOA dispute case. You must extract specific facts from the document analysis and output ONLY valid JSON.&#10;&#10;Case Details:&#10;- HOA: {hoa_name}&#10;- Violation Type: {violation_type}&#10;- Property Address: {property_address}&#10;- Owner: {owner_name}&#10;- Case Description: {case_description}&#10;&#10;Document Analysis ({doc_count} documents ready):&#10;{doc_text[:3000] if doc_text else 'No document content available'}&#10;&#10;Output ONLY valid JSON with this exact structure:&#10;{{&#10;  &quot;version&quot;: &quot;preview_v2_sales&quot;,&#10;  &quot;headline&quot;: &quot;string (8-14 words, specific to this case. If a deadline is present, headline must include it as 'X-Day Deadline' or the exact date)&quot;,&#10;  &quot;why_now&quot;: &quot;string (1-2 sentences, tie urgency to the required action: inspection + written report + video, and the deadline)&quot;,&#10;  &quot;your_situation&quot;: {{&#10;    &quot;alleged_violation&quot;: &quot;string (extracted from documents)&quot;,&#10;    &quot;hoa_demands&quot;: [&quot;string&quot;, &quot;string&quot;, &quot;...&quot;],&#10;    &quot;deadline&quot;: &quot;string (use exact date if present; else 'Not stated')&quot;,&#10;    &quot;rules_cited&quot;: [&quot;string (each entry must be either 'Paragraph &lt;...&gt;' / 'Section &lt;...&gt;' / 'Article &lt;...&gt;' if present, otherwise 'Not stated'. Do NOT include vague narrative like 'previous notices' unless no citations exist; if included, compress to 'Prior notices (YYYY)')&quot;, &quot;...&quot;]&#10;  }},&#10;  &quot;critical_detail_locked&quot;: {{&#10;    &quot;title&quot;: &quot;Critical Response Wording (Locked)&quot;,&#10;    &quot;body&quot;: &quot;The exact clause/paragraph language to cite, proof checklist (what the HOA will accept: report + video format), and extension request wording (preserves rights without admitting fault) are locked. Our analysis shows specific language about [evidence acceptance/rule interpretation/compliance deadlines] that could weaken your position if worded incorrectly. This critical phrasing is included in the unlock package.&quot;&#10;  }},&#10;  &quot;risk_if_wrong&quot;: [&#10;    &quot;string (specific consequence)&quot;,&#10;    &quot;string&quot;,&#10;    &quot;string&quot;&#10;  ],&#10;  &quot;what_you_get_when_you_unlock&quot;: [&#10;    &quot;Professional response letter tailored to your specific violation&quot;,&#10;    &quot;Certified mail template with proper legal language&quot;,&#10;    &quot;Evidence checklist to document your defense&quot;,&#10;    &quot;Extension request template if deadline is tight&quot;,&#10;    &quot;Negotiation strategies for your specific situation&quot;&#10;  ],&#10;  &quot;hard_stop&quot;: &quot;string (1-2 lines that create unfinished business - must mention 2-3 concrete items such as: exact paragraph language to quote, proof checklist, extension request template)&quot;,&#10;  &quot;cta&quot;: {{&#10;    &quot;primary&quot;: &quot;Unlock full response package&quot;,&#10;    &quot;secondary&quot;: &quot;See exactly what proof the HOA will accept&quot;&#10;  }}&#10;}}&#10;&#10;RULES:&#10;- Extract specific facts from document analysis for your_situation fields&#10;- Use &quot;you&quot; voice throughout&#10;- Be concrete and specific to this case&#10;- Include at least 5 concrete deliverables in what_you_get_when_you_unlock&#10;- Make hard_stop create genuine unfinished business with concrete locked items&#10;- For critical_detail_locked body, reference exact clause language, proof checklist, extension request wording&#10;- Avoid 'admit liability' language unless phrased as 'without admitting fault' or 'without admitting liability'&quot;&quot;&quot;&#10;        else:&#10;            # NEW: Updated prompt for documents pending - improved hard_stop and critical_detail_locked&#10;            docs_pending_text = &quot;docs are still being processed&quot; if doc_status == &quot;processing&quot; else &quot;no documents uploaded yet&quot;&#10;            user_prompt = f&quot;&quot;&quot;Create a conversion-optimized preview for this HOA dispute case. Documents are pending but you must still create a persuasive preview. Output ONLY valid JSON.&#10;&#10;Case Details:&#10;- HOA: {hoa_name}&#10;- Violation Type: {violation_type}&#10;- Property Address: {property_address}&#10;- Owner: {owner_name}&#10;- Case Description: {case_description}&#10;- Document Status: {docs_pending_text}&#10;&#10;Output ONLY valid JSON with this exact structure:&#10;{{&#10;  &quot;version&quot;: &quot;preview_v2_sales&quot;,&#10;  &quot;headline&quot;: &quot;string (8-14 words, specific to this case type)&quot;,&#10;  &quot;why_now&quot;: &quot;string (1-2 sentences, mention documents pending but emphasize urgency)&quot;,&#10;  &quot;your_situation&quot;: {{&#10;    &quot;alleged_violation&quot;: &quot;{violation_type} (details pending document analysis)&quot;,&#10;    &quot;hoa_demands&quot;: [&quot;Pending document analysis&quot;],&#10;    &quot;deadline&quot;: &quot;Not stated - will be extracted from documents&quot;,&#10;    &quot;rules_cited&quot;: [&quot;Pending document analysis&quot;]&#10;  }},&#10;  &quot;critical_detail_locked&quot;: {{&#10;    &quot;title&quot;: &quot;Critical Response Wording (Locked)&quot;,&#10;    &quot;body&quot;: &quot;Exact rule language, deadline extraction, proof checklist, and extension request template are being extracted from your documents. The precise response phrasing that avoids admitting liability and preserves your rights will be available after processing. This includes the exact language needed for compliance responses and extension requests.&quot;&#10;  }},&#10;  &quot;risk_if_wrong&quot;: [&#10;    &quot;Missing critical response deadlines&quot;,&#10;    &quot;Accepting invalid HOA demands without challenge&quot;,&#10;    &quot;Paying unnecessary fines or agreeing to unreasonable compliance&quot;&#10;  ],&#10;  &quot;what_you_get_when_you_unlock&quot;: [&#10;    &quot;Professional response letter once documents are analyzed&quot;,&#10;    &quot;Certified mail template with proper legal language&quot;,&#10;    &quot;Evidence checklist for your specific violation type&quot;,&#10;    &quot;Deadline tracking and extension strategies&quot;,&#10;    &quot;Complete document analysis and defense strategy&quot;&#10;  ],&#10;  &quot;hard_stop&quot;: &quot;Your documents are being analyzed to identify exact rule language, deadline extraction, proof checklist, and extension request template. Once complete, you'll get these specific locked items needed for your response.&quot;,&#10;  &quot;cta&quot;: {{&#10;    &quot;primary&quot;: &quot;Unlock full response package&quot;,&#10;    &quot;secondary&quot;: &quot;Get complete analysis once documents are ready&quot;&#10;  }}&#10;}}&#10;&#10;RULES:&#10;- Use &quot;you&quot; voice throughout&#10;- Be specific about what the unlock will provide once docs are ready&#10;- Make it clear docs are pending but tool is still valuable&#10;- Create urgency around not missing opportunities&#10;- For critical_detail_locked, emphasize that exact response phrasing comes after document analysis&quot;&quot;&quot;&#10;&#10;        headers = {&#10;            'Authorization': f'Bearer {OPENAI_API_KEY}',&#10;            'Content-Type': 'application/json'&#10;        }&#10;&#10;        data = {&#10;            &quot;model&quot;: &quot;gpt-4o-mini&quot;,&#10;            &quot;messages&quot;: [&#10;                {&#10;                    &quot;role&quot;: &quot;system&quot;,&#10;                    &quot;content&quot;: &quot;You write conversion-optimized previews for HOA dispute tool. Output ONLY valid JSON. No explanation, no markdown, just the JSON object.&quot;&#10;                },&#10;                {&#10;                    &quot;role&quot;: &quot;user&quot;,&#10;                    &quot;content&quot;: user_prompt&#10;                }&#10;            ],&#10;            &quot;temperature&quot;: 0.3,&#10;            &quot;max_tokens&quot;: 1200&#10;        }&#10;&#10;        response = requests.post(&#10;            'https://api.openai.com/v1/chat/completions',&#10;            headers=headers,&#10;            json=data,&#10;            timeout=30&#10;        )&#10;        response.raise_for_status()&#10;&#10;        result = response.json()&#10;        json_response = result['choices'][0]['message']['content'].strip()&#10;&#10;        # Calculate latency&#10;        latency_ms = int((time.time() - start_time) * 1000)&#10;&#10;        # Extract token usage&#10;        token_usage = {}&#10;        if 'usage' in result:&#10;            usage = result['usage']&#10;            token_usage = {&#10;                'prompt_tokens': usage.get('prompt_tokens', 0),&#10;                'completion_tokens': usage.get('completion_tokens', 0),&#10;                'total_tokens': usage.get('total_tokens', 0)&#10;            }&#10;&#10;            # Estimate cost (approximate rates for gpt-4o-mini)&#10;            input_cost = (token_usage['prompt_tokens'] / 1000) * 0.00015  # $0.15 per 1K input tokens&#10;            output_cost = (token_usage['completion_tokens'] / 1000) * 0.0006  # $0.60 per 1K output tokens&#10;            token_usage['cost_usd'] = round(input_cost + output_cost, 6)&#10;&#10;        # Parse JSON safely and create markdown&#10;        preview_json = None&#10;        try:&#10;            # Clean the response in case there's extra text&#10;            json_start = json_response.find('{')&#10;            json_end = json_response.rfind('}') + 1&#10;            if json_start &gt;= 0 and json_end &gt; json_start:&#10;                clean_json = json_response[json_start:json_end]&#10;                preview_json = json.loads(clean_json)&#10;&#10;                # NEW: Post-processing to clean up rules_cited narrative&#10;                if preview_json and 'your_situation' in preview_json and 'rules_cited' in preview_json['your_situation']:&#10;                    preview_json['your_situation']['rules_cited'] = clean_rules_cited(preview_json['your_situation']['rules_cited'])&#10;&#10;                # Create markdown from JSON&#10;                markdown_preview = render_preview_markdown(preview_json)&#10;&#10;                logger.info(f&quot;Generated case preview: {len(markdown_preview)} characters, {latency_ms}ms&quot;)&#10;                return markdown_preview, token_usage, latency_ms, preview_json&#10;&#10;            else:&#10;                raise json.JSONDecodeError(&quot;No valid JSON found&quot;, json_response, 0)&#10;&#10;        except json.JSONDecodeError as e:&#10;            logger.warning(f&quot;Failed to parse JSON response: {str(e)}&quot;)&#10;            logger.warning(f&quot;Raw response: {json_response}&quot;)&#10;&#10;            # Fallback to original response as markdown but no preview_json&#10;            markdown_preview = f&quot;# HOA Case Analysis\n\n{json_response}&quot;&#10;            logger.info(f&quot;Generated fallback case preview: {len(markdown_preview)} characters, {latency_ms}ms&quot;)&#10;            return markdown_preview, token_usage, latency_ms, None&#10;&#10;    except Exception as e:&#10;        latency_ms = int((time.time() - start_time) * 1000)&#10;        logger.error(f&quot;Failed to generate case preview: {str(e)}&quot;)&#10;        return f&quot;Error generating preview: {str(e)}&quot;, {}, latency_ms, None&#10;&#10;&#10;def generate_preview_without_documents(case_data: Dict) -&gt; Tuple[str, Dict, int, Optional[Dict]]:&#10;    &quot;&quot;&quot;Generate a conversion-optimized case preview when no documents are available yet.&quot;&quot;&quot;&#10;    start_time = time.time()&#10;&#10;    try:&#10;        if not OPENAI_API_KEY:&#10;            logger.warning(&quot;OpenAI API key not configured&quot;)&#10;            return &quot;Preview generation unavailable - OpenAI not configured&quot;, {}, 0, None&#10;&#10;        # Extract case information from payload&#10;        payload = case_data.get('payload', {})&#10;        if isinstance(payload, str):&#10;            try:&#10;                payload = json.loads(payload)&#10;            except:&#10;                payload = {}&#10;&#10;        hoa_name = payload.get('hoaName', payload.get('hoa_name', 'Unknown HOA'))&#10;        violation_type = payload.get('violationType', payload.get('noticeType', 'Unknown violation'))&#10;        case_description = payload.get('caseDescription', payload.get('case_description', 'No description provided'))&#10;        property_address = payload.get('propertyAddress', payload.get('property_address', ''))&#10;        owner_name = payload.get('ownerName', payload.get('owner_name', ''))&#10;&#10;        # NEW: Updated prompt for no documents case - improved hard_stop and critical_detail_locked&#10;        user_prompt = f&quot;&quot;&quot;Create a conversion-optimized preview for this HOA dispute case. No documents have been uploaded yet, but you must still create a persuasive preview. Output ONLY valid JSON.&#10;&#10;Case Details:&#10;- HOA: {hoa_name}&#10;- Violation Type: {violation_type}&#10;- Property Address: {property_address}&#10;- Owner: {owner_name}&#10;- Case Description: {case_description}&#10;- Document Status: No documents uploaded yet&#10;&#10;Output ONLY valid JSON with this exact structure:&#10;{{&#10;  &quot;version&quot;: &quot;preview_v2_sales&quot;,&#10;  &quot;headline&quot;: &quot;string (8-14 words, specific to this {violation_type} case)&quot;,&#10;  &quot;why_now&quot;: &quot;string (1-2 sentences, create urgency about acting before it's too late)&quot;,&#10;  &quot;your_situation&quot;: {{&#10;    &quot;alleged_violation&quot;: &quot;{violation_type} by {hoa_name}&quot;,&#10;    &quot;hoa_demands&quot;: [&quot;Upload documents to see specific demands&quot;],&#10;    &quot;deadline&quot;: &quot;Unknown - upload documents to identify deadlines&quot;,&#10;    &quot;rules_cited&quot;: [&quot;Upload documents to see specific rules cited&quot;]&#10;  }},&#10;  &quot;critical_detail_locked&quot;: {{&#10;    &quot;title&quot;: &quot;Critical Response Wording (Locked)&quot;,&#10;    &quot;body&quot;: &quot;Exact rule language, deadline extraction, proof checklist, and extension request template will be extracted from your documents. The precise response phrasing that avoids admitting liability and preserves your rights will be available after processing. This includes the exact language needed for compliance responses and extension requests.&quot;&#10;  }},&#10;  &quot;risk_if_wrong&quot;: [&#10;    &quot;Missing critical response deadlines that could escalate penalties&quot;,&#10;    &quot;Accepting invalid HOA demands without proper challenge&quot;,&#10;    &quot;Paying unnecessary fines or agreeing to unreasonable compliance&quot;&#10;  ],&#10;  &quot;what_you_get_when_you_unlock&quot;: [&#10;    &quot;Professional response letter template for {violation_type} cases&quot;,&#10;    &quot;Certified mail template with proper legal language&quot;,&#10;    &quot;Evidence checklist for {violation_type} violations&quot;,&#10;    &quot;Extension request strategies if deadlines are tight&quot;,&#10;    &quot;Complete analysis once you upload your HOA documents&quot;&#10;  ],&#10;  &quot;hard_stop&quot;: &quot;Upload your HOA notice and we'll analyze exact rule language, deadline extraction, proof checklist, and extension request template. You'll get these specific locked items needed for your response.&quot;,&#10;  &quot;cta&quot;: {{&#10;    &quot;primary&quot;: &quot;Unlock full response package&quot;,&#10;    &quot;secondary&quot;: &quot;Upload documents for complete analysis&quot;&#10;  }}&#10;}}&#10;&#10;RULES:&#10;- Use &quot;you&quot; voice throughout&#10;- Be specific about {violation_type} violations&#10;- Create urgency around not delaying action&#10;- Make it clear uploading documents unlocks much more value&#10;- For critical_detail_locked, emphasize that exact response phrasing comes after document analysis&quot;&quot;&quot;&#10;&#10;        headers = {&#10;            'Authorization': f'Bearer {OPENAI_API_KEY}',&#10;            'Content-Type': 'application/json'&#10;        }&#10;&#10;        data = {&#10;            &quot;model&quot;: &quot;gpt-4o-mini&quot;,&#10;            &quot;messages&quot;: [&#10;                {&#10;                    &quot;role&quot;: &quot;system&quot;,&#10;                    &quot;content&quot;: &quot;You write conversion-optimized previews for HOA dispute tool. Output ONLY valid JSON. No explanation, no markdown, just the JSON object.&quot;&#10;                },&#10;                {&#10;                    &quot;role&quot;: &quot;user&quot;,&#10;                    &quot;content&quot;: user_prompt&#10;                }&#10;            ],&#10;            &quot;temperature&quot;: 0.3,&#10;            &quot;max_tokens&quot;: 1000&#10;        }&#10;&#10;        response = requests.post(&#10;            'https://api.openai.com/v1/chat/completions',&#10;            headers=headers,&#10;            json=data,&#10;            timeout=30&#10;        )&#10;        response.raise_for_status()&#10;&#10;        result = response.json()&#10;        json_response = result['choices'][0]['message']['content'].strip()&#10;&#10;        # Calculate latency&#10;        latency_ms = int((time.time() - start_time) * 1000)&#10;&#10;        # Extract token usage&#10;        token_usage = {}&#10;        if 'usage' in result:&#10;            usage = result['usage']&#10;            token_usage = {&#10;                'prompt_tokens': usage.get('prompt_tokens', 0),&#10;                'completion_tokens': usage.get('completion_tokens', 0),&#10;                'total_tokens': usage.get('total_tokens', 0)&#10;            }&#10;&#10;            # Estimate cost (approximate rates for gpt-4o-mini)&#10;            input_cost = (token_usage['prompt_tokens'] / 1000) * 0.00015  # $0.15 per 1K input tokens&#10;            output_cost = (token_usage['completion_tokens'] / 1000) * 0.0006  # $0.60 per 1K output tokens&#10;            token_usage['cost_usd'] = round(input_cost + output_cost, 6)&#10;&#10;        # Parse JSON safely and create markdown&#10;        preview_json = None&#10;        try:&#10;            # Clean the response in case there's extra text&#10;            json_start = json_response.find('{')&#10;            json_end = json_response.rfind('}') + 1&#10;            if json_start &gt;= 0 and json_end &gt; json_start:&#10;                clean_json = json_response[json_start:json_end]&#10;                preview_json = json.loads(clean_json)&#10;&#10;                # NEW: Post-processing to clean up rules_cited narrative&#10;                if preview_json and 'your_situation' in preview_json and 'rules_cited' in preview_json['your_situation']:&#10;                    preview_json['your_situation']['rules_cited'] = clean_rules_cited(preview_json['your_situation']['rules_cited'])&#10;&#10;                # Create markdown from JSON&#10;                markdown_preview = render_preview_markdown(preview_json)&#10;&#10;                logger.info(f&quot;Generated preliminary case preview: {len(markdown_preview)} characters, {latency_ms}ms&quot;)&#10;                return markdown_preview, token_usage, latency_ms, preview_json&#10;&#10;            else:&#10;                raise json.JSONDecodeError(&quot;No valid JSON found&quot;, json_response, 0)&#10;&#10;        except json.JSONDecodeError as e:&#10;            logger.warning(f&quot;Failed to parse JSON response: {str(e)}&quot;)&#10;            logger.warning(f&quot;Raw response: {json_response}&quot;)&#10;&#10;            # Fallback to original response as markdown but no preview_json&#10;            markdown_preview = f&quot;# HOA Case Analysis\n\n{json_response}&quot;&#10;            logger.info(f&quot;Generated fallback preliminary case preview: {len(markdown_preview)} characters, {latency_ms}ms&quot;)&#10;            return markdown_preview, token_usage, latency_ms, None&#10;&#10;    except Exception as e:&#10;        latency_ms = int((time.time() - start_time) * 1000)&#10;        logger.error(f&quot;Failed to generate case preview: {str(e)}&quot;)&#10;        return f&quot;Error generating preview: {str(e)}&quot;, {}, latency_ms, None&#10;&#10;&#10;def auto_generate_case_preview(token: str, case_id: str, force_regenerate: bool = False) -&gt; bool:&#10;    &quot;&quot;&quot;Automatically generate case preview - immediate or deferred based on document status.&quot;&quot;&quot;&#10;    try:&#10;        # NEW: Use improved concurrency guard to prevent duplicate active previews&#10;        if not force_regenerate and not upsert_active_preview_lock(case_id):&#10;            return True  # Skip generation, already handled or in progress&#10;&#10;        try:&#10;            # Fetch case data first&#10;            case = read_case_by_token(token)&#10;            if not case:&#10;                logger.error(f&quot;Case not found for token {token[:12]}...&quot;)&#10;                return False&#10;&#10;            # Check document status&#10;            documents = fetch_ready_documents_by_token(token, limit=5)&#10;            all_documents = fetch_any_documents_status_by_token(token)&#10;&#10;            has_processing_documents = any(doc.get('status') in ['pending', 'processing'] for doc in all_documents)&#10;&#10;            # Check existing preview&#10;            existing_preview = read_active_preview(case_id)&#10;&#10;            # Determine if we need to generate/upgrade preview&#10;            should_generate = False&#10;            preview_type = &quot;preliminary&quot;&#10;&#10;            if not existing_preview:&#10;                # No preview exists - always generate&#10;                should_generate = True&#10;                logger.info(f&quot;No preview exists for case {token[:12]}... - generating new preview&quot;)&#10;            elif force_regenerate:&#10;                # Forced regeneration&#10;                should_generate = True&#10;                logger.info(f&quot;Force regenerating preview for case {token[:12]}...&quot;)&#10;            else:&#10;                # Check if we can upgrade from preliminary to full&#10;                existing_content = existing_preview.get('preview_content', {})&#10;                existing_doc_summary = existing_content.get('doc_summary', {})&#10;                existing_doc_status = existing_doc_summary.get('doc_status', 'none')&#10;&#10;                if documents and existing_doc_status in ['none', 'processing']:&#10;                    # We have ready documents but existing preview doesn't - upgrade to full&#10;                    should_generate = True&#10;                    preview_type = &quot;upgrade&quot;&#10;                    logger.info(f&quot;Upgrading preview from {existing_doc_status} to full for case {token[:12]}...&quot;)&#10;                else:&#10;                    logger.info(f&quot;Preview already exists and up-to-date for case {token[:12]}... - skipping generation&quot;)&#10;                    return True&#10;&#10;            if not should_generate:&#10;                return True&#10;&#10;            # Generate preview based on available data&#10;            preview_json = None&#10;            if documents:&#10;                # Documents are ready - generate full preview with document analysis&#10;                logger.info(f&quot;Generating full preview with {len(documents)} ready documents for case {token[:12]}...&quot;)&#10;                doc_brief = build_doc_brief(documents)&#10;                preview_text, token_usage, latency_ms, preview_json = generate_case_preview_with_openai(case, doc_brief)&#10;                preview_type = &quot;full&quot;&#10;&#10;            elif has_processing_documents:&#10;                # Documents are still processing - generate preliminary preview&#10;                logger.info(f&quot;Documents still processing for case {token[:12]}... - generating preliminary preview&quot;)&#10;                doc_brief = {&#10;                    &quot;doc_status&quot;: &quot;processing&quot;,&#10;                    &quot;doc_count&quot;: len(all_documents),&#10;                    &quot;sources&quot;: [],&#10;                    &quot;brief_text&quot;: &quot;Documents are currently being processed and analyzed.&quot;&#10;                }&#10;                preview_text, token_usage, latency_ms, preview_json = generate_preview_without_documents(case)&#10;                preview_type = &quot;preliminary&quot;&#10;&#10;            else:&#10;                # No documents at all - generate basic preview&#10;                logger.info(f&quot;No documents found for case {token[:12]}... - generating basic preview&quot;)&#10;                doc_brief = {&#10;                    &quot;doc_status&quot;: &quot;none&quot;,&#10;                    &quot;doc_count&quot;: 0,&#10;                    &quot;sources&quot;: [],&#10;                    &quot;brief_text&quot;: &quot;No documents have been uploaded for analysis.&quot;&#10;                }&#10;                preview_text, token_usage, latency_ms, preview_json = generate_preview_without_documents(case)&#10;                preview_type = &quot;basic&quot;&#10;&#10;            # Save preview (this will deactivate existing ones automatically)&#10;            success = save_case_preview_to_new_table(case_id, preview_text, doc_brief, token_usage, latency_ms, preview_json)&#10;&#10;            if success:&#10;                logger.info(f&quot;Successfully generated {preview_type} preview for case {token[:12]}...&quot;)&#10;            else:&#10;                logger.error(f&quot;Failed to save {preview_type} preview for case {token[:12]}...&quot;)&#10;&#10;            return success&#10;&#10;        finally:&#10;            # NEW: Always release the lock when done&#10;            release_preview_lock(case_id)&#10;&#10;    except Exception as e:&#10;        # NEW: Release lock on exception&#10;        release_preview_lock(case_id)&#10;        logger.error(f&quot;Error auto-generating preview for case {token[:12]}...: {str(e)}&quot;)&#10;        return False&#10;&#10;&#10;&#10;@app.route('/webhooks/generate-preview', methods=['POST'])&#10;def generate_preview_webhook():&#10;    &quot;&quot;&quot;Webhook endpoint to generate case preview - can be called after case creation.&quot;&quot;&quot;&#10;    # Validate webhook secret&#10;    webhook_secret = request.headers.get('X-Webhook-Secret')&#10;    if not webhook_secret or webhook_secret != DOC_EXTRACT_WEBHOOK_SECRET:&#10;        logger.warning(&quot;Invalid or missing webhook secret for generate-preview&quot;)&#10;        return jsonify({'error': 'Unauthorized'}), 401&#10;&#10;    try:&#10;        # Parse JSON body&#10;        data = request.get_json()&#10;        if not data:&#10;            return jsonify({'error': 'Invalid JSON body'}), 400&#10;&#10;        # Validate required fields&#10;        token = data.get('token')&#10;        case_id = data.get('case_id')&#10;&#10;        if not token:&#10;            return jsonify({'error': 'Missing required field: token'}), 400&#10;&#10;        logger.info(f&quot;Generating preview for case - Token: {token[:8]}..., Case ID: {case_id}&quot;)&#10;&#10;        # If case_id not provided, look it up by token&#10;        if not case_id:&#10;            case = read_case_by_token(token)&#10;            if not case:&#10;                return jsonify({'error': 'Case not found for token'}), 404&#10;            case_id = case.get('id')&#10;            if not case_id:&#10;                return jsonify({'error': 'Case ID not found'}), 404&#10;&#10;        # Generate preview&#10;        success = auto_generate_case_preview(token, case_id)&#10;&#10;        if success:&#10;            return jsonify({&#10;                'message': 'Preview generated successfully',&#10;                'token': token,&#10;                'case_id': case_id&#10;            }), 200&#10;        else:&#10;            return jsonify({&#10;                'error': 'Failed to generate preview',&#10;                'token': token,&#10;                'case_id': case_id&#10;            }), 500&#10;&#10;    except Exception as e:&#10;        error_msg = f&quot;Unexpected error generating preview: {str(e)}&quot;&#10;        logger.error(error_msg)&#10;        return jsonify({'error': error_msg}), 500&#10;&#10;&#10;def schedule_delayed_preview_generation(token: str, case_id: str, delay_seconds: int = 30):&#10;    &quot;&quot;&quot;Schedule preview generation with a delay to allow documents to be uploaded and processed.&quot;&quot;&quot;&#10;    def delayed_preview():&#10;        time.sleep(delay_seconds)&#10;        try:&#10;            logger.info(f&quot;Executing delayed preview generation for case {token[:8]}...&quot;)&#10;            success = auto_generate_case_preview(token, case_id)&#10;            if success:&#10;                logger.info(f&quot;Delayed preview generation successful for case {token[:8]}...&quot;)&#10;            else:&#10;                logger.warning(f&quot;Delayed preview generation failed for case {token[:8]}...&quot;)&#10;        except Exception as e:&#10;            logger.error(f&quot;Error in delayed preview generation for case {token[:8]}...: {str(e)}&quot;)&#10;&#10;    # Start the delayed task in a background thread&#10;    thread = threading.Thread(target=delayed_preview, daemon=True)&#10;    thread.start()&#10;    logger.info(f&quot;Scheduled delayed preview generation for case {token[:8]}... in {delay_seconds} seconds&quot;)&#10;&#10;&#10;@app.route('/webhooks/case-created', methods=['POST'])&#10;def case_created_webhook():&#10;    &quot;&quot;&quot;Webhook to handle case creation and trigger initial preview generation.&quot;&quot;&quot;&#10;    # Validate webhook secret&#10;    webhook_secret = request.headers.get('X-Webhook-Secret')&#10;    if not webhook_secret or webhook_secret != DOC_EXTRACT_WEBHOOK_SECRET:&#10;        logger.warning(&quot;Invalid or missing webhook secret for case-created&quot;)&#10;        return jsonify({'error': 'Unauthorized'}), 401&#10;&#10;    try:&#10;        # Parse JSON body&#10;        data = request.get_json()&#10;        if not data:&#10;            return jsonify({'error': 'Invalid JSON body'}), 400&#10;&#10;        token = data.get('token')&#10;        case_id = data.get('case_id')&#10;&#10;        if not token:&#10;            return jsonify({'error': 'Missing required field: token'}), 400&#10;&#10;        logger.info(f&quot;Case created - Token: {token[:8]}..., Case ID: {case_id}&quot;)&#10;&#10;        # If case_id not provided, look it up by token&#10;        if not case_id:&#10;            case = read_case_by_token(token)&#10;            if not case:&#10;                return jsonify({'error': 'Case not found for token'}), 404&#10;            case_id = case.get('id')&#10;&#10;        # Generate immediate preview&#10;        immediate_success = auto_generate_case_preview(token, case_id)&#10;&#10;        # Check if there are any documents that might need processing&#10;        all_documents = fetch_any_documents_status_by_token(token)&#10;        pending_or_processing_docs = [doc for doc in all_documents if doc.get('status') in ['pending', 'processing']]&#10;&#10;        # Only schedule delayed jobs if there are documents that might become ready&#10;        if pending_or_processing_docs:&#10;            logger.info(f&quot;Found {len(pending_or_processing_docs)} pending/processing documents - scheduling delayed preview generations&quot;)&#10;            # Delayed: Give time for documents to be uploaded and processed, then regenerate&#10;            schedule_delayed_preview_generation(token, case_id, delay_seconds=60)&#10;            # Also schedule a longer delay for cases where document processing might take longer&#10;            schedule_delayed_preview_generation(token, case_id, delay_seconds=300)  # 5 minutes&#10;        else:&#10;            logger.info(f&quot;No pending/processing documents found - skipping delayed preview generations&quot;)&#10;&#10;        return jsonify({&#10;            'message': 'Case creation handled and preview generation scheduled',&#10;            'token': token,&#10;            'case_id': case_id,&#10;            'immediate_preview': immediate_success,&#10;            'delayed_jobs_scheduled': len(pending_or_processing_docs) &gt; 0&#10;        }), 200&#10;&#10;    except Exception as e:&#10;        error_msg = f&quot;Unexpected error handling case creation: {str(e)}&quot;&#10;        logger.error(error_msg)&#10;        return jsonify({'error': error_msg}), 500&#10;&#10;&#10;def create_case_in_supabase(case_data: Dict) -&gt; Tuple[bool, Optional[str], Optional[str]]:&#10;    &quot;&quot;&quot;Create a new case in Supabase database or update existing one.&quot;&quot;&quot;&#10;    try:&#10;        token = case_data.get('token')&#10;        if not token:&#10;            logger.error(&quot;No token provided for case creation&quot;)&#10;            return False, None, None&#10;&#10;        # First, check if case already exists&#10;        check_url = f&quot;{SUPABASE_URL}/rest/v1/dmhoa_cases&quot;&#10;        check_params = {'token': f'eq.{token}', 'select': 'id,token,status'}&#10;        check_headers = supabase_headers()&#10;&#10;        check_response = requests.get(check_url, params=check_params, headers=check_headers, timeout=TIMEOUT)&#10;        check_response.raise_for_status()&#10;        existing_cases = check_response.json()&#10;&#10;        # Get email from the case data (check multiple possible field names)&#10;        email = (case_data.get('email') or&#10;                case_data.get('ownerEmail') or&#10;                case_data.get('owner_email') or&#10;                case_data.get('userEmail') or&#10;                case_data.get('user_email'))&#10;&#10;        # Prepare case data according to the actual table structure&#10;        case_payload = {&#10;            'token': token,&#10;            'email': email,&#10;            'payload': case_data,  # Store the entire case data as JSONB in payload column&#10;            'status': 'preview'  # Set default status&#10;        }&#10;&#10;        # Remove None/empty values except for payload which should always be included&#10;        case_payload = {k: v for k, v in case_payload.items() if v is not None and (k == 'payload' or v)}&#10;&#10;        if existing_cases and len(existing_cases) &gt; 0:&#10;            # Case exists, update it&#10;            existing_case = existing_cases[0]&#10;            case_id = existing_case.get('id')&#10;&#10;            logger.info(f&quot;Case with token {token[:8]}... already exists, updating with ID: {case_id}&quot;)&#10;&#10;            # Update existing case&#10;            update_url = f&quot;{SUPABASE_URL}/rest/v1/dmhoa_cases&quot;&#10;            update_params = {'id': f'eq.{case_id}'}&#10;            update_headers = supabase_headers()&#10;            update_headers['Prefer'] = 'return=representation'&#10;&#10;            # Only update payload and email, preserve other fields&#10;            update_payload = {&#10;                'payload': case_data,&#10;                'email': email&#10;            }&#10;            update_payload = {k: v for k, v in update_payload.items() if v is not None}&#10;&#10;            update_response = requests.patch(update_url, params=update_params, headers=update_headers, json=update_payload, timeout=TIMEOUT)&#10;            update_response.raise_for_status()&#10;&#10;            result = update_response.json()&#10;            if result and len(result) &gt; 0:&#10;                logger.info(f&quot;Updated existing case successfully - ID: {case_id}, Token: {token[:8]}...&quot;)&#10;                return True, case_id, token&#10;            else:&#10;                logger.warning(f&quot;Update succeeded but no data returned for case {case_id}&quot;)&#10;                return True, case_id, token&#10;&#10;        else:&#10;            # Case doesn't exist, create new one&#10;            logger.info(f&quot;Creating new case with token {token[:8]}...&quot;)&#10;&#10;            url = f&quot;{SUPABASE_URL}/rest/v1/dmhoa_cases&quot;&#10;            headers = supabase_headers()&#10;            headers['Prefer'] = 'return=representation'&#10;&#10;            logger.info(f&quot;Creating case with payload keys: {list(case_payload.keys())}&quot;)&#10;&#10;            response = requests.post(url, headers=headers, json=case_payload, timeout=TIMEOUT)&#10;            response.raise_for_status()&#10;&#10;            result = response.json()&#10;            if result and len(result) &gt; 0:&#10;                case_id = result[0].get('id')&#10;                token_returned = result[0].get('token')&#10;                logger.info(f&quot;Created case successfully - ID: {case_id}, Token: {token_returned[:8]}...&quot;)&#10;                return True, case_id, token_returned&#10;            else:&#10;                logger.error(&quot;No case data returned from Supabase&quot;)&#10;                return False, None, None&#10;&#10;    except Exception as e:&#10;        logger.error(f&quot;Failed to create/update case in Supabase: {str(e)}&quot;)&#10;        return False, None, None&#10;&#10;&#10;@app.route('/api/save-case', methods=['POST', 'OPTIONS'])&#10;def save_case():&#10;    &quot;&quot;&quot;Save a new case and trigger preview generation.&quot;&quot;&quot;&#10;    # Handle CORS preflight&#10;    if request.method == 'OPTIONS':&#10;        response = jsonify({'message': 'OK'})&#10;        return response&#10;&#10;    try:&#10;        # Parse JSON body&#10;        case_data = request.get_json()&#10;        if not case_data:&#10;            return jsonify({'error': 'Invalid JSON body'}), 400&#10;&#10;        # Log the incoming data for debugging&#10;        logger.info(f&quot;Received case data: {json.dumps(case_data, indent=2)}&quot;)&#10;&#10;        # Handle nested payload structure - check both top level and inside payload&#10;        payload_data = case_data.get('payload', {})&#10;&#10;        # Combine top-level fields and payload fields for field extraction&#10;        combined_data = {**case_data, **payload_data}&#10;&#10;        logger.info(f&quot;Combined data fields: {list(combined_data.keys())}&quot;)&#10;&#10;        # Handle multiple possible field name variations from combined data&#10;        hoa_name = (combined_data.get('hoaName') or&#10;                   combined_data.get('hoa_name') or&#10;                   combined_data.get('HOAName') or&#10;                   combined_data.get('organizationName') or&#10;                   combined_data.get('organization_name'))&#10;&#10;        violation_type = (combined_data.get('violationType') or&#10;                         combined_data.get('violation_type') or&#10;                         combined_data.get('ViolationType') or&#10;                         combined_data.get('noticeType') or&#10;                         combined_data.get('notice_type') or&#10;                         combined_data.get('violationCategory'))&#10;&#10;        token = combined_data.get('token')&#10;&#10;        # Log extracted values&#10;        logger.info(f&quot;Extracted values - Token: {token}, HOA: {hoa_name}, Violation: {violation_type}&quot;)&#10;&#10;        # If we don't have HOA name or violation type, try to infer from other fields&#10;        if not hoa_name:&#10;            # Try to get organization info from other fields&#10;            hoa_name = (combined_data.get('organization') or&#10;                       combined_data.get('company') or&#10;                       combined_data.get('entity') or&#10;                       &quot;Unknown HOA&quot;)&#10;            logger.info(f&quot;Inferred HOA name: {hoa_name}&quot;)&#10;&#10;        if not violation_type:&#10;            # Use noticeType or infer from context&#10;            violation_type = (combined_data.get('noticeType') or&#10;                             combined_data.get('issueType') or&#10;                             combined_data.get('category') or&#10;                             &quot;violation&quot;)  # Default fallback&#10;            logger.info(f&quot;Using violation type: {violation_type}&quot;)&#10;&#10;        # Validate required fields with more flexible requirements&#10;        missing_fields = []&#10;        if not token:&#10;            missing_fields.append('token')&#10;&#10;        if missing_fields:&#10;            error_msg = f'Missing required fields: {&quot;, &quot;.join(missing_fields)}'&#10;            logger.error(f&quot;Validation failed: {error_msg}&quot;)&#10;            logger.error(f&quot;Available fields in combined data: {list(combined_data.keys())}&quot;)&#10;            return jsonify({'error': error_msg}), 400&#10;&#10;        # Create the final case data structure&#10;        if 'payload' in case_data and isinstance(case_data['payload'], dict):&#10;            # If payload exists, use it as the base and add normalized fields&#10;            final_case_data = case_data['payload'].copy()&#10;            final_case_data['token'] = token&#10;            final_case_data['hoaName'] = hoa_name&#10;            final_case_data['violationType'] = violation_type&#10;        else:&#10;            # Otherwise use the combined data&#10;            final_case_data = combined_data.copy()&#10;            final_case_data['token'] = token&#10;            final_case_data['hoaName'] = hoa_name&#10;            final_case_data['violationType'] = violation_type&#10;&#10;        logger.info(f&quot;Saving case - Token: {token[:8]}..., HOA: {hoa_name}, Violation: {violation_type}&quot;)&#10;        logger.info(f&quot;Final case data keys: {list(final_case_data.keys())}&quot;)&#10;&#10;        # Create case in database&#10;        success, case_id, returned_token = create_case_in_supabase(final_case_data)&#10;&#10;        if not success:&#10;            return jsonify({&#10;                'error': 'Failed to create case in database'&#10;            }), 500&#10;&#10;        # NEW: Direct preview generation trigger instead of HTTP call to localhost&#10;        try:&#10;            if case_id:&#10;                logger.info(f&quot;Triggering direct preview generation for case {token[:8]}...&quot;)&#10;&#10;                # Call auto_generate_case_preview directly instead of making HTTP request&#10;                immediate_success = auto_generate_case_preview(token, case_id, force_regenerate=False)&#10;&#10;                # Check if there are any documents that might need processing&#10;                all_documents = fetch_any_documents_status_by_token(token)&#10;                pending_or_processing_docs = [doc for doc in all_documents if doc.get('status') in ['pending', 'processing']]&#10;&#10;                # Only schedule delayed jobs if there are documents that might become ready&#10;                if pending_or_processing_docs:&#10;                    logger.info(f&quot;Found {len(pending_or_processing_docs)} pending/processing documents - scheduling delayed preview generations&quot;)&#10;                    # Delayed: Give time for documents to be uploaded and processed, then regenerate&#10;                    schedule_delayed_preview_generation(token, case_id, delay_seconds=60)&#10;                    # Also schedule a longer delay for cases where document processing might take longer&#10;                    schedule_delayed_preview_generation(token, case_id, delay_seconds=300)  # 5 minutes&#10;                else:&#10;                    logger.info(f&quot;No pending/processing documents found - skipping delayed preview generations&quot;)&#10;&#10;                if immediate_success:&#10;                    logger.info(f&quot;Successfully generated immediate preview for case {token[:8]}...&quot;)&#10;                else:&#10;                    logger.warning(f&quot;Failed to generate immediate preview for case {token[:8]}...&quot;)&#10;        except Exception as e:&#10;            logger.warning(f&quot;Failed to trigger preview generation: {str(e)}&quot;)&#10;            # Don't fail the main request if preview generation fails&#10;&#10;        response_data = {&#10;            'success': True,&#10;            'message': 'Case saved successfully',&#10;            'token': token,&#10;            'case_id': case_id&#10;        }&#10;&#10;        return jsonify(response_data), 200&#10;&#10;    except Exception as e:&#10;        error_msg = f&quot;Unexpected error saving case: {str(e)}&quot;&#10;        logger.error(error_msg)&#10;        return jsonify({'error': error_msg}), 500&#10;&#10;&#10;@app.route('/api/case-preview/&lt;case_id&gt;', methods=['GET', 'OPTIONS'])&#10;def get_case_preview(case_id):&#10;    &quot;&quot;&quot;Get the active case preview for frontend display.&quot;&quot;&quot;&#10;    # Handle CORS preflight&#10;    if request.method == 'OPTIONS':&#10;        response = jsonify({'message': 'OK'})&#10;        return response&#10;&#10;    try:&#10;        # Validate case_id format (basic UUID check)&#10;        if not case_id or len(case_id) &lt; 10:&#10;            return jsonify({'error': 'Invalid case ID format'}), 400&#10;&#10;        logger.info(f&quot;Fetching preview for case ID: {case_id}&quot;)&#10;&#10;        # Get the active preview from database&#10;        preview_data = read_active_preview(case_id)&#10;&#10;        if not preview_data:&#10;            return jsonify({&#10;                'error': 'No active preview found for this case',&#10;                'case_id': case_id&#10;            }), 404&#10;&#10;        # Extract the structured data for frontend&#10;        preview_content = preview_data.get('preview_content', {})&#10;        preview_json = preview_content.get('preview_json', {})&#10;&#10;        # Build frontend response with all the data the frontend needs&#10;        frontend_response = {&#10;            'case_id': case_id,&#10;            'preview_id': preview_data.get('id'),&#10;            'preview_snippet': preview_data.get('preview_snippet'),&#10;            'generated_at': preview_content.get('generated_at'),&#10;            'doc_status': preview_content.get('doc_summary', {}).get('doc_status', 'none'),&#10;            'doc_count': preview_content.get('doc_summary', {}).get('doc_count', 0),&#10;&#10;            # Main preview content from preview_json&#10;            'headline': preview_json.get('headline', 'HOA Case Analysis'),&#10;            'why_now': preview_json.get('why_now', ''),&#10;            'your_situation': preview_json.get('your_situation', {}),&#10;            'risk_if_wrong': preview_json.get('risk_if_wrong', []),&#10;            'what_you_get_when_you_unlock': preview_json.get('what_you_get_when_you_unlock', []),&#10;            'critical_detail_locked': preview_json.get('critical_detail_locked', {}),&#10;            'cta': preview_json.get('cta', {&#10;                'primary': 'Unlock full response package',&#10;                'secondary': 'See detailed analysis'&#10;            }),&#10;&#10;            # Full preview text (markdown)&#10;            'preview_text': preview_content.get('preview_text', ''),&#10;&#10;            # Metadata&#10;            'model': preview_data.get('model', 'gpt-4o-mini'),&#10;            'prompt_version': preview_data.get('prompt_version', 'v2.0_sales'),&#10;            'is_active': preview_data.get('is_active', True)&#10;        }&#10;&#10;        logger.info(f&quot;Successfully retrieved preview for case {case_id}&quot;)&#10;        return jsonify(frontend_response), 200&#10;&#10;    except Exception as e:&#10;        error_msg = f&quot;Error fetching case preview: {str(e)}&quot;&#10;        logger.error(error_msg)&#10;        return jsonify({'error': error_msg}), 500&#10;&#10;&#10;@app.route('/api/case-preview/by-token/&lt;token&gt;', methods=['GET', 'OPTIONS'])&#10;def get_case_preview_by_token(token):&#10;    &quot;&quot;&quot;Get case preview by token (alternative endpoint for frontend convenience).&quot;&quot;&quot;&#10;    # Handle CORS preflight&#10;    if request.method == 'OPTIONS':&#10;        response = jsonify({'message': 'OK'})&#10;        return response&#10;&#10;    try:&#10;        # Validate token format&#10;        if not token or len(token) &lt; 10:&#10;            return jsonify({'error': 'Invalid token format'}), 400&#10;&#10;        logger.info(f&quot;Fetching case by token: {token[:8]}...&quot;)&#10;&#10;        # First get the case to find the case_id&#10;        case = read_case_by_token(token)&#10;        if not case:&#10;            return jsonify({&#10;                'error': 'Case not found for token',&#10;                'token': token[:8] + '...'&#10;            }), 404&#10;&#10;        case_id = case.get('id')&#10;        if not case_id:&#10;            return jsonify({'error': 'Case ID not found'}), 404&#10;&#10;        # Now get the preview using the case_id&#10;        return get_case_preview(case_id)&#10;&#10;    except Exception as e:&#10;        error_msg = f&quot;Error fetching case preview by token: {str(e)}&quot;&#10;        logger.error(error_msg)&#10;        return jsonify({'error': error_msg}), 500&#10;&#10;&#10;@app.route('/api/case-data', methods=['GET', 'OPTIONS'])&#10;def get_case_data():&#10;    &quot;&quot;&quot;Get case data by token for frontend display.&quot;&quot;&quot;&#10;    # Handle CORS preflight&#10;    if request.method == 'OPTIONS':&#10;        response = jsonify({'message': 'OK'})&#10;        return response&#10;&#10;    try:&#10;        # Get token from query parameters&#10;        token = request.args.get('token', '').strip()&#10;&#10;        if not token:&#10;            return jsonify({'error': 'token is required'}), 400&#10;&#10;        logger.info(f&quot;Fetching case data for token: {token[:8]}...&quot;)&#10;&#10;        # Fetch case data from Supabase&#10;        case = read_case_by_token(token)&#10;        if not case:&#10;            return jsonify({'error': 'Case not found'}), 404&#10;&#10;        # Also fetch the case outputs (full analysis) if available&#10;        case_outputs = None&#10;        try:&#10;            outputs_url = f&quot;{SUPABASE_URL}/rest/v1/dmhoa_case_outputs&quot;&#10;            outputs_params = {&#10;                'case_token': f'eq.{token}',&#10;                'select': '*'&#10;            }&#10;            outputs_headers = supabase_headers()&#10;&#10;            outputs_response = requests.get(outputs_url, params=outputs_params, headers=outputs_headers, timeout=TIMEOUT)&#10;            outputs_response.raise_for_status()&#10;            outputs_data = outputs_response.json()&#10;            &#10;            if outputs_data:&#10;                case_outputs = outputs_data[0]&#10;                logger.info(f&quot;Found case outputs for token {token[:8]}...&quot;)&#10;            else:&#10;                logger.info(f&quot;No case outputs found for token {token[:8]}...&quot;)&#10;&#10;        except Exception as e:&#10;            logger.warning(f&quot;Error fetching case outputs for {token[:8]}...: {str(e)}&quot;)&#10;            # Continue without outputs - not critical&#10;&#10;        # Build response with case data and outputs if available&#10;        response_data = {&#10;            'token': case.get('token'),&#10;            'status': case.get('status', 'preview'),&#10;            'created_at': case.get('created_at'),&#10;            'updated_at': case.get('updated_at'),&#10;            'payload': case.get('payload', {}),&#10;            'outputs': case_outputs.get('outputs') if case_outputs else None,&#10;            'outputs_status': case_outputs.get('status') if case_outputs else None&#10;        }&#10;&#10;        logger.info(f&quot;Successfully retrieved case data for token {token[:8]}...&quot;)&#10;        return jsonify(response_data), 200&#10;&#10;    except Exception as e:&#10;        error_msg = f&quot;Error fetching case data: {str(e)}&quot;&#10;        logger.error(error_msg)&#10;        return jsonify({'error': error_msg}), 500&#10;&#10;&#10;# NEW: Concurrency guard function to prevent duplicate preview generation&#10;def upsert_active_preview_lock(case_id: str) -&gt; bool:&#10;    &quot;&quot;&quot;&#10;    Check if preview generation should proceed to prevent duplicates.&#10;    Returns True if generation should proceed, False if should skip.&#10;    &quot;&quot;&quot;&#10;    with preview_lock:&#10;        # Check if another thread is already generating for this case&#10;        if case_id in preview_generation_locks:&#10;            logger.info(f&quot;Preview generation already in progress for case {case_id}&quot;)&#10;            return False&#10;&#10;        # Check existing active preview and doc status&#10;        existing_preview = read_active_preview(case_id)&#10;        if existing_preview:&#10;            existing_content = existing_preview.get('preview_content', {})&#10;            existing_doc_summary = existing_content.get('doc_summary', {})&#10;            existing_doc_status = existing_doc_summary.get('doc_status', 'none')&#10;&#10;            # If we already have a &quot;ready&quot; status preview, skip generation&#10;            if existing_doc_status == &quot;ready&quot;:&#10;                logger.info(f&quot;Preview with ready docs already exists for case {case_id}&quot;)&#10;                return False&#10;&#10;        # Mark this case as being processed&#10;        preview_generation_locks[case_id] = True&#10;        return True&#10;&#10;def release_preview_lock(case_id: str):&#10;    &quot;&quot;&quot;Release the preview generation lock for a case.&quot;&quot;&quot;&#10;    with preview_lock:&#10;        preview_generation_locks.pop(case_id, None)&#10;&#10;&#10;@app.route('/api/create-checkout-session', methods=['POST', 'OPTIONS'])&#10;def create_checkout_session():&#10;    &quot;&quot;&quot;Create a Stripe checkout session for case purchase.&quot;&quot;&quot;&#10;    # Handle CORS preflight&#10;    if request.method == 'OPTIONS':&#10;        response = jsonify({'message': 'OK'})&#10;        return response&#10;&#10;    try:&#10;        # Validate required environment variables&#10;        if not STRIPE_SECRET_KEY:&#10;            logger.error(&quot;STRIPE_SECRET_KEY not configured&quot;)&#10;            return jsonify({'error': 'Payment system not configured'}), 500&#10;&#10;        if not STRIPE_PRICE_ID:&#10;            logger.error(&quot;STRIPE_PRICE_ID not configured&quot;)&#10;            return jsonify({'error': 'Product pricing not configured'}), 500&#10;&#10;        # Get request data&#10;        data = request.get_json()&#10;        if not data:&#10;            return jsonify({'error': 'No data provided'}), 400&#10;&#10;        # Accept either case_id or case_token for flexibility&#10;        case_id = data.get('case_id')&#10;        case_token = data.get('case_token') or data.get('token')&#10;&#10;        logger.info(f&quot;Checkout request received - case_id: {case_id}, case_token: {case_token}&quot;)&#10;&#10;        if not case_id and not case_token:&#10;            return jsonify({'error': 'case_id or case_token is required'}), 400&#10;&#10;        # If we have a token but no case_id, look up the case_id by token&#10;        if case_token and not case_id:&#10;            case = read_case_by_token(case_token)&#10;            if not case:&#10;                return jsonify({'error': 'Case not found for token'}), 404&#10;            case_id = case.get('id')&#10;            if not case_id:&#10;                return jsonify({'error': 'Case ID not found for token'}), 404&#10;        elif case_id:&#10;            # Validate case exists by case_id&#10;            case = read_case_by_id(case_id)&#10;            if not case:&#10;                return jsonify({'error': 'Case not found'}), 404&#10;        else:&#10;            return jsonify({'error': 'Unable to identify case'}), 400&#10;&#10;        # Create Stripe checkout session&#10;        try:&#10;            # Use the correct frontend URL for development/staging&#10;            frontend_url = &quot;https://dmhoadev.netlify.app&quot;  # Use your actual frontend domain&#10;&#10;            checkout_session = stripe.checkout.Session.create(&#10;                payment_method_types=['card'],&#10;                line_items=[{&#10;                    'price': STRIPE_PRICE_ID,&#10;                    'quantity': 1,&#10;                }],&#10;                mode='payment',&#10;                success_url=f&quot;{frontend_url}/success?case_id={case_id}&amp;session_id={{CHECKOUT_SESSION_ID}}&quot;,&#10;                cancel_url=f&quot;{frontend_url}/case-preview?case={case_id}&quot;,&#10;                metadata={&#10;                    'case_id': case_id,&#10;                    'case_token': case.get('token', ''),&#10;                }&#10;            )&#10;&#10;            logger.info(f&quot;Created checkout session {checkout_session.id} for case {case_id}&quot;)&#10;&#10;            # Create response with multiple field names for frontend compatibility&#10;            response_data = {&#10;                'checkout_url': checkout_session.url,&#10;                'url': checkout_session.url,  # Alternative field name&#10;                'session_id': checkout_session.id,&#10;                'id': checkout_session.id,  # Alternative field name&#10;                'success': True&#10;            }&#10;&#10;            logger.info(f&quot;Returning checkout response: {response_data}&quot;)&#10;            return jsonify(response_data), 200&#10;&#10;        except stripe.error.StripeError as e:&#10;            logger.error(f&quot;Stripe error creating checkout session: {str(e)}&quot;)&#10;            return jsonify({'error': 'Payment system error'}), 500&#10;&#10;    except Exception as e:&#10;        error_msg = f&quot;Error creating checkout session: {str(e)}&quot;&#10;        logger.error(error_msg)&#10;        return jsonify({'error': error_msg}), 500&#10;&#10;&#10;@app.route('/webhooks/stripe', methods=['POST'])&#10;def stripe_webhook():&#10;    &quot;&quot;&quot;Handle Stripe webhook events, particularly checkout.session.completed.&quot;&quot;&quot;&#10;    payload = request.get_data()&#10;    sig_header = request.headers.get('Stripe-Signature')&#10;&#10;    try:&#10;        # Verify webhook signature&#10;        if not STRIPE_WEBHOOK_SECRET:&#10;            logger.error(&quot;STRIPE_WEBHOOK_SECRET not configured&quot;)&#10;            return jsonify({'error': 'Webhook secret not configured'}), 500&#10;&#10;        event = stripe.Webhook.construct_event(&#10;            payload, sig_header, STRIPE_WEBHOOK_SECRET&#10;        )&#10;    except ValueError as e:&#10;        logger.error(f&quot;Invalid payload: {e}&quot;)&#10;        return jsonify({'error': 'Invalid payload'}), 400&#10;    except stripe.error.SignatureVerificationError as e:&#10;        logger.error(f&quot;Invalid signature: {e}&quot;)&#10;        return jsonify({'error': 'Invalid signature'}), 400&#10;&#10;    # Handle the event&#10;    if event['type'] == 'checkout.session.completed':&#10;        session = event['data']['object']&#10;        logger.info(f&quot;Payment completed for session: {session['id']}&quot;)&#10;&#10;        # Extract metadata&#10;        metadata = session.get('metadata', {})&#10;        case_id = metadata.get('case_id')&#10;        case_token = metadata.get('case_token')&#10;&#10;        if not case_id:&#10;            logger.error(f&quot;No case_id in session metadata: {session['id']}&quot;)&#10;            return jsonify({'error': 'No case_id in metadata'}), 400&#10;&#10;        # Get case info&#10;        case = read_case_by_id(case_id)&#10;        if not case:&#10;            logger.error(f&quot;Case not found for ID: {case_id}&quot;)&#10;            return jsonify({'error': 'Case not found'}), 404&#10;&#10;        # Get the token from case if not in metadata&#10;        if not case_token:&#10;            case_token = case.get('token')&#10;&#10;        if not case_token:&#10;            logger.error(f&quot;No token found for case: {case_id}&quot;)&#10;            return jsonify({'error': 'No token found for case'}), 400&#10;&#10;        # STEP 1: Mark the case as unlocked/paid in the database&#10;        try:&#10;            case_update_url = f&quot;{SUPABASE_URL}/rest/v1/dmhoa_cases&quot;&#10;            case_update_params = {'id': f'eq.{case_id}'}&#10;            # Only update fields that exist in the database schema&#10;            case_update_data = {&#10;                'status': 'paid',&#10;                'updated_at': datetime.utcnow().isoformat()&#10;            }&#10;            case_update_headers = supabase_headers()&#10;&#10;            update_response = requests.patch(case_update_url, params=case_update_params,&#10;                                           headers=case_update_headers, json=case_update_data, timeout=TIMEOUT)&#10;            update_response.raise_for_status()&#10;&#10;            logger.info(f&quot;Successfully marked case {case_id} as paid&quot;)&#10;&#10;        except Exception as e:&#10;            logger.error(f&quot;Failed to mark case {case_id} as paid: {str(e)}&quot;)&#10;            # Log the full response for debugging&#10;            try:&#10;                logger.error(f&quot;Response status: {update_response.status_code}&quot;)&#10;                logger.error(f&quot;Response text: {update_response.text}&quot;)&#10;            except:&#10;                pass&#10;            return jsonify({'error': 'Failed to update case status'}), 500&#10;&#10;        # STEP 2: Generate the actual full case analysis using GPT (asynchronously)&#10;        logger.info(f&quot;Scheduling full case analysis for paid case {case_token}&quot;)&#10;&#10;        try:&#10;            # Schedule the analysis generation in a background thread to avoid webhook timeout&#10;            def generate_analysis_async():&#10;                try:&#10;                    logger.info(f&quot;Starting background analysis generation for {case_token}&quot;)&#10;                    full_analysis_result = generate_full_case_analysis_internal(case_token)&#10;&#10;                    if not full_analysis_result['success']:&#10;                        logger.error(f&quot;Background analysis failed for {case_token}: {full_analysis_result['error']}&quot;)&#10;                        return&#10;&#10;                    logger.info(f&quot;Successfully generated background analysis for {case_token}&quot;)&#10;&#10;                    # Add payment metadata to the existing output&#10;                    case_url = f&quot;{SITE_URL}/case.html?case={case_token}&amp;session_id={session['id']}&quot;&#10;&#10;                    # Update the case outputs with payment information&#10;                    try:&#10;                        outputs_update_url = f&quot;{SUPABASE_URL}/rest/v1/dmhoa_case_outputs&quot;&#10;                        outputs_update_params = {'case_token': f'eq.{case_token}'}&#10;&#10;                        # Get the current outputs to merge with payment info&#10;                        current_outputs = full_analysis_result['outputs']&#10;&#10;                        # Add payment metadata to the outputs&#10;                        enhanced_outputs = {&#10;                            **current_outputs,&#10;                            'payment_info': {&#10;                                'case_url': case_url,&#10;                                'session_id': session['id'],&#10;                                'payment_amount': session.get('amount_total'),&#10;                                'currency': session.get('currency'),&#10;                                'customer_email': session.get('customer_details', {}).get('email'),&#10;                                'payment_completed_at': datetime.utcnow().isoformat()&#10;                            }&#10;                        }&#10;&#10;                        outputs_update_data = {&#10;                            'outputs': enhanced_outputs,&#10;                            'updated_at': datetime.utcnow().isoformat()&#10;                        }&#10;                        outputs_update_headers = supabase_headers()&#10;&#10;                        outputs_update_response = requests.patch(outputs_update_url, params=outputs_update_params,&#10;                                                                headers=outputs_update_headers, json=outputs_update_data, timeout=TIMEOUT)&#10;                        outputs_update_response.raise_for_status()&#10;&#10;                        logger.info(f&quot;Successfully enhanced case outputs with payment info for {case_token}&quot;)&#10;                        logger.info(f&quot;Case URL: {case_url}&quot;)&#10;&#10;                    except Exception as e:&#10;                        logger.error(f&quot;Failed to enhance outputs with payment info: {str(e)}&quot;)&#10;                        # Don't fail the whole process if this fails - the analysis is already generated&#10;&#10;                except Exception as e:&#10;                    logger.error(f&quot;Error in background analysis generation for {case_token}: {str(e)}&quot;)&#10;&#10;            # Start the analysis in a background thread&#10;            analysis_thread = threading.Thread(target=generate_analysis_async, daemon=True)&#10;            analysis_thread.start()&#10;&#10;            logger.info(f&quot;Successfully scheduled background analysis generation for {case_token}&quot;)&#10;&#10;        except Exception as e:&#10;            logger.error(f&quot;Failed to schedule analysis generation for {case_token}: {str(e)}&quot;)&#10;            # Don't fail the webhook response - we can retry later&#10;&#10;    else:&#10;        logger.info(f&quot;Unhandled event type: {event['type']}&quot;)&#10;&#10;    return jsonify({'status': 'success'}), 200&#10;&#10;&#10;def generate_full_case_analysis_internal(case_token: str) -&gt; Dict[str, Any]:&#10;    &quot;&quot;&quot;&#10;    Generate a comprehensive GPT-powered case analysis for a paid case.&#10;    This replaces the preview with a full, detailed analysis.&#10;    &quot;&quot;&quot;&#10;    logger.info(f&quot;Starting internal case analysis for token: {case_token}&quot;)&#10;&#10;    try:&#10;        # Fetch documents for this case&#10;        docs = fetch_ready_documents_by_token(case_token, limit=5)&#10;        if not docs:&#10;            logger.error(f&quot;No ready documents found for token: {case_token}&quot;)&#10;            return {'success': False, 'error': 'No documents available for analysis'}&#10;&#10;        # Get case information&#10;        case_info = read_case_by_token(case_token)&#10;        if not case_info:&#10;            logger.error(f&quot;Case not found for token: {case_token}&quot;)&#10;            return {'success': False, 'error': 'Case not found'}&#10;&#10;        # Prepare document content for GPT&#10;        usable_docs = [d for d in docs if d.get('extracted_text') and len(d.get('extracted_text', '').strip()) &gt; 100]&#10;&#10;        if not usable_docs:&#10;            logger.error(f&quot;No usable documents with extracted text for token: {case_token}&quot;)&#10;            return {'success': False, 'error': 'No usable documents with text content'}&#10;&#10;        docs_newest = max(d.get('updated_at', '') for d in docs)&#10;        docs_block = &quot;\n\n&quot;.join([&#10;            f&quot;=== Document {i+1} ===\n&quot;&#10;            f&quot;Filename: {d.get('filename', 'Unknown')}\n&quot;&#10;            f&quot;Pages: {d.get('page_count', 'Unknown')}\n&quot;&#10;            f&quot;Characters: {d.get('char_count', 'Unknown')}\n\n&quot;&#10;            f&quot;{d.get('extracted_text', '').strip()}&quot;&#10;            for i, d in enumerate(usable_docs)&#10;        ])&#10;&#10;        # Prepare case payload for GPT&#10;        payload = {&#10;            'case_token': case_token,&#10;            'customer_email': case_info.get('customer_email'),&#10;            'documents_count': len(docs),&#10;            'usable_documents_count': len(usable_docs)&#10;        }&#10;&#10;        # Define draft titles - these are the three main response types we generate&#10;        draft_titles = {&#10;            'clarification': 'Request for Clarification and Documentation',&#10;            'extension': 'Request for Deadline Extension',&#10;            'compliance': 'Compliance Response and Action Plan'&#10;        }&#10;&#10;        # Define the JSON schema for structured output&#10;        schema = {&#10;            &quot;type&quot;: &quot;object&quot;,&#10;            &quot;additionalProperties&quot;: False,&#10;            &quot;properties&quot;: {&#10;                &quot;summary_html&quot;: {&quot;type&quot;: &quot;string&quot;},&#10;                &quot;letter_summary&quot;: {&quot;type&quot;: &quot;string&quot;},&#10;                &quot;draft_titles&quot;: {&#10;                    &quot;type&quot;: &quot;object&quot;,&#10;                    &quot;additionalProperties&quot;: False,&#10;                    &quot;properties&quot;: {&#10;                        &quot;clarification&quot;: {&quot;type&quot;: &quot;string&quot;},&#10;                        &quot;extension&quot;: {&quot;type&quot;: &quot;string&quot;},&#10;                        &quot;compliance&quot;: {&quot;type&quot;: &quot;string&quot;}&#10;                    },&#10;                    &quot;required&quot;: [&quot;clarification&quot;, &quot;extension&quot;, &quot;compliance&quot;]&#10;                },&#10;                &quot;risks_and_deadlines&quot;: {&#10;                    &quot;type&quot;: &quot;object&quot;,&#10;                    &quot;additionalProperties&quot;: False,&#10;                    &quot;properties&quot;: {&#10;                        &quot;deadlines&quot;: {&quot;type&quot;: &quot;array&quot;, &quot;items&quot;: {&quot;type&quot;: &quot;string&quot;}, &quot;minItems&quot;: 1},&#10;                        &quot;risks&quot;: {&quot;type&quot;: &quot;array&quot;, &quot;items&quot;: {&quot;type&quot;: &quot;string&quot;}, &quot;minItems&quot;: 3}&#10;                    },&#10;                    &quot;required&quot;: [&quot;deadlines&quot;, &quot;risks&quot;]&#10;                },&#10;                &quot;action_plan&quot;: {&quot;type&quot;: &quot;array&quot;, &quot;items&quot;: {&quot;type&quot;: &quot;string&quot;}, &quot;minItems&quot;: 6},&#10;                &quot;drafts&quot;: {&#10;                    &quot;type&quot;: &quot;object&quot;,&#10;                    &quot;additionalProperties&quot;: False,&#10;                    &quot;properties&quot;: {&#10;                        &quot;clarification&quot;: {&quot;type&quot;: &quot;string&quot;},&#10;                        &quot;extension&quot;: {&quot;type&quot;: &quot;string&quot;},&#10;                        &quot;compliance&quot;: {&quot;type&quot;: &quot;string&quot;}&#10;                    },&#10;                    &quot;required&quot;: [&quot;clarification&quot;, &quot;extension&quot;, &quot;compliance&quot;]&#10;                },&#10;                &quot;questions_to_ask&quot;: {&quot;type&quot;: &quot;array&quot;, &quot;items&quot;: {&quot;type&quot;: &quot;string&quot;}, &quot;minItems&quot;: 6},&#10;                &quot;lowest_cost_path&quot;: {&quot;type&quot;: &quot;array&quot;, &quot;items&quot;: {&quot;type&quot;: &quot;string&quot;}, &quot;minItems&quot;: 4}&#10;            },&#10;            &quot;required&quot;: [&#10;                &quot;summary_html&quot;, &quot;letter_summary&quot;, &quot;draft_titles&quot;, &quot;risks_and_deadlines&quot;,&#10;                &quot;action_plan&quot;, &quot;drafts&quot;, &quot;questions_to_ask&quot;, &quot;lowest_cost_path&quot;&#10;            ]&#10;        }&#10;&#10;        doc_fingerprint = {&#10;            'count': len(docs),&#10;            'usableCount': len(usable_docs),&#10;            'newestUpdatedAt': docs_newest,&#10;            'ids': [d['id'] for d in docs],&#10;            'statuses': [d.get('status') for d in docs],&#10;            'charCounts': [d.get('char_count') for d in docs]&#10;        }&#10;&#10;        # Prepare messages for GPT&#10;        messages = [&#10;            {&#10;                &quot;role&quot;: &quot;system&quot;,&#10;                &quot;content&quot;: &quot;&quot;&quot;You are an expert HOA dispute assistant helping homeowners respond to violations and disputes.&#10;&#10;Your job is to analyze the homeowner's situation and provide comprehensive, actionable guidance including:&#10;1. A detailed HTML summary of their situation&#10;2. Three complete draft response letters (clarification, extension, compliance)  &#10;3. Risk assessment with specific deadlines&#10;4. Step-by-step action plan&#10;5. Strategic questions to ask the HOA&#10;6. Lowest-cost resolution path&#10;&#10;OUTPUT RULES (CRITICAL):&#10;- ONLY &quot;summary_html&quot; may contain HTML tags&#10;- summary_html must use ONLY these HTML tags: &lt;div&gt;, &lt;strong&gt;, &lt;ul&gt;, &lt;li&gt;, &lt;p&gt;&#10;- ALL draft letters MUST be PLAIN TEXT with \\n for line breaks&#10;- Each draft must be a complete, professional letter ready to send&#10;- Include specific facts, dates, amounts, and references from the documents&#10;- Be professional but firm in tone&#10;&#10;TONE AND POSITIONING RULES (VERY IMPORTANT):&#10;- NEVER admit fault, liability, or acknowledge violations in any draft&#10;- Use non-admitting language: &quot;responding to your notice&quot; instead of &quot;acknowledging the violation&quot;&#10;- For compliance drafts, say &quot;addressing the Association's stated concerns&quot; not &quot;acknowledging the violation&quot;&#10;- For extension requests, provide specific, factual reasons (vendor availability, scheduling constraints) not vague excuses&#10;- Preserve homeowner's rights while showing cooperation&#10;- Frame responses as addressing concerns, not admitting wrongdoing&#10;&#10;DRAFT REQUIREMENTS:&#10;Each draft letter must include:&#10;- Professional subject line&#10;- Proper greeting and introduction&#10;- Specific references to HOA rules/violations mentioned (without admission)&#10;- Clear requests with deadlines&#10;- Professional closing requesting written confirmation&#10;- Homeowner's rights and procedural protections&#10;&#10;SPECIFIC LANGUAGE GUIDANCE:&#10;- Compliance Draft: &quot;I am responding to the notice regarding [issue] and am taking steps to address the Association's stated concerns&quot;&#10;- Extension Draft: &quot;Due to vendor availability and inspection scheduling constraints&quot; (not &quot;unforeseen circumstances&quot;)&#10;- Never use phrases like &quot;I acknowledge the violation&quot; or &quot;I admit fault&quot;&#10;- Always frame as &quot;responding to notice&quot; or &quot;addressing concerns&quot;&#10;&#10;Make this comprehensive - worth the $29 the customer paid.&quot;&quot;&quot;&#10;            },&#10;            {&#10;                &quot;role&quot;: &quot;user&quot;,&#10;                &quot;content&quot;: f&quot;&quot;&quot;Please analyze this HOA case and provide comprehensive assistance.&#10;&#10;CASE INFORMATION:&#10;{json.dumps(payload, indent=2)}&#10;&#10;DOCUMENT ANALYSIS:&#10;{docs_block}&#10;&#10;REQUIRED DRAFT TYPES:&#10;- Clarification: &quot;{draft_titles['clarification']}&quot;&#10;- Extension: &quot;{draft_titles['extension']}&quot;  &#10;- Compliance: &quot;{draft_titles['compliance']}&quot;&#10;&#10;Please provide a thorough analysis with actionable advice, specific draft letters, and strategic guidance. Focus on protecting the homeowner's rights while working toward resolution.&quot;&quot;&quot;&#10;            }&#10;        ]&#10;&#10;        # Make OpenAI API call using the correct chat completions endpoint&#10;        openai_payload = {&#10;            &quot;model&quot;: &quot;gpt-4o-mini&quot;,&#10;            &quot;messages&quot;: messages,&#10;            &quot;response_format&quot;: {&#10;                &quot;type&quot;: &quot;json_schema&quot;,&#10;                &quot;json_schema&quot;: {&#10;                    &quot;name&quot;: &quot;dmhoa_case_analysis&quot;,&#10;                    &quot;strict&quot;: True,&#10;                    &quot;schema&quot;: schema&#10;                }&#10;            },&#10;            &quot;temperature&quot;: 0.7&#10;        }&#10;&#10;        logger.info(f&quot;Making OpenAI API call for case analysis: {case_token}&quot;)&#10;&#10;        try:&#10;            openai_response = requests.post(&#10;                'https://api.openai.com/v1/chat/completions',&#10;                headers={&#10;                    'Authorization': f'Bearer {OPENAI_API_KEY}',&#10;                    'Content-Type': 'application/json'&#10;                },&#10;                json=openai_payload,&#10;                timeout=(10, 180)  # 3 minute timeout for complex analysis&#10;            )&#10;        except requests.exceptions.Timeout as e:&#10;            logger.error(f&quot;OpenAI API call timed out for case {case_token}: {str(e)}&quot;)&#10;            return {'success': False, 'error': f'OpenAI API call timed out: {str(e)}'}&#10;        except requests.exceptions.RequestException as e:&#10;            logger.error(f&quot;OpenAI API call failed for case {case_token}: {str(e)}&quot;)&#10;            return {'success': False, 'error': f'OpenAI API call failed: {str(e)}'}&#10;&#10;        if not openai_response.ok:&#10;            error_text = openai_response.text&#10;            logger.error(f'OpenAI API call failed for {case_token}: {error_text}')&#10;            return {'success': False, 'error': f'OpenAI API failed: {error_text}'}&#10;&#10;        openai_json = openai_response.json()&#10;&#10;        # Extract the structured result&#10;        try:&#10;            message_content = openai_json['choices'][0]['message']['content']&#10;            structured_result = json.loads(message_content)&#10;        except (KeyError, json.JSONDecodeError) as e:&#10;            logger.error(f'Failed to parse OpenAI response for {case_token}: {str(e)}')&#10;            return {'success': False, 'error': f'Failed to parse AI response: {str(e)}'}&#10;&#10;        # Prepare final output&#10;        outputs_to_store = {&#10;            **structured_result,&#10;            'draft_titles': structured_result.get('draft_titles', draft_titles),&#10;            'doc_fingerprint': doc_fingerprint,&#10;            'generated_at': datetime.utcnow().isoformat(),&#10;            'generation_source': 'post_payment_analysis'&#10;        }&#10;&#10;        # Save the full analysis to dmhoa_case_outputs&#10;        upsert_url = f&quot;{SUPABASE_URL}/rest/v1/dmhoa_case_outputs&quot;&#10;        upsert_headers = supabase_headers()&#10;        upsert_headers['Prefer'] = 'resolution=merge-duplicates'&#10;&#10;        success_data = {&#10;            'case_token': case_token,&#10;            'status': 'ready',&#10;            'outputs': outputs_to_store,&#10;            'error': None,&#10;            'model': 'gpt-4o-mini',&#10;            'prompt_version': 'v4_post_payment',&#10;            'updated_at': datetime.utcnow().isoformat()&#10;        }&#10;&#10;        try:&#10;            save_response = requests.post(upsert_url, headers=upsert_headers, json=success_data, timeout=TIMEOUT)&#10;            save_response.raise_for_status()&#10;            logger.info(f'Successfully saved full case analysis for {case_token}')&#10;        except Exception as e:&#10;            logger.error(f'Failed to save case outputs for {case_token}: {str(e)}')&#10;            return {'success': False, 'error': f'Failed to save analysis: {str(e)}'}&#10;&#10;        # Update case timestamp&#10;        try:&#10;            case_update_url = f&quot;{SUPABASE_URL}/rest/v1/dmhoa_cases&quot;&#10;            case_update_params = {'token': f'eq.{case_token}'}&#10;            case_update_data = {'updated_at': datetime.utcnow().isoformat()}&#10;            case_update_headers = supabase_headers()&#10;            requests.patch(case_update_url, params=case_update_params,&#10;                         headers=case_update_headers, json=case_update_data, timeout=TIMEOUT)&#10;        except Exception:&#10;            pass  # Best effort&#10;&#10;        return {'success': True, 'outputs': outputs_to_store}&#10;&#10;    except Exception as e:&#10;        logger.error(f'Error generating full case analysis for {case_token}: {str(e)}')&#10;        return {'success': False, 'error': f'Analysis generation failed: {str(e)}'}&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>